{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1598549735847",
   "display_name": "Python 3.8.3 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-4c34eb6e79ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Normalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Imports complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Data management\n",
    "import pandas as pd\n",
    "\n",
    "# For stratified 10-fold cross validation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Scikit-Learn ML Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Normalization\n",
    "from keras.utils import normalize, to_categorical\n",
    "\n",
    "print('Imports complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_on(X, y, feature_set):\n",
    "    \"\"\"\n",
    "    train_and_eval_on function\n",
    "        Description: This function will train all the models on the given feature set of the X (data) for predicting y (target)\n",
    "\n",
    "        Args: \n",
    "            X => pd.DataFrame object containing the data\n",
    "            y => pd.Series object containings the target classifications\n",
    "            feature_set => list of features in X to use for training\n",
    "\n",
    "        Returns:\n",
    "            metrics => dictionary where the model names are the key and a list of accuracies across all folds is the value\n",
    "                    Keys:\n",
    "                        Random Forest => rf\n",
    "                        Decision Tree => dt\n",
    "                        k-Nearest Neighbors => knn\n",
    "                        Support Vector Machine => svm\n",
    "                        Logistic Regression => lr\n",
    "                        Linear Discriminant Analysis => lda\n",
    "                        AdaBoost => ab\n",
    "                        Naive Bayes => nb\n",
    "                        Keras-TensorFlow => keras\n",
    "                        Fast.ai => fastai\n",
    "    \"\"\"\n",
    "    metrics = {'rf':[],\n",
    "                'dt':[],\n",
    "                'knn':[],\n",
    "                'svm':[],\n",
    "                'lr':[],\n",
    "                'lda':[],\n",
    "                'ab':[],\n",
    "                'nb':[],\n",
    "                'keras':[],\n",
    "                'fastai':[]}\n",
    "\n",
    "    # Select the given features within the data\n",
    "    X = X[feature_set]\n",
    "\n",
    "    # Create stratified, 10-fold cross validation object\n",
    "    random_state = 0\n",
    "    sss = StratifiedKFold(n_splits=10, shuffle=True,                                                random_state=random_state)\n",
    "\n",
    "    # Experiment with 10-fold cross validation\n",
    "    for train_idx, test_idx in sss.split(X, y):\n",
    "        # Split the data into the training and testing sets\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        \n",
    "        # Random Forest Model\n",
    "        rf = RandomForestClassifier(random_state=random_state)\n",
    "        rf.fit(X_train, y_train)\n",
    "        score = rf.score(X_test, y_test)\n",
    "        metrics['rf'].append(score)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the best features for multi-class experiment provided from the feature selection jupyter notebook\n",
    "best_features_multiclass = ['Entropy_Afterpath', 'argPathRatio', 'NumberRate_AfterPath', 'NumberRate_Domain', 'ArgUrlRatio', 'Extension_DigitCount', 'dld_getArg', 'ldl_getArg', 'Query_DigitCount', 'LongestVariableValue', 'Querylength', 'Query_LetterCount', 'ldl_path', 'ArgLen', 'ldl_url', 'dld_path', 'Extension_LetterCount', 'argDomanRatio', 'dld_url', 'URL_DigitCount', 'LongestPathTokenLength', 'URLQueries_variable', 'fileNameLen', 'delimeter_Count', 'NumberRate_URL', 'SymbolCount_Domain', 'domain_token_count', 'tld', 'SymbolCount_Extension', 'this.fileExtLen', 'pathLength', 'subDirLen', 'urlLen', 'charcompace', 'host_DigitCount', 'SymbolCount_Afterpath', 'URL_Letter_Count', 'pathDomainRatio', 'SymbolCount_FileName', 'domainUrlRatio', 'NumberRate_Extension', 'NumberRate_FileName', 'SymbolCount_URL', 'Entropy_Filename', 'Entropy_DirectoryName', 'delimeter_path', 'Arguments_LongestWordLength', 'charcompvowels', 'CharacterContinuityRate', 'spcharUrl', 'executable', 'pathurlRatio', 'Filename_LetterCount', 'Entropy_Extension', 'dld_filename', 'ldl_filename', 'SymbolCount_Directoryname', 'avgdomaintokenlen', 'path_token_count', 'File_name_DigitCount', 'NumberRate_DirectoryName', 'delimeter_Domain', 'Domain_LongestWordLength', 'NumberofDotsinURL', 'Directory_DigitCount', 'Directory_LetterCount', 'URL_sensitiveWord', 'longdomaintokenlen', 'ldl_domain', 'domainlength', 'Entropy_Domain', 'host_letter_count', 'avgpathtokenlen', 'isPortEighty', 'sub-Directory_LongestWordLength', 'dld_domain', 'Path_LongestWordLength', 'Entropy_URL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Data Read:\n   Querylength  domain_token_count  path_token_count  avgdomaintokenlen  \\\n0            0                   4                 5                5.5   \n1            0                   4                 5                5.5   \n2            0                   4                 5                5.5   \n3            0                   4                12                5.5   \n4            0                   4                 6                5.5   \n\n   longdomaintokenlen  avgpathtokenlen  tld  charcompvowels  charcompace  \\\n0                  14         4.400000    4               8            3   \n1                  14         6.000000    4              12            4   \n2                  14         5.800000    4              12            5   \n3                  14         5.500000    4              32           16   \n4                  14         7.333334    4              18           11   \n\n   ldl_url  ...  SymbolCount_FileName  SymbolCount_Extension  \\\n0        0  ...                     1                      0   \n1        0  ...                     0                      0   \n2        0  ...                     0                      0   \n3        0  ...                     0                      0   \n4        0  ...                     0                      0   \n\n   SymbolCount_Afterpath  Entropy_URL  Entropy_Domain  Entropy_DirectoryName  \\\n0                     -1     0.726298        0.784493               0.894886   \n1                     -1     0.688635        0.784493               0.814725   \n2                     -1     0.695049        0.784493               0.814725   \n3                     -1     0.640130        0.784493               0.814725   \n4                     -1     0.681307        0.784493               0.814725   \n\n   Entropy_Filename  Entropy_Extension  Entropy_Afterpath  URL_Type_obf_Type  \n0          0.850608                NaN               -1.0         Defacement  \n1          0.859793                0.0               -1.0         Defacement  \n2          0.801880                0.0               -1.0         Defacement  \n3          0.663210                0.0               -1.0         Defacement  \n4          0.804526                0.0               -1.0         Defacement  \n\n[5 rows x 80 columns]\n"
    }
   ],
   "source": [
    "# Import the data\n",
    "path = './FinalDataset/'\n",
    "fille = 'All.csv'\n",
    "df = pd.read_csv(path + fille)\n",
    "print('Data Read:')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 80 columns and 36707 rows in the provided data.\n"
    }
   ],
   "source": [
    "dep_var = 'URL_Type_obf_Type'\n",
    "\n",
    "print('There are {} columns and {} rows in the provided data.'.format(len(df.columns), len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Below is the dataset's composition\nDefacement    7930\nbenign        7781\nphishing      7586\nmalware       6712\nspam          6698\nName: URL_Type_obf_Type, dtype: int64\n"
    }
   ],
   "source": [
    "print('Below is the dataset\\'s composition')\n",
    "print(df[dep_var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes all rows if they contain NaN values\n",
    "df.dropna(axis='index', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "There are 80 columns and 18982 rows in the provided data.\nBelow is the dataset's composition\nspam          5342\nmalware       4440\nphishing      4014\nbenign        2709\nDefacement    2477\nName: URL_Type_obf_Type, dtype: int64\n"
    }
   ],
   "source": [
    "print('There are {} columns and {} rows in the provided data.'.format(len(df.columns), len(df)))\n",
    "\n",
    "print('Below is the dataset\\'s composition')\n",
    "print(df[dep_var].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'normalize' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-d0c7fae5ade5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create the X (data) and y (labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdep_var\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdep_var\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'normalize' is not defined"
     ]
    }
   ],
   "source": [
    "# Create the X (data) and y (labels)\n",
    "X = normalize( df.loc[:, df.columns != dep_var] )\n",
    "y = df[dep_var]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-b0670b491015>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_features_multiclass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_features_multiclass\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mperformance_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_and_eval_on\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperformance_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "for i in range(5, len(best_features_multiclass)):\n",
    "    features = best_features_multiclass[:i]\n",
    "    performance_metrics = train_and_eval_on(X=X, y=y, feature_set=features)\n",
    "\n",
    "    print(performance_metrics)"
   ]
  }
 ]
}