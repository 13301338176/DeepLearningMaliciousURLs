{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras-Tensorflow-Experiments.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rambasnet/DeepLearningMaliciousURLs/blob/master/Keras-Tensorflow-Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFAQJfeWplvv",
        "colab_type": "text"
      },
      "source": [
        "# Keras-Tensorflow Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ6Ngesk0F9d",
        "colab_type": "text"
      },
      "source": [
        "##### Sources:\n",
        " + https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
        " + https://www.kaggle.com/grafiszti/98-59-acc-on-10-fold-with-testing-7-keras-models\n",
        " + https://keras.io/visualization/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47GgPW0G0Pi7",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GbTWr3JeuzLm"
      },
      "source": [
        "### Include needed files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sBMU4MElu9GB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f548a78-b956-4c95-ca01-5e8e06fb7197"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import operator\n",
        "import time\n",
        "\n",
        "import seaborn as sn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils.np_utils import to_categorical, normalize\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07jwatic7-Jk"
      },
      "source": [
        "### Include Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ngdiiVOJ8Bt4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "5b758e86-2041-49ed-f4c1-9eabf54c612c"
      },
      "source": [
        "%%bash\n",
        "URL=https://iscxdownloads.cs.unb.ca/iscxdownloads/ISCX-URL-2016/\n",
        "FILES=(ISCXURL2016.zip) \n",
        "for FILE in ${FILES[*]}; do\n",
        "    if [ ! -f \"$FILE\" ]; then\n",
        "        printf \"downloading %s\\n\" $FILE\n",
        "        curl -O $URL$FILE\n",
        "        # unzip files\n",
        "        echo 'unzipping ' $FILE\n",
        "        unzip -o $FILE #overwrite exiting files/folders if exists\n",
        "    fi\n",
        "done"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading ISCXURL2016.zip\n",
            "unzipping  ISCXURL2016.zip\n",
            "Archive:  ISCXURL2016.zip\n",
            "   creating: FinalDataset/\n",
            "  inflating: FinalDataset/Spam_Infogain_test.csv  \n",
            "  inflating: FinalDataset/Spam_Infogain.csv  \n",
            "  inflating: FinalDataset/Spam_BestFirst_test.csv  \n",
            "  inflating: FinalDataset/Spam_BestFirst.csv  \n",
            "  inflating: FinalDataset/Spam.csv   \n",
            "  inflating: FinalDataset/Phishing_Infogain_test.csv  \n",
            "  inflating: FinalDataset/Phishing_Infogain.csv  \n",
            "  inflating: FinalDataset/Phishing.csv  \n",
            "  inflating: FinalDataset/Malware_Infogain_test.csv  \n",
            "  inflating: FinalDataset/Malware_Infogain.csv  \n",
            "  inflating: FinalDataset/Malware_BestFirst.csv  \n",
            "  inflating: FinalDataset/Malware.csv  \n",
            "  inflating: FinalDataset/Defacement_Infogain_test.csv  \n",
            "  inflating: FinalDataset/Defacement_Infogain.csv  \n",
            "  inflating: FinalDataset/Defacement_BestFirst.csv  \n",
            "  inflating: FinalDataset/Defacement.csv  \n",
            "  inflating: FinalDataset/All_Infogain_test.csv  \n",
            "  inflating: FinalDataset/All_Infogain.csv  \n",
            "  inflating: FinalDataset/All_BestFirst_test.csv  \n",
            "  inflating: FinalDataset/Phishing_BestFirst.csv  \n",
            "  inflating: FinalDataset/All_BestFirst.csv  \n",
            "  inflating: FinalDataset/All.csv    \n",
            "   creating: FinalDataset/URL/\n",
            "  inflating: FinalDataset/URL/spam_dataset.csv  \n",
            "  inflating: FinalDataset/URL/phishing_dataset.csv  \n",
            "  inflating: FinalDataset/URL/Malware_dataset.csv  \n",
            "  inflating: FinalDataset/URL/DefacementSitesURLFiltered.csv  \n",
            "  inflating: FinalDataset/URL/Benign_list_big_final.csv  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r  1 10.6M    1  208k    0     0   220k      0  0:00:49 --:--:--  0:00:49  219k\r100 10.6M  100 10.6M    0     0  7510k      0  0:00:01  0:00:01 --:--:-- 7510k\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MIqy2Xmc8HoD"
      },
      "source": [
        "### Check Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zl2T8MRq8PNT",
        "outputId": "83fc8fce-8c07-44fb-aa59-0c0263a37b66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "! ls FinalDataset"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All_BestFirst.csv\t      Malware_Infogain_test.csv\n",
            "All_BestFirst_test.csv\t      Phishing_BestFirst.csv\n",
            "All.csv\t\t\t      Phishing.csv\n",
            "All_Infogain.csv\t      Phishing_Infogain.csv\n",
            "All_Infogain_test.csv\t      Phishing_Infogain_test.csv\n",
            "Defacement_BestFirst.csv      Spam_BestFirst.csv\n",
            "Defacement.csv\t\t      Spam_BestFirst_test.csv\n",
            "Defacement_Infogain.csv       Spam.csv\n",
            "Defacement_Infogain_test.csv  Spam_Infogain.csv\n",
            "Malware_BestFirst.csv\t      Spam_Infogain_test.csv\n",
            "Malware.csv\t\t      URL\n",
            "Malware_Infogain.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IeCm3LP08XOy"
      },
      "source": [
        "### Set some data\n",
        "> Some data needs to be set, we need to ensure that constants are set properly. These are important but will not be used until later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qJuZTgd0u_WG",
        "colab": {}
      },
      "source": [
        "resultPath = 'results_keras_tensorflow'\n",
        "if not os.path.exists(resultPath):\n",
        "   print('result path {} created.'.format(resultPath))\n",
        "   os.mkdir(resultPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EHnzBhcavSR-",
        "colab": {}
      },
      "source": [
        "model_name=\"init\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Vo0Cwne9wRJ"
      },
      "source": [
        "## Functions for Testing\n",
        "> Now that our data has been collected it is time to create functions that will be used in later tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFtoJCs_7T8p",
        "colab": {}
      },
      "source": [
        "def loadData(csvFile):\n",
        "    pickleDump = '{}.pickle'.format(csvFile)\n",
        "    if os.path.exists(pickleDump):\n",
        "        df = pd.read_pickle(pickleDump)\n",
        "    else:\n",
        "        df = pd.read_csv(csvFile, low_memory=False, na_values='NaN')\n",
        "        # clean data\n",
        "        # strip the whitspaces from column names\n",
        "        df = df.rename(str.strip, axis='columns')\n",
        "        #df.drop(columns=[], inplace=True)\n",
        "        # drop missing values/NaN etc.\n",
        "        #df.dropna(inplace=True)\n",
        "        # drop Infinity rows and NaN string from each column\n",
        "        for col in df.columns:\n",
        "            indexNames = df[df[col]=='Infinity'].index\n",
        "            if not indexNames.empty:\n",
        "                print('deleting {} rows with Infinity in column {}'.format(len(indexNames), col))\n",
        "                df.drop(indexNames, inplace=True)\n",
        "        \n",
        "        df.to_pickle(pickleDump)\n",
        "    \n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GB51N0iLCK9J",
        "colab": {}
      },
      "source": [
        "def baseline_model(inputDim=-1,batch_size=32,outputDim=-1):\n",
        "    global model_name, model_extension\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(inputDim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(.5),\n",
        "        Dense(batch_size, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(.5),\n",
        "        Dense(outputDim, activation='softmax')\n",
        "    ]) #This is the output layer\n",
        "\n",
        "    if outputDim > 2:\n",
        "        print('Categorical Cross-Entropy Loss Function')\n",
        "        model_extension = \"_categorical\"\n",
        "        model.compile(optimizer='adam',\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    else:\n",
        "        model_extension = \"_binary\"\n",
        "        print('Binary Cross-Entropy Loss Function')\n",
        "        model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbiT-Sf9plwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_labels(dataframe):\n",
        "    dataframe=dataframe.copy()\n",
        "    data_y=dataframe.pop(dep_var)\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(data_y)\n",
        "    data_y = encoder.transform(data_y)\n",
        "    dummy_y = to_categorical(data_y)\n",
        "    return dummy_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aQvhWPU1H8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(title):\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GkZ-uZuz9_pi"
      },
      "source": [
        "## Data\n",
        "### Load and Clean\n",
        "> First we will load our data, scan the columns for Infinity and NaN values, and remove those columns from testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h_7Y2Ytp7xa-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "04438150-817f-4f56-f3f6-25f223a96d1c"
      },
      "source": [
        "df1 = loadData('FinalDataset/All.csv')\n",
        "df1=df1.dropna(axis=1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/pandas/core/ops.py:1649: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
            "  result = method(y)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "deleting 10 rows with Infinity in column argPathRatio\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mekPFv6lplwm",
        "colab_type": "text"
      },
      "source": [
        "### Display Data Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ZwM-GC1-LPj",
        "outputId": "39b5b271-f0d6-4f80-9171-89e1bfe15964",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "df1.columns"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Querylength', 'domain_token_count', 'path_token_count',\n",
              "       'avgdomaintokenlen', 'longdomaintokenlen', 'tld', 'charcompvowels',\n",
              "       'charcompace', 'ldl_url', 'ldl_domain', 'ldl_path', 'ldl_filename',\n",
              "       'ldl_getArg', 'dld_url', 'dld_domain', 'dld_path', 'dld_filename',\n",
              "       'dld_getArg', 'urlLen', 'domainlength', 'pathLength', 'subDirLen',\n",
              "       'fileNameLen', 'this.fileExtLen', 'ArgLen', 'pathurlRatio',\n",
              "       'ArgUrlRatio', 'argDomanRatio', 'domainUrlRatio', 'pathDomainRatio',\n",
              "       'argPathRatio', 'executable', 'isPortEighty', 'NumberofDotsinURL',\n",
              "       'ISIpAddressInDomainName', 'CharacterContinuityRate',\n",
              "       'LongestVariableValue', 'URL_DigitCount', 'host_DigitCount',\n",
              "       'Directory_DigitCount', 'File_name_DigitCount', 'Extension_DigitCount',\n",
              "       'Query_DigitCount', 'URL_Letter_Count', 'host_letter_count',\n",
              "       'Directory_LetterCount', 'Filename_LetterCount',\n",
              "       'Extension_LetterCount', 'Query_LetterCount', 'LongestPathTokenLength',\n",
              "       'Domain_LongestWordLength', 'Path_LongestWordLength',\n",
              "       'sub-Directory_LongestWordLength', 'Arguments_LongestWordLength',\n",
              "       'URL_sensitiveWord', 'URLQueries_variable', 'spcharUrl',\n",
              "       'delimeter_Domain', 'delimeter_path', 'delimeter_Count',\n",
              "       'NumberRate_URL', 'NumberRate_Domain', 'NumberRate_DirectoryName',\n",
              "       'NumberRate_FileName', 'SymbolCount_URL', 'SymbolCount_Domain',\n",
              "       'SymbolCount_Directoryname', 'SymbolCount_FileName',\n",
              "       'SymbolCount_Extension', 'SymbolCount_Afterpath', 'Entropy_URL',\n",
              "       'Entropy_Domain', 'URL_Type_obf_Type'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX24a-Woplwt",
        "colab_type": "text"
      },
      "source": [
        "### Display Matrix Shape of Data\n",
        "> In the format (samples, columns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NBvgfW-J--g8",
        "outputId": "29f5e371-9bda-4cfd-9268-3d8e15809c38",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df1.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36697, 73)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSjQXVYKplw0",
        "colab_type": "text"
      },
      "source": [
        "### Display First Samples in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9eYtvDf5AAR5",
        "outputId": "31b59103-1aa1-454a-acbd-07cbd9648317",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Querylength</th>\n",
              "      <th>domain_token_count</th>\n",
              "      <th>path_token_count</th>\n",
              "      <th>avgdomaintokenlen</th>\n",
              "      <th>longdomaintokenlen</th>\n",
              "      <th>tld</th>\n",
              "      <th>charcompvowels</th>\n",
              "      <th>charcompace</th>\n",
              "      <th>ldl_url</th>\n",
              "      <th>ldl_domain</th>\n",
              "      <th>...</th>\n",
              "      <th>NumberRate_FileName</th>\n",
              "      <th>SymbolCount_URL</th>\n",
              "      <th>SymbolCount_Domain</th>\n",
              "      <th>SymbolCount_Directoryname</th>\n",
              "      <th>SymbolCount_FileName</th>\n",
              "      <th>SymbolCount_Extension</th>\n",
              "      <th>SymbolCount_Afterpath</th>\n",
              "      <th>Entropy_URL</th>\n",
              "      <th>Entropy_Domain</th>\n",
              "      <th>URL_Type_obf_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.726298</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.688635</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.695049</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>32</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.640130</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.681307</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 73 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Querylength  domain_token_count  path_token_count  avgdomaintokenlen  \\\n",
              "0            0                   4                 5                5.5   \n",
              "1            0                   4                 5                5.5   \n",
              "2            0                   4                 5                5.5   \n",
              "3            0                   4                12                5.5   \n",
              "4            0                   4                 6                5.5   \n",
              "\n",
              "   longdomaintokenlen  tld  charcompvowels  charcompace  ldl_url  ldl_domain  \\\n",
              "0                  14    4               8            3        0           0   \n",
              "1                  14    4              12            4        0           0   \n",
              "2                  14    4              12            5        0           0   \n",
              "3                  14    4              32           16        0           0   \n",
              "4                  14    4              18           11        0           0   \n",
              "\n",
              "   ...  NumberRate_FileName  SymbolCount_URL  SymbolCount_Domain  \\\n",
              "0  ...             0.066667                8                   3   \n",
              "1  ...             0.000000                8                   3   \n",
              "2  ...             0.000000                8                   3   \n",
              "3  ...             0.000000                8                   3   \n",
              "4  ...             0.000000                8                   3   \n",
              "\n",
              "   SymbolCount_Directoryname  SymbolCount_FileName  SymbolCount_Extension  \\\n",
              "0                          2                     1                      0   \n",
              "1                          3                     0                      0   \n",
              "2                          3                     0                      0   \n",
              "3                          3                     0                      0   \n",
              "4                          3                     0                      0   \n",
              "\n",
              "   SymbolCount_Afterpath  Entropy_URL  Entropy_Domain  URL_Type_obf_Type  \n",
              "0                     -1     0.726298        0.784493         Defacement  \n",
              "1                     -1     0.688635        0.784493         Defacement  \n",
              "2                     -1     0.695049        0.784493         Defacement  \n",
              "3                     -1     0.640130        0.784493         Defacement  \n",
              "4                     -1     0.681307        0.784493         Defacement  \n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLiGVjzOplw6",
        "colab_type": "text"
      },
      "source": [
        "### Display Last Samples in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsqLE8Wnplw7",
        "colab_type": "code",
        "colab": {},
        "outputId": "7110398e-aae8-43f4-e997-f4b41af3e4b5"
      },
      "source": [
        "df1.tail()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Querylength</th>\n",
              "      <th>domain_token_count</th>\n",
              "      <th>path_token_count</th>\n",
              "      <th>avgdomaintokenlen</th>\n",
              "      <th>longdomaintokenlen</th>\n",
              "      <th>tld</th>\n",
              "      <th>charcompvowels</th>\n",
              "      <th>charcompace</th>\n",
              "      <th>ldl_url</th>\n",
              "      <th>ldl_domain</th>\n",
              "      <th>...</th>\n",
              "      <th>NumberRate_FileName</th>\n",
              "      <th>SymbolCount_URL</th>\n",
              "      <th>SymbolCount_Domain</th>\n",
              "      <th>SymbolCount_Directoryname</th>\n",
              "      <th>SymbolCount_FileName</th>\n",
              "      <th>SymbolCount_Extension</th>\n",
              "      <th>SymbolCount_Afterpath</th>\n",
              "      <th>Entropy_URL</th>\n",
              "      <th>Entropy_Domain</th>\n",
              "      <th>URL_Type_obf_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36702</th>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>5.750000</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.690555</td>\n",
              "      <td>0.791265</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36703</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>23</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>15</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.665492</td>\n",
              "      <td>0.820010</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36704</th>\n",
              "      <td>58</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>6.666666</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>41</td>\n",
              "      <td>34</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.377778</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0.656807</td>\n",
              "      <td>0.801139</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36705</th>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>4.333334</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.284091</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0.725963</td>\n",
              "      <td>0.897617</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36706</th>\n",
              "      <td>40</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "      <td>6.666666</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>35</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.365079</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>0.674351</td>\n",
              "      <td>0.801139</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 73 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Querylength  domain_token_count  path_token_count  avgdomaintokenlen  \\\n",
              "36702           29                   4                14           5.750000   \n",
              "36703            0                   4                13           3.750000   \n",
              "36704           58                   3                27           6.666666   \n",
              "36705           35                   3                13           4.333334   \n",
              "36706           40                   3                25           6.666666   \n",
              "\n",
              "       longdomaintokenlen  tld  charcompvowels  charcompace  ldl_url  \\\n",
              "36702                  12    4              20           24        3   \n",
              "36703                   8    4              24           23        0   \n",
              "36704                  16    3              41           34       20   \n",
              "36705                   9    3              15           13        7   \n",
              "36706                  16    3              35           31       19   \n",
              "\n",
              "       ldl_domain  ...  NumberRate_FileName  SymbolCount_URL  \\\n",
              "36702           0  ...             0.529412               19   \n",
              "36703           0  ...             0.187500               23   \n",
              "36704           0  ...             0.377778               26   \n",
              "36705           0  ...             0.284091               14   \n",
              "36706           0  ...             0.365079               24   \n",
              "\n",
              "       SymbolCount_Domain  SymbolCount_Directoryname  SymbolCount_FileName  \\\n",
              "36702                   3                         11                     3   \n",
              "36703                   3                          2                    16   \n",
              "36704                   2                         14                     8   \n",
              "36705                   2                          1                     9   \n",
              "36706                   2                         13                     7   \n",
              "\n",
              "       SymbolCount_Extension  SymbolCount_Afterpath  Entropy_URL  \\\n",
              "36702                      2                      7     0.690555   \n",
              "36703                     15                     -1     0.665492   \n",
              "36704                      7                      9     0.656807   \n",
              "36705                      8                      3     0.725963   \n",
              "36706                      6                      7     0.674351   \n",
              "\n",
              "       Entropy_Domain  URL_Type_obf_Type  \n",
              "36702        0.791265               spam  \n",
              "36703        0.820010               spam  \n",
              "36704        0.801139               spam  \n",
              "36705        0.897617               spam  \n",
              "36706        0.801139               spam  \n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f2zcKS_DAGcx"
      },
      "source": [
        "  ## Experimenting with Final Dataset/All.csv\n",
        "  \n",
        "  #### Total Samples for each Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NuFeBCWJABj9",
        "outputId": "a855187a-4649-478b-c6fe-c1e028192825",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "label = 'URL_Type_obf_Type'\n",
        "lblTypes=set(df1[label])\n",
        "for lbl in lblTypes:\n",
        "    print('| {} | {} |'.format(lbl, len(df1[df1[label] == lbl].index)))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| spam | 6698 |\n",
            "| Defacement | 7930 |\n",
            "| malware | 6711 |\n",
            "| phishing | 7577 |\n",
            "| benign | 7781 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KlTOWuf8A_Yy",
        "colab": {}
      },
      "source": [
        "dataPath = 'FinalDataset'\n",
        "dep_var = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1keV97eplxW",
        "colab_type": "text"
      },
      "source": [
        "### Cast column values to float\n",
        "> Values in this column register as object type, which isn't valid for testing, so cast them to float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2nFBzqafCKkG",
        "colab": {}
      },
      "source": [
        "df1.argPathRatio = df1['argPathRatio'].astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaUqSRUiItig"
      },
      "source": [
        "## Experimenting with Tensorflow Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4c-GxIVIIZAR"
      },
      "source": [
        "#### Globals for Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x-fMOCE2I7MI",
        "colab": {}
      },
      "source": [
        "dataFile = 'All.csv'\n",
        "epochs=100\n",
        "batch_size=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxNlAPk2plxh",
        "colab_type": "text"
      },
      "source": [
        "#### Show uncoded label column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfKJek8splxi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f8deb5b1-2b39-41f4-f874-bd480140950e"
      },
      "source": [
        "df1[dep_var]"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Defacement\n",
              "1        Defacement\n",
              "2        Defacement\n",
              "3        Defacement\n",
              "4        Defacement\n",
              "5        Defacement\n",
              "6        Defacement\n",
              "7        Defacement\n",
              "8        Defacement\n",
              "9        Defacement\n",
              "10       Defacement\n",
              "11       Defacement\n",
              "12       Defacement\n",
              "13       Defacement\n",
              "14       Defacement\n",
              "15       Defacement\n",
              "16       Defacement\n",
              "17       Defacement\n",
              "18       Defacement\n",
              "19       Defacement\n",
              "20       Defacement\n",
              "21       Defacement\n",
              "22       Defacement\n",
              "23       Defacement\n",
              "24       Defacement\n",
              "25       Defacement\n",
              "26       Defacement\n",
              "27       Defacement\n",
              "28       Defacement\n",
              "29       Defacement\n",
              "            ...    \n",
              "36677          spam\n",
              "36678          spam\n",
              "36679          spam\n",
              "36680          spam\n",
              "36681          spam\n",
              "36682          spam\n",
              "36683          spam\n",
              "36684          spam\n",
              "36685          spam\n",
              "36686          spam\n",
              "36687          spam\n",
              "36688          spam\n",
              "36689          spam\n",
              "36690          spam\n",
              "36691          spam\n",
              "36692          spam\n",
              "36693          spam\n",
              "36694          spam\n",
              "36695          spam\n",
              "36696          spam\n",
              "36697          spam\n",
              "36698          spam\n",
              "36699          spam\n",
              "36700          spam\n",
              "36701          spam\n",
              "36702          spam\n",
              "36703          spam\n",
              "36704          spam\n",
              "36705          spam\n",
              "36706          spam\n",
              "Name: URL_Type_obf_Type, Length: 36697, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ED-PijliFjJL"
      },
      "source": [
        "#### Logging Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCau-nq_plxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time_gen = int(time.time())\n",
        "\n",
        "global model_name\n",
        "model_name = f\"{dataFile}_{time_gen}\"\n",
        "\n",
        "tensorboard = TensorBoard(log_dir='keras_tensorflow_logs/{}'.format(model_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikcFoxzcplxt",
        "colab_type": "text"
      },
      "source": [
        "#### Random seed for splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41We5rvNplxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6p8ArvBplxx",
        "colab_type": "text"
      },
      "source": [
        "#### Function to determine train and validation indexes, and fit the data to the model we constructed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "y8TPcnYhFl_F",
        "colab": {}
      },
      "source": [
        "def experiment(dataframe):\n",
        "    \n",
        "    #10-fold cross validation, choosing random indices for training and validation\n",
        "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "    \n",
        "    # Encode the label column for model fitting\n",
        "    encoded_y = dataframe.copy()\n",
        "    encoded_y = encode_labels(encoded_y)\n",
        "    \n",
        "    # X is our data/features to train the model with\n",
        "    X=StandardScaler().fit_transform(dataframe.drop(dep_var, axis=1))\n",
        "    \n",
        "    \n",
        "    # Y is our labels to classify the data\n",
        "    y=LabelEncoder().fit_transform(dataframe[dep_var].values)\n",
        "\n",
        "    for index, (train_indices, val_indices) in enumerate(kfold.split(X, y)):\n",
        "        \n",
        "        xtrain, xval = X[train_indices], X[val_indices]\n",
        "        ytrain, yval = encoded_y[train_indices], encoded_y[val_indices]\n",
        "\n",
        "        inputDim=xtrain.shape[1]\n",
        "        outputDim=ytrain.shape[1]\n",
        "        print(\"Running fold #\" + str(index+1))\n",
        "\n",
        "        model = baseline_model(inputDim,batch_size,outputDim)\n",
        "        history = model.fit(xtrain, ytrain, epochs=epochs, validation_data=(xval,yval), callbacks=[tensorboard, early_stop], batch_size=batch_size)\n",
        "        \n",
        "    return model, history, X, encoded_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxvO-5wqplx1",
        "colab_type": "text"
      },
      "source": [
        "### Run the experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDKDVrhZplx2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b25b5773-55e0-4dd8-e08f-0144c5eab751"
      },
      "source": [
        "categorical_df = df1.copy()\n",
        "model, history, X , encoded_y = experiment(categorical_df)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running fold #1\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33025 samples, validate on 3672 samples\n",
            "Epoch 1/100\n",
            "33025/33025 [==============================] - 2s 72us/sample - loss: 1.0072 - acc: 0.6356 - val_loss: 0.5122 - val_acc: 0.8252\n",
            "Epoch 2/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.6712 - acc: 0.7573 - val_loss: 0.4147 - val_acc: 0.8546\n",
            "Epoch 3/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.5741 - acc: 0.7911 - val_loss: 0.3601 - val_acc: 0.8745\n",
            "Epoch 4/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.5179 - acc: 0.8138 - val_loss: 0.3297 - val_acc: 0.8922\n",
            "Epoch 5/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.4794 - acc: 0.8299 - val_loss: 0.2999 - val_acc: 0.9080\n",
            "Epoch 6/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.4461 - acc: 0.8392 - val_loss: 0.2848 - val_acc: 0.9082\n",
            "Epoch 7/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.4260 - acc: 0.8500 - val_loss: 0.2722 - val_acc: 0.9082\n",
            "Epoch 8/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.4081 - acc: 0.8564 - val_loss: 0.2600 - val_acc: 0.9131\n",
            "Epoch 9/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.3922 - acc: 0.8623 - val_loss: 0.2447 - val_acc: 0.9123\n",
            "Epoch 10/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.3814 - acc: 0.8640 - val_loss: 0.2305 - val_acc: 0.9208\n",
            "Epoch 11/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.3652 - acc: 0.8725 - val_loss: 0.2254 - val_acc: 0.9224\n",
            "Epoch 12/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.3525 - acc: 0.8755 - val_loss: 0.2204 - val_acc: 0.9254\n",
            "Epoch 13/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.3476 - acc: 0.8801 - val_loss: 0.2107 - val_acc: 0.9270\n",
            "Epoch 14/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.3426 - acc: 0.8782 - val_loss: 0.2082 - val_acc: 0.9262\n",
            "Epoch 15/100\n",
            "33025/33025 [==============================] - 1s 44us/sample - loss: 0.3297 - acc: 0.8856 - val_loss: 0.2007 - val_acc: 0.9316\n",
            "Epoch 16/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.3310 - acc: 0.8851 - val_loss: 0.1951 - val_acc: 0.9349\n",
            "Epoch 17/100\n",
            "33025/33025 [==============================] - 2s 45us/sample - loss: 0.3236 - acc: 0.8881 - val_loss: 0.1927 - val_acc: 0.9357\n",
            "Epoch 18/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.3174 - acc: 0.8905 - val_loss: 0.1962 - val_acc: 0.9300\n",
            "Epoch 19/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.3139 - acc: 0.8921 - val_loss: 0.1908 - val_acc: 0.9352\n",
            "Epoch 20/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.3015 - acc: 0.8955 - val_loss: 0.1815 - val_acc: 0.9385\n",
            "Epoch 21/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.3005 - acc: 0.8966 - val_loss: 0.1847 - val_acc: 0.9368\n",
            "Epoch 22/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.3005 - acc: 0.8979 - val_loss: 0.1769 - val_acc: 0.9355\n",
            "Epoch 23/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2949 - acc: 0.8990 - val_loss: 0.1777 - val_acc: 0.9387\n",
            "Epoch 24/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2941 - acc: 0.9002 - val_loss: 0.1740 - val_acc: 0.9409\n",
            "Epoch 25/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2894 - acc: 0.9004 - val_loss: 0.1789 - val_acc: 0.9371\n",
            "Epoch 26/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2899 - acc: 0.9017 - val_loss: 0.1688 - val_acc: 0.9414\n",
            "Epoch 27/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2838 - acc: 0.9019 - val_loss: 0.1655 - val_acc: 0.9414\n",
            "Epoch 28/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2894 - acc: 0.9013 - val_loss: 0.1733 - val_acc: 0.9368\n",
            "Epoch 29/100\n",
            "33025/33025 [==============================] - 2s 48us/sample - loss: 0.2834 - acc: 0.9028 - val_loss: 0.1709 - val_acc: 0.9379\n",
            "Epoch 30/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2797 - acc: 0.9065 - val_loss: 0.1674 - val_acc: 0.9382\n",
            "Epoch 31/100\n",
            "33025/33025 [==============================] - 2s 48us/sample - loss: 0.2759 - acc: 0.9062 - val_loss: 0.1667 - val_acc: 0.9428\n",
            "Epoch 32/100\n",
            "33025/33025 [==============================] - 2s 48us/sample - loss: 0.2705 - acc: 0.9081 - val_loss: 0.1643 - val_acc: 0.9453\n",
            "Epoch 33/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2724 - acc: 0.9073 - val_loss: 0.1626 - val_acc: 0.9412\n",
            "Epoch 34/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2759 - acc: 0.9075 - val_loss: 0.1633 - val_acc: 0.9417\n",
            "Epoch 35/100\n",
            "33025/33025 [==============================] - 2s 48us/sample - loss: 0.2695 - acc: 0.9074 - val_loss: 0.1539 - val_acc: 0.9420\n",
            "Epoch 36/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2694 - acc: 0.9072 - val_loss: 0.1560 - val_acc: 0.9455\n",
            "Epoch 37/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2704 - acc: 0.9085 - val_loss: 0.1553 - val_acc: 0.9466\n",
            "Epoch 38/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2674 - acc: 0.9090 - val_loss: 0.1567 - val_acc: 0.9458\n",
            "Epoch 39/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2627 - acc: 0.9110 - val_loss: 0.1515 - val_acc: 0.9458\n",
            "Epoch 40/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2614 - acc: 0.9097 - val_loss: 0.1524 - val_acc: 0.9485\n",
            "Epoch 41/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2644 - acc: 0.9096 - val_loss: 0.1504 - val_acc: 0.9444\n",
            "Epoch 42/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2593 - acc: 0.9137 - val_loss: 0.1488 - val_acc: 0.9483\n",
            "Epoch 43/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2538 - acc: 0.9129 - val_loss: 0.1469 - val_acc: 0.9472\n",
            "Epoch 44/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.2616 - acc: 0.9126 - val_loss: 0.1539 - val_acc: 0.9453\n",
            "Epoch 45/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.2515 - acc: 0.9148 - val_loss: 0.1522 - val_acc: 0.9461\n",
            "Epoch 46/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2594 - acc: 0.9129 - val_loss: 0.1529 - val_acc: 0.9461\n",
            "Epoch 47/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2485 - acc: 0.9147 - val_loss: 0.1424 - val_acc: 0.9480\n",
            "Epoch 48/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2521 - acc: 0.9140 - val_loss: 0.1475 - val_acc: 0.9480\n",
            "Epoch 49/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2463 - acc: 0.9166 - val_loss: 0.1458 - val_acc: 0.9477\n",
            "Epoch 50/100\n",
            "33025/33025 [==============================] - 2s 48us/sample - loss: 0.2470 - acc: 0.9155 - val_loss: 0.1434 - val_acc: 0.9474\n",
            "Epoch 51/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.2445 - acc: 0.9179 - val_loss: 0.1418 - val_acc: 0.9464\n",
            "Epoch 52/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2485 - acc: 0.9162 - val_loss: 0.1449 - val_acc: 0.9480\n",
            "Epoch 53/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2446 - acc: 0.9171 - val_loss: 0.1376 - val_acc: 0.9537\n",
            "Epoch 54/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2448 - acc: 0.9149 - val_loss: 0.1360 - val_acc: 0.9521\n",
            "Epoch 55/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2422 - acc: 0.9171 - val_loss: 0.1409 - val_acc: 0.9493\n",
            "Epoch 56/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2412 - acc: 0.9167 - val_loss: 0.1429 - val_acc: 0.9504\n",
            "Epoch 57/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2429 - acc: 0.9170 - val_loss: 0.1374 - val_acc: 0.9502\n",
            "Epoch 58/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2436 - acc: 0.9176 - val_loss: 0.1479 - val_acc: 0.9472\n",
            "Epoch 59/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2410 - acc: 0.9189 - val_loss: 0.1355 - val_acc: 0.9540\n",
            "Epoch 60/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.2365 - acc: 0.9187 - val_loss: 0.1436 - val_acc: 0.9526\n",
            "Epoch 61/100\n",
            "33025/33025 [==============================] - 2s 45us/sample - loss: 0.2423 - acc: 0.9180 - val_loss: 0.1373 - val_acc: 0.9507\n",
            "Epoch 62/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2380 - acc: 0.9188 - val_loss: 0.1418 - val_acc: 0.9496\n",
            "Epoch 63/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2378 - acc: 0.9192 - val_loss: 0.1398 - val_acc: 0.9534\n",
            "Epoch 64/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.2396 - acc: 0.9189 - val_loss: 0.1344 - val_acc: 0.9534\n",
            "Epoch 65/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.2398 - acc: 0.9202 - val_loss: 0.1390 - val_acc: 0.9502\n",
            "Epoch 66/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2363 - acc: 0.9205 - val_loss: 0.1318 - val_acc: 0.9559\n",
            "Epoch 67/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2391 - acc: 0.9181 - val_loss: 0.1372 - val_acc: 0.9518\n",
            "Epoch 68/100\n",
            "33025/33025 [==============================] - 2s 46us/sample - loss: 0.2333 - acc: 0.9213 - val_loss: 0.1379 - val_acc: 0.9507\n",
            "Epoch 69/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2324 - acc: 0.9212 - val_loss: 0.1335 - val_acc: 0.9548\n",
            "Epoch 70/100\n",
            "33025/33025 [==============================] - 2s 48us/sample - loss: 0.2380 - acc: 0.9185 - val_loss: 0.1370 - val_acc: 0.9515\n",
            "Epoch 71/100\n",
            "33025/33025 [==============================] - 2s 47us/sample - loss: 0.2307 - acc: 0.9222 - val_loss: 0.1359 - val_acc: 0.9518\n",
            "Running fold #2\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 77us/sample - loss: 1.0273 - acc: 0.6351 - val_loss: 0.5427 - val_acc: 0.8174\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.6859 - acc: 0.7512 - val_loss: 0.4507 - val_acc: 0.8455\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.5892 - acc: 0.7860 - val_loss: 0.3928 - val_acc: 0.8578\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.5282 - acc: 0.8099 - val_loss: 0.3515 - val_acc: 0.8744\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.4944 - acc: 0.8224 - val_loss: 0.3239 - val_acc: 0.8896\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4573 - acc: 0.8380 - val_loss: 0.3063 - val_acc: 0.8924\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.4372 - acc: 0.8432 - val_loss: 0.2822 - val_acc: 0.9005\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.4149 - acc: 0.8535 - val_loss: 0.2675 - val_acc: 0.9052\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3999 - acc: 0.8591 - val_loss: 0.2595 - val_acc: 0.9128\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3801 - acc: 0.8654 - val_loss: 0.2454 - val_acc: 0.9153\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3734 - acc: 0.8664 - val_loss: 0.2387 - val_acc: 0.9196\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3629 - acc: 0.8729 - val_loss: 0.2261 - val_acc: 0.9253\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3598 - acc: 0.8748 - val_loss: 0.2238 - val_acc: 0.9234\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3462 - acc: 0.8805 - val_loss: 0.2174 - val_acc: 0.9253\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3413 - acc: 0.8820 - val_loss: 0.2077 - val_acc: 0.9335\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3345 - acc: 0.8822 - val_loss: 0.2125 - val_acc: 0.9302\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3273 - acc: 0.8867 - val_loss: 0.2028 - val_acc: 0.9335\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3335 - acc: 0.8828 - val_loss: 0.1988 - val_acc: 0.9343\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3324 - acc: 0.8856 - val_loss: 0.2050 - val_acc: 0.9283\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3174 - acc: 0.8903 - val_loss: 0.1915 - val_acc: 0.9384\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3090 - acc: 0.8933 - val_loss: 0.1915 - val_acc: 0.9376\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3098 - acc: 0.8915 - val_loss: 0.1949 - val_acc: 0.9351\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3120 - acc: 0.8919 - val_loss: 0.1926 - val_acc: 0.9360\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3055 - acc: 0.8942 - val_loss: 0.1852 - val_acc: 0.9371\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3010 - acc: 0.8952 - val_loss: 0.1790 - val_acc: 0.9409\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2907 - acc: 0.9001 - val_loss: 0.1783 - val_acc: 0.9398\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3012 - acc: 0.8952 - val_loss: 0.1743 - val_acc: 0.9403\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2915 - acc: 0.9000 - val_loss: 0.1782 - val_acc: 0.9411\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2913 - acc: 0.9006 - val_loss: 0.1715 - val_acc: 0.9452\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2868 - acc: 0.9038 - val_loss: 0.1730 - val_acc: 0.9420\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2824 - acc: 0.9033 - val_loss: 0.1654 - val_acc: 0.9439\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2818 - acc: 0.9022 - val_loss: 0.1665 - val_acc: 0.9466\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2844 - acc: 0.9036 - val_loss: 0.1702 - val_acc: 0.9409\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2760 - acc: 0.9066 - val_loss: 0.1677 - val_acc: 0.9447\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2831 - acc: 0.9008 - val_loss: 0.1672 - val_acc: 0.9420\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2748 - acc: 0.9048 - val_loss: 0.1664 - val_acc: 0.9433\n",
            "Running fold #3\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 79us/sample - loss: 0.9892 - acc: 0.6429 - val_loss: 0.5381 - val_acc: 0.8109\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.6715 - acc: 0.7557 - val_loss: 0.4375 - val_acc: 0.8447\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.5774 - acc: 0.7914 - val_loss: 0.3778 - val_acc: 0.8646\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.5184 - acc: 0.8121 - val_loss: 0.3501 - val_acc: 0.8757\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4839 - acc: 0.8285 - val_loss: 0.3279 - val_acc: 0.8798\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.4504 - acc: 0.8380 - val_loss: 0.2998 - val_acc: 0.8935\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4344 - acc: 0.8443 - val_loss: 0.2824 - val_acc: 0.9041\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.4076 - acc: 0.8555 - val_loss: 0.2711 - val_acc: 0.9030\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3975 - acc: 0.8588 - val_loss: 0.2594 - val_acc: 0.9057\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3817 - acc: 0.8642 - val_loss: 0.2475 - val_acc: 0.9180\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3723 - acc: 0.8693 - val_loss: 0.2434 - val_acc: 0.9125\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3653 - acc: 0.8725 - val_loss: 0.2324 - val_acc: 0.9196\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3488 - acc: 0.8775 - val_loss: 0.2336 - val_acc: 0.9125\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3398 - acc: 0.8827 - val_loss: 0.2258 - val_acc: 0.9281\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3402 - acc: 0.8827 - val_loss: 0.2188 - val_acc: 0.9234\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3296 - acc: 0.8862 - val_loss: 0.2210 - val_acc: 0.9243\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3253 - acc: 0.8863 - val_loss: 0.2079 - val_acc: 0.9278\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3185 - acc: 0.8895 - val_loss: 0.2067 - val_acc: 0.9240\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3194 - acc: 0.8889 - val_loss: 0.2024 - val_acc: 0.9300\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3138 - acc: 0.8899 - val_loss: 0.2012 - val_acc: 0.9341\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3192 - acc: 0.8905 - val_loss: 0.2065 - val_acc: 0.9240\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3078 - acc: 0.8942 - val_loss: 0.2014 - val_acc: 0.9281\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3135 - acc: 0.8926 - val_loss: 0.1944 - val_acc: 0.9349\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3026 - acc: 0.8966 - val_loss: 0.1951 - val_acc: 0.9343\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2940 - acc: 0.8993 - val_loss: 0.1876 - val_acc: 0.9341\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2902 - acc: 0.8991 - val_loss: 0.1862 - val_acc: 0.9327\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2922 - acc: 0.8989 - val_loss: 0.1899 - val_acc: 0.9332\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2904 - acc: 0.9003 - val_loss: 0.1775 - val_acc: 0.9371\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2897 - acc: 0.8997 - val_loss: 0.1767 - val_acc: 0.9406\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.2811 - acc: 0.9028 - val_loss: 0.1805 - val_acc: 0.9384\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2877 - acc: 0.9031 - val_loss: 0.1799 - val_acc: 0.9365\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2893 - acc: 0.9009 - val_loss: 0.1825 - val_acc: 0.9392\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2769 - acc: 0.9057 - val_loss: 0.1775 - val_acc: 0.9406\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2820 - acc: 0.9049 - val_loss: 0.1753 - val_acc: 0.9403\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2697 - acc: 0.9071 - val_loss: 0.1784 - val_acc: 0.9379\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2710 - acc: 0.9079 - val_loss: 0.1774 - val_acc: 0.9379\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2801 - acc: 0.9049 - val_loss: 0.1712 - val_acc: 0.9422\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2663 - acc: 0.9102 - val_loss: 0.1725 - val_acc: 0.9401\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2700 - acc: 0.9070 - val_loss: 0.1715 - val_acc: 0.9411\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 45us/sample - loss: 0.2612 - acc: 0.9101 - val_loss: 0.1653 - val_acc: 0.9425\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2640 - acc: 0.9100 - val_loss: 0.1693 - val_acc: 0.9398\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2636 - acc: 0.9099 - val_loss: 0.1718 - val_acc: 0.9420\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2590 - acc: 0.9104 - val_loss: 0.1686 - val_acc: 0.9420\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2642 - acc: 0.9087 - val_loss: 0.1719 - val_acc: 0.9387\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2628 - acc: 0.9111 - val_loss: 0.1656 - val_acc: 0.9420\n",
            "Running fold #4\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 85us/sample - loss: 1.0030 - acc: 0.6368 - val_loss: 0.5645 - val_acc: 0.8003\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.6744 - acc: 0.7525 - val_loss: 0.4564 - val_acc: 0.8414\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.5749 - acc: 0.7930 - val_loss: 0.3994 - val_acc: 0.8605\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.5259 - acc: 0.8102 - val_loss: 0.3584 - val_acc: 0.8752\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4864 - acc: 0.8288 - val_loss: 0.3341 - val_acc: 0.8779\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.4552 - acc: 0.8369 - val_loss: 0.3096 - val_acc: 0.8886\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4294 - acc: 0.8486 - val_loss: 0.2880 - val_acc: 0.8978\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.4094 - acc: 0.8541 - val_loss: 0.2732 - val_acc: 0.9057\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3988 - acc: 0.8588 - val_loss: 0.2681 - val_acc: 0.8984\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3850 - acc: 0.8649 - val_loss: 0.2609 - val_acc: 0.9060\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3736 - acc: 0.8685 - val_loss: 0.2593 - val_acc: 0.9038\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3661 - acc: 0.8722 - val_loss: 0.2394 - val_acc: 0.9155\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3599 - acc: 0.8750 - val_loss: 0.2347 - val_acc: 0.9183\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3447 - acc: 0.8797 - val_loss: 0.2290 - val_acc: 0.9196\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.3404 - acc: 0.8803 - val_loss: 0.2283 - val_acc: 0.9188\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3388 - acc: 0.8815 - val_loss: 0.2115 - val_acc: 0.9270\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3260 - acc: 0.8867 - val_loss: 0.2108 - val_acc: 0.9292\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3247 - acc: 0.8874 - val_loss: 0.2160 - val_acc: 0.9237\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3215 - acc: 0.8872 - val_loss: 0.2070 - val_acc: 0.9346\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3290 - acc: 0.8875 - val_loss: 0.2061 - val_acc: 0.9297\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3161 - acc: 0.8898 - val_loss: 0.2035 - val_acc: 0.9319\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3058 - acc: 0.8941 - val_loss: 0.1990 - val_acc: 0.9311\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3099 - acc: 0.8932 - val_loss: 0.1911 - val_acc: 0.9376\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2977 - acc: 0.8978 - val_loss: 0.1927 - val_acc: 0.9365\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2997 - acc: 0.8974 - val_loss: 0.1924 - val_acc: 0.9368\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2964 - acc: 0.8984 - val_loss: 0.1934 - val_acc: 0.9319\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3047 - acc: 0.8948 - val_loss: 0.1917 - val_acc: 0.9330\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2892 - acc: 0.8991 - val_loss: 0.1835 - val_acc: 0.9387\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2955 - acc: 0.9001 - val_loss: 0.1910 - val_acc: 0.9362\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2907 - acc: 0.8995 - val_loss: 0.1866 - val_acc: 0.9387\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2906 - acc: 0.9000 - val_loss: 0.1880 - val_acc: 0.9368\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2983 - acc: 0.8967 - val_loss: 0.1837 - val_acc: 0.9403\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2828 - acc: 0.9035 - val_loss: 0.1827 - val_acc: 0.9392\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2760 - acc: 0.9063 - val_loss: 0.1831 - val_acc: 0.9384\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2787 - acc: 0.9047 - val_loss: 0.1806 - val_acc: 0.9403\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2720 - acc: 0.9051 - val_loss: 0.1740 - val_acc: 0.9433\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2749 - acc: 0.9050 - val_loss: 0.1746 - val_acc: 0.9403\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2769 - acc: 0.9055 - val_loss: 0.1709 - val_acc: 0.9444\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2706 - acc: 0.9064 - val_loss: 0.1740 - val_acc: 0.9409\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2682 - acc: 0.9083 - val_loss: 0.1760 - val_acc: 0.9431\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2636 - acc: 0.9092 - val_loss: 0.1709 - val_acc: 0.9433\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2660 - acc: 0.9103 - val_loss: 0.1730 - val_acc: 0.9414\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2619 - acc: 0.9105 - val_loss: 0.1651 - val_acc: 0.9447\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2681 - acc: 0.9086 - val_loss: 0.1671 - val_acc: 0.9460\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2690 - acc: 0.9076 - val_loss: 0.1656 - val_acc: 0.9439\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2588 - acc: 0.9119 - val_loss: 0.1716 - val_acc: 0.9444\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2620 - acc: 0.9099 - val_loss: 0.1673 - val_acc: 0.9433\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2625 - acc: 0.9116 - val_loss: 0.1646 - val_acc: 0.9458\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2622 - acc: 0.9099 - val_loss: 0.1688 - val_acc: 0.9433\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2662 - acc: 0.9087 - val_loss: 0.1630 - val_acc: 0.9452\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2606 - acc: 0.9117 - val_loss: 0.1667 - val_acc: 0.9441\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2513 - acc: 0.9159 - val_loss: 0.1580 - val_acc: 0.9499\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2627 - acc: 0.9119 - val_loss: 0.1678 - val_acc: 0.9433\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2524 - acc: 0.9144 - val_loss: 0.1611 - val_acc: 0.9444\n",
            "Epoch 55/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2462 - acc: 0.9164 - val_loss: 0.1605 - val_acc: 0.9482\n",
            "Epoch 56/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2463 - acc: 0.9153 - val_loss: 0.1573 - val_acc: 0.9490\n",
            "Epoch 57/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2540 - acc: 0.9123 - val_loss: 0.1649 - val_acc: 0.9433\n",
            "Epoch 58/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2495 - acc: 0.9158 - val_loss: 0.1581 - val_acc: 0.9466\n",
            "Epoch 59/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2460 - acc: 0.9161 - val_loss: 0.1603 - val_acc: 0.9450\n",
            "Epoch 60/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.2448 - acc: 0.9169 - val_loss: 0.1542 - val_acc: 0.9490\n",
            "Epoch 61/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.2460 - acc: 0.9161 - val_loss: 0.1524 - val_acc: 0.9501\n",
            "Epoch 62/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2395 - acc: 0.9183 - val_loss: 0.1555 - val_acc: 0.9488\n",
            "Epoch 63/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2424 - acc: 0.9181 - val_loss: 0.1551 - val_acc: 0.9510\n",
            "Epoch 64/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2478 - acc: 0.9166 - val_loss: 0.1494 - val_acc: 0.9485\n",
            "Epoch 65/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2368 - acc: 0.9202 - val_loss: 0.1521 - val_acc: 0.9493\n",
            "Epoch 66/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2422 - acc: 0.9188 - val_loss: 0.1465 - val_acc: 0.9507\n",
            "Epoch 67/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2439 - acc: 0.9187 - val_loss: 0.1507 - val_acc: 0.9510\n",
            "Epoch 68/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2401 - acc: 0.9201 - val_loss: 0.1520 - val_acc: 0.9496\n",
            "Epoch 69/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.2350 - acc: 0.9206 - val_loss: 0.1557 - val_acc: 0.9480\n",
            "Epoch 70/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2363 - acc: 0.9211 - val_loss: 0.1489 - val_acc: 0.9512\n",
            "Epoch 71/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2384 - acc: 0.9205 - val_loss: 0.1509 - val_acc: 0.9515\n",
            "Running fold #5\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 92us/sample - loss: 1.0075 - acc: 0.6397 - val_loss: 0.5309 - val_acc: 0.8128\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.6752 - acc: 0.7536 - val_loss: 0.4386 - val_acc: 0.8504\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.5853 - acc: 0.7907 - val_loss: 0.3870 - val_acc: 0.8643\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.5268 - acc: 0.8101 - val_loss: 0.3608 - val_acc: 0.8714\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.4852 - acc: 0.8245 - val_loss: 0.3254 - val_acc: 0.8839\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.4670 - acc: 0.8332 - val_loss: 0.3162 - val_acc: 0.8864\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.4436 - acc: 0.8417 - val_loss: 0.2924 - val_acc: 0.8956\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.4206 - acc: 0.8480 - val_loss: 0.2838 - val_acc: 0.8959\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.4038 - acc: 0.8569 - val_loss: 0.2672 - val_acc: 0.9041\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3949 - acc: 0.8614 - val_loss: 0.2678 - val_acc: 0.9044\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3808 - acc: 0.8645 - val_loss: 0.2503 - val_acc: 0.9060\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3731 - acc: 0.8681 - val_loss: 0.2381 - val_acc: 0.9136\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3532 - acc: 0.8763 - val_loss: 0.2380 - val_acc: 0.9109\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3538 - acc: 0.8754 - val_loss: 0.2312 - val_acc: 0.9142\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3408 - acc: 0.8796 - val_loss: 0.2264 - val_acc: 0.9142\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3368 - acc: 0.8825 - val_loss: 0.2172 - val_acc: 0.9199\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3284 - acc: 0.8867 - val_loss: 0.2127 - val_acc: 0.9248\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3228 - acc: 0.8870 - val_loss: 0.2082 - val_acc: 0.9243\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3158 - acc: 0.8894 - val_loss: 0.2182 - val_acc: 0.9180\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.3289 - acc: 0.8864 - val_loss: 0.2063 - val_acc: 0.9229\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3241 - acc: 0.8898 - val_loss: 0.2011 - val_acc: 0.9294\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.3100 - acc: 0.8932 - val_loss: 0.2006 - val_acc: 0.9262\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.3013 - acc: 0.8966 - val_loss: 0.1980 - val_acc: 0.9316\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2977 - acc: 0.8978 - val_loss: 0.1921 - val_acc: 0.9324\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.3013 - acc: 0.8969 - val_loss: 0.1878 - val_acc: 0.9341\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2961 - acc: 0.8991 - val_loss: 0.1864 - val_acc: 0.9327\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2926 - acc: 0.8971 - val_loss: 0.1878 - val_acc: 0.9341\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2965 - acc: 0.8993 - val_loss: 0.1861 - val_acc: 0.9349\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2868 - acc: 0.9008 - val_loss: 0.1845 - val_acc: 0.9360\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2811 - acc: 0.9046 - val_loss: 0.1790 - val_acc: 0.9379\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2783 - acc: 0.9030 - val_loss: 0.1805 - val_acc: 0.9335\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2845 - acc: 0.9019 - val_loss: 0.1762 - val_acc: 0.9384\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2875 - acc: 0.9005 - val_loss: 0.1798 - val_acc: 0.9390\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2786 - acc: 0.9044 - val_loss: 0.1793 - val_acc: 0.9360\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2815 - acc: 0.9030 - val_loss: 0.1790 - val_acc: 0.9365\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2763 - acc: 0.9037 - val_loss: 0.1797 - val_acc: 0.9376\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2765 - acc: 0.9058 - val_loss: 0.1775 - val_acc: 0.9381\n",
            "Running fold #6\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 91us/sample - loss: 1.0228 - acc: 0.6348 - val_loss: 0.5246 - val_acc: 0.8177\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.6834 - acc: 0.7493 - val_loss: 0.4322 - val_acc: 0.8485\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.5855 - acc: 0.7868 - val_loss: 0.3785 - val_acc: 0.8708\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.5231 - acc: 0.8131 - val_loss: 0.3414 - val_acc: 0.8809\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.4769 - acc: 0.8287 - val_loss: 0.3133 - val_acc: 0.8842\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.4541 - acc: 0.8392 - val_loss: 0.2954 - val_acc: 0.8967\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.4246 - acc: 0.8486 - val_loss: 0.2853 - val_acc: 0.9033\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.4072 - acc: 0.8555 - val_loss: 0.2775 - val_acc: 0.9046\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3842 - acc: 0.8649 - val_loss: 0.2567 - val_acc: 0.9098\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3799 - acc: 0.8652 - val_loss: 0.2484 - val_acc: 0.9161\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3638 - acc: 0.8730 - val_loss: 0.2412 - val_acc: 0.9183\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3534 - acc: 0.8768 - val_loss: 0.2350 - val_acc: 0.9166\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3531 - acc: 0.8759 - val_loss: 0.2266 - val_acc: 0.9262\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3437 - acc: 0.8798 - val_loss: 0.2138 - val_acc: 0.9264\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3300 - acc: 0.8841 - val_loss: 0.2123 - val_acc: 0.9286\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.3312 - acc: 0.8849 - val_loss: 0.2110 - val_acc: 0.9278\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.3260 - acc: 0.8890 - val_loss: 0.2055 - val_acc: 0.9319\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.3190 - acc: 0.8893 - val_loss: 0.2083 - val_acc: 0.9292\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.3111 - acc: 0.8929 - val_loss: 0.2014 - val_acc: 0.9294\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3126 - acc: 0.8918 - val_loss: 0.1977 - val_acc: 0.9324\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.3036 - acc: 0.8949 - val_loss: 0.1925 - val_acc: 0.9387\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3003 - acc: 0.8986 - val_loss: 0.1965 - val_acc: 0.9319\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2992 - acc: 0.8970 - val_loss: 0.1883 - val_acc: 0.9368\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2984 - acc: 0.8976 - val_loss: 0.1822 - val_acc: 0.9381\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2968 - acc: 0.8980 - val_loss: 0.1883 - val_acc: 0.9379\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2905 - acc: 0.9007 - val_loss: 0.1841 - val_acc: 0.9373\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2873 - acc: 0.9010 - val_loss: 0.1827 - val_acc: 0.9371\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2930 - acc: 0.8987 - val_loss: 0.1797 - val_acc: 0.9392\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2840 - acc: 0.9033 - val_loss: 0.1784 - val_acc: 0.9414\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2773 - acc: 0.9043 - val_loss: 0.1747 - val_acc: 0.9422\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2828 - acc: 0.9017 - val_loss: 0.1778 - val_acc: 0.9409\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2763 - acc: 0.9065 - val_loss: 0.1737 - val_acc: 0.9428\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2746 - acc: 0.9053 - val_loss: 0.1671 - val_acc: 0.9471\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2712 - acc: 0.9073 - val_loss: 0.1710 - val_acc: 0.9422\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.2675 - acc: 0.9083 - val_loss: 0.1628 - val_acc: 0.9485\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2744 - acc: 0.9066 - val_loss: 0.1700 - val_acc: 0.9420\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2600 - acc: 0.9102 - val_loss: 0.1731 - val_acc: 0.9417\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2660 - acc: 0.9090 - val_loss: 0.1725 - val_acc: 0.9428\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2684 - acc: 0.9100 - val_loss: 0.1658 - val_acc: 0.9452\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2649 - acc: 0.9094 - val_loss: 0.1671 - val_acc: 0.9422\n",
            "Running fold #7\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 95us/sample - loss: 1.0133 - acc: 0.6328 - val_loss: 0.5289 - val_acc: 0.8218\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.6779 - acc: 0.7540 - val_loss: 0.4214 - val_acc: 0.8580\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.5804 - acc: 0.7892 - val_loss: 0.3683 - val_acc: 0.8708\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.5259 - acc: 0.8112 - val_loss: 0.3380 - val_acc: 0.8877\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.4799 - acc: 0.8286 - val_loss: 0.3051 - val_acc: 0.9035\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.4594 - acc: 0.8341 - val_loss: 0.2850 - val_acc: 0.9041\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.4362 - acc: 0.8436 - val_loss: 0.2779 - val_acc: 0.9087\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.4291 - acc: 0.8483 - val_loss: 0.2548 - val_acc: 0.9131\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.4017 - acc: 0.8580 - val_loss: 0.2510 - val_acc: 0.9112\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.3890 - acc: 0.8638 - val_loss: 0.2340 - val_acc: 0.9196\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3754 - acc: 0.8701 - val_loss: 0.2308 - val_acc: 0.9207\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3658 - acc: 0.8709 - val_loss: 0.2242 - val_acc: 0.9229\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3561 - acc: 0.8749 - val_loss: 0.2153 - val_acc: 0.9234\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3531 - acc: 0.8774 - val_loss: 0.2136 - val_acc: 0.9251\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.3420 - acc: 0.8823 - val_loss: 0.2038 - val_acc: 0.9305\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.3372 - acc: 0.8821 - val_loss: 0.2002 - val_acc: 0.9286\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.3293 - acc: 0.8860 - val_loss: 0.1994 - val_acc: 0.9292\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3229 - acc: 0.8872 - val_loss: 0.1943 - val_acc: 0.9343\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3207 - acc: 0.8896 - val_loss: 0.1953 - val_acc: 0.9343\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.3111 - acc: 0.8921 - val_loss: 0.1905 - val_acc: 0.9311\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.3082 - acc: 0.8930 - val_loss: 0.1868 - val_acc: 0.9351\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.3031 - acc: 0.8956 - val_loss: 0.1828 - val_acc: 0.9351\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.3046 - acc: 0.8938 - val_loss: 0.1824 - val_acc: 0.9376\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.3086 - acc: 0.8941 - val_loss: 0.1828 - val_acc: 0.9354\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3000 - acc: 0.8965 - val_loss: 0.1768 - val_acc: 0.9371\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2942 - acc: 0.8991 - val_loss: 0.1730 - val_acc: 0.9371\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2911 - acc: 0.9004 - val_loss: 0.1756 - val_acc: 0.9395\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2876 - acc: 0.9006 - val_loss: 0.1688 - val_acc: 0.9425\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2850 - acc: 0.9017 - val_loss: 0.1658 - val_acc: 0.9414\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2923 - acc: 0.8991 - val_loss: 0.1677 - val_acc: 0.9414\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2865 - acc: 0.8997 - val_loss: 0.1619 - val_acc: 0.9455\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2847 - acc: 0.9026 - val_loss: 0.1647 - val_acc: 0.9447\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2787 - acc: 0.9046 - val_loss: 0.1632 - val_acc: 0.9417\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2788 - acc: 0.9020 - val_loss: 0.1630 - val_acc: 0.9431\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2756 - acc: 0.9063 - val_loss: 0.1611 - val_acc: 0.9488\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2698 - acc: 0.9085 - val_loss: 0.1614 - val_acc: 0.9433\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2789 - acc: 0.9040 - val_loss: 0.1688 - val_acc: 0.9401\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2698 - acc: 0.9070 - val_loss: 0.1551 - val_acc: 0.9477\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.2695 - acc: 0.9080 - val_loss: 0.1572 - val_acc: 0.9425\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.2707 - acc: 0.9087 - val_loss: 0.1500 - val_acc: 0.9493\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.2638 - acc: 0.9094 - val_loss: 0.1557 - val_acc: 0.9463\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.2622 - acc: 0.9106 - val_loss: 0.1500 - val_acc: 0.9455\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.2607 - acc: 0.9124 - val_loss: 0.1498 - val_acc: 0.9485\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.2590 - acc: 0.9120 - val_loss: 0.1489 - val_acc: 0.9480\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2583 - acc: 0.9101 - val_loss: 0.1531 - val_acc: 0.9471\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2610 - acc: 0.9114 - val_loss: 0.1500 - val_acc: 0.9477\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.2612 - acc: 0.9093 - val_loss: 0.1530 - val_acc: 0.9485\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2596 - acc: 0.9105 - val_loss: 0.1531 - val_acc: 0.9458\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2545 - acc: 0.9134 - val_loss: 0.1432 - val_acc: 0.9480\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2527 - acc: 0.9143 - val_loss: 0.1477 - val_acc: 0.9469\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2644 - acc: 0.9097 - val_loss: 0.1485 - val_acc: 0.9537\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2542 - acc: 0.9150 - val_loss: 0.1494 - val_acc: 0.9512\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.2485 - acc: 0.9149 - val_loss: 0.1512 - val_acc: 0.9515\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2515 - acc: 0.9153 - val_loss: 0.1480 - val_acc: 0.9490\n",
            "Running fold #8\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 3s 93us/sample - loss: 0.9936 - acc: 0.6417 - val_loss: 0.5500 - val_acc: 0.8035\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.6605 - acc: 0.7602 - val_loss: 0.4466 - val_acc: 0.8457\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.5701 - acc: 0.7943 - val_loss: 0.3935 - val_acc: 0.8656\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.5106 - acc: 0.8151 - val_loss: 0.3553 - val_acc: 0.8768\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.4753 - acc: 0.8285 - val_loss: 0.3290 - val_acc: 0.8839\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.4461 - acc: 0.8422 - val_loss: 0.3032 - val_acc: 0.8934\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.4253 - acc: 0.8505 - val_loss: 0.2926 - val_acc: 0.8942\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.4134 - acc: 0.8541 - val_loss: 0.2819 - val_acc: 0.8992\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3940 - acc: 0.8602 - val_loss: 0.2685 - val_acc: 0.9054\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.3848 - acc: 0.8662 - val_loss: 0.2575 - val_acc: 0.9131\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.3682 - acc: 0.8708 - val_loss: 0.2461 - val_acc: 0.9193\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3640 - acc: 0.8730 - val_loss: 0.2443 - val_acc: 0.9166\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.3565 - acc: 0.8743 - val_loss: 0.2394 - val_acc: 0.9212\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.3428 - acc: 0.8797 - val_loss: 0.2319 - val_acc: 0.9210\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3405 - acc: 0.8815 - val_loss: 0.2273 - val_acc: 0.9234\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.3304 - acc: 0.8846 - val_loss: 0.2235 - val_acc: 0.9294\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3286 - acc: 0.8863 - val_loss: 0.2142 - val_acc: 0.9308\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3231 - acc: 0.8898 - val_loss: 0.2156 - val_acc: 0.9272\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.3188 - acc: 0.8895 - val_loss: 0.2163 - val_acc: 0.9305\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.3161 - acc: 0.8925 - val_loss: 0.2104 - val_acc: 0.9340\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.3089 - acc: 0.8945 - val_loss: 0.2048 - val_acc: 0.9321\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.3037 - acc: 0.8957 - val_loss: 0.2089 - val_acc: 0.9294\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.3027 - acc: 0.8961 - val_loss: 0.2037 - val_acc: 0.9327\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2950 - acc: 0.8972 - val_loss: 0.1978 - val_acc: 0.9346\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.2949 - acc: 0.8982 - val_loss: 0.1997 - val_acc: 0.9330\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2951 - acc: 0.8983 - val_loss: 0.1930 - val_acc: 0.9400\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.2945 - acc: 0.8995 - val_loss: 0.1943 - val_acc: 0.9379\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2884 - acc: 0.9004 - val_loss: 0.1881 - val_acc: 0.9384\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2926 - acc: 0.9011 - val_loss: 0.1936 - val_acc: 0.9370\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2827 - acc: 0.9036 - val_loss: 0.1844 - val_acc: 0.9409\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.2786 - acc: 0.9055 - val_loss: 0.1874 - val_acc: 0.9406\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2730 - acc: 0.9069 - val_loss: 0.1847 - val_acc: 0.9370\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.2762 - acc: 0.9045 - val_loss: 0.1811 - val_acc: 0.9414\n",
            "Epoch 34/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.2751 - acc: 0.9078 - val_loss: 0.1808 - val_acc: 0.9387\n",
            "Epoch 35/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2720 - acc: 0.9076 - val_loss: 0.1755 - val_acc: 0.9422\n",
            "Epoch 36/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2729 - acc: 0.9070 - val_loss: 0.1700 - val_acc: 0.9449\n",
            "Epoch 37/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.2653 - acc: 0.9080 - val_loss: 0.1791 - val_acc: 0.9406\n",
            "Epoch 38/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.2675 - acc: 0.9092 - val_loss: 0.1679 - val_acc: 0.9463\n",
            "Epoch 39/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.2669 - acc: 0.9108 - val_loss: 0.1746 - val_acc: 0.9444\n",
            "Epoch 40/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.2684 - acc: 0.9069 - val_loss: 0.1716 - val_acc: 0.9441\n",
            "Epoch 41/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2669 - acc: 0.9087 - val_loss: 0.1682 - val_acc: 0.9460\n",
            "Epoch 42/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2579 - acc: 0.9124 - val_loss: 0.1727 - val_acc: 0.9452\n",
            "Epoch 43/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2592 - acc: 0.9121 - val_loss: 0.1728 - val_acc: 0.9428\n",
            "Running fold #9\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33029 samples, validate on 3668 samples\n",
            "Epoch 1/100\n",
            "33029/33029 [==============================] - 3s 95us/sample - loss: 0.9834 - acc: 0.6455 - val_loss: 0.5471 - val_acc: 0.8105\n",
            "Epoch 2/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.6594 - acc: 0.7584 - val_loss: 0.4336 - val_acc: 0.8571\n",
            "Epoch 3/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.5676 - acc: 0.7947 - val_loss: 0.3830 - val_acc: 0.8670\n",
            "Epoch 4/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.5110 - acc: 0.8173 - val_loss: 0.3470 - val_acc: 0.8773\n",
            "Epoch 5/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.4759 - acc: 0.8285 - val_loss: 0.3135 - val_acc: 0.8918\n",
            "Epoch 6/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.4505 - acc: 0.8397 - val_loss: 0.2954 - val_acc: 0.8959\n",
            "Epoch 7/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.4233 - acc: 0.8500 - val_loss: 0.2804 - val_acc: 0.9013\n",
            "Epoch 8/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.4056 - acc: 0.8544 - val_loss: 0.2673 - val_acc: 0.9051\n",
            "Epoch 9/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3954 - acc: 0.8621 - val_loss: 0.2553 - val_acc: 0.9049\n",
            "Epoch 10/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3780 - acc: 0.8668 - val_loss: 0.2397 - val_acc: 0.9138\n",
            "Epoch 11/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3629 - acc: 0.8711 - val_loss: 0.2307 - val_acc: 0.9179\n",
            "Epoch 12/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.3570 - acc: 0.8747 - val_loss: 0.2214 - val_acc: 0.9291\n",
            "Epoch 13/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3462 - acc: 0.8790 - val_loss: 0.2165 - val_acc: 0.9283\n",
            "Epoch 14/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3425 - acc: 0.8810 - val_loss: 0.2100 - val_acc: 0.9275\n",
            "Epoch 15/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3314 - acc: 0.8849 - val_loss: 0.2018 - val_acc: 0.9324\n",
            "Epoch 16/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.3267 - acc: 0.8888 - val_loss: 0.1942 - val_acc: 0.9348\n",
            "Epoch 17/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3227 - acc: 0.8883 - val_loss: 0.1931 - val_acc: 0.9343\n",
            "Epoch 18/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3169 - acc: 0.8899 - val_loss: 0.1932 - val_acc: 0.9354\n",
            "Epoch 19/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3117 - acc: 0.8925 - val_loss: 0.1860 - val_acc: 0.9381\n",
            "Epoch 20/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3102 - acc: 0.8912 - val_loss: 0.1916 - val_acc: 0.9348\n",
            "Epoch 21/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.3048 - acc: 0.8963 - val_loss: 0.1846 - val_acc: 0.9348\n",
            "Epoch 22/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3005 - acc: 0.8976 - val_loss: 0.1775 - val_acc: 0.9395\n",
            "Epoch 23/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2969 - acc: 0.8984 - val_loss: 0.1811 - val_acc: 0.9365\n",
            "Epoch 24/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2928 - acc: 0.8992 - val_loss: 0.1760 - val_acc: 0.9400\n",
            "Epoch 25/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2838 - acc: 0.9052 - val_loss: 0.1694 - val_acc: 0.9444\n",
            "Epoch 26/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2795 - acc: 0.9042 - val_loss: 0.1692 - val_acc: 0.9411\n",
            "Epoch 27/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2907 - acc: 0.9008 - val_loss: 0.1673 - val_acc: 0.9436\n",
            "Epoch 28/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2787 - acc: 0.9048 - val_loss: 0.1632 - val_acc: 0.9455\n",
            "Epoch 29/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2787 - acc: 0.9043 - val_loss: 0.1675 - val_acc: 0.9427\n",
            "Epoch 30/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2762 - acc: 0.9044 - val_loss: 0.1664 - val_acc: 0.9414\n",
            "Epoch 31/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2769 - acc: 0.9063 - val_loss: 0.1620 - val_acc: 0.9457\n",
            "Epoch 32/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2729 - acc: 0.9065 - val_loss: 0.1629 - val_acc: 0.9436\n",
            "Epoch 33/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2775 - acc: 0.9068 - val_loss: 0.1594 - val_acc: 0.9449\n",
            "Epoch 34/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2685 - acc: 0.9101 - val_loss: 0.1680 - val_acc: 0.9392\n",
            "Epoch 35/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2711 - acc: 0.9078 - val_loss: 0.1538 - val_acc: 0.9485\n",
            "Epoch 36/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2673 - acc: 0.9081 - val_loss: 0.1526 - val_acc: 0.9496\n",
            "Epoch 37/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2728 - acc: 0.9067 - val_loss: 0.1544 - val_acc: 0.9490\n",
            "Epoch 38/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2594 - acc: 0.9117 - val_loss: 0.1506 - val_acc: 0.9485\n",
            "Epoch 39/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2678 - acc: 0.9073 - val_loss: 0.1515 - val_acc: 0.9496\n",
            "Epoch 40/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2617 - acc: 0.9114 - val_loss: 0.1577 - val_acc: 0.9468\n",
            "Epoch 41/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2658 - acc: 0.9093 - val_loss: 0.1446 - val_acc: 0.9515\n",
            "Epoch 42/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2573 - acc: 0.9127 - val_loss: 0.1477 - val_acc: 0.9507\n",
            "Epoch 43/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2530 - acc: 0.9130 - val_loss: 0.1483 - val_acc: 0.9512\n",
            "Epoch 44/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2578 - acc: 0.9145 - val_loss: 0.1445 - val_acc: 0.9517\n",
            "Epoch 45/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2547 - acc: 0.9126 - val_loss: 0.1472 - val_acc: 0.9496\n",
            "Epoch 46/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2518 - acc: 0.9158 - val_loss: 0.1477 - val_acc: 0.9512\n",
            "Epoch 47/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2486 - acc: 0.9168 - val_loss: 0.1420 - val_acc: 0.9528\n",
            "Epoch 48/100\n",
            "33029/33029 [==============================] - 2s 59us/sample - loss: 0.2489 - acc: 0.9153 - val_loss: 0.1461 - val_acc: 0.9517\n",
            "Epoch 49/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2540 - acc: 0.9133 - val_loss: 0.1458 - val_acc: 0.9501\n",
            "Epoch 50/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2396 - acc: 0.9183 - val_loss: 0.1419 - val_acc: 0.9509\n",
            "Epoch 51/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2464 - acc: 0.9178 - val_loss: 0.1464 - val_acc: 0.9547\n",
            "Epoch 52/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2460 - acc: 0.9172 - val_loss: 0.1425 - val_acc: 0.9512\n",
            "Epoch 53/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2436 - acc: 0.9184 - val_loss: 0.1382 - val_acc: 0.9556\n",
            "Epoch 54/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2446 - acc: 0.9185 - val_loss: 0.1428 - val_acc: 0.9537\n",
            "Epoch 55/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2490 - acc: 0.9153 - val_loss: 0.1425 - val_acc: 0.9531\n",
            "Epoch 56/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2431 - acc: 0.9173 - val_loss: 0.1340 - val_acc: 0.9580\n",
            "Epoch 57/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2451 - acc: 0.9170 - val_loss: 0.1374 - val_acc: 0.9567\n",
            "Epoch 58/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2439 - acc: 0.9190 - val_loss: 0.1375 - val_acc: 0.9542\n",
            "Epoch 59/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2408 - acc: 0.9170 - val_loss: 0.1385 - val_acc: 0.9545\n",
            "Epoch 60/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2439 - acc: 0.9178 - val_loss: 0.1360 - val_acc: 0.9561\n",
            "Epoch 61/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2376 - acc: 0.9190 - val_loss: 0.1391 - val_acc: 0.9523\n",
            "Running fold #10\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33029 samples, validate on 3668 samples\n",
            "Epoch 1/100\n",
            "33029/33029 [==============================] - 3s 99us/sample - loss: 0.9860 - acc: 0.6445 - val_loss: 0.5226 - val_acc: 0.8100\n",
            "Epoch 2/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.6655 - acc: 0.7584 - val_loss: 0.4219 - val_acc: 0.8473\n",
            "Epoch 3/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.5689 - acc: 0.7928 - val_loss: 0.3675 - val_acc: 0.8667\n",
            "Epoch 4/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.5205 - acc: 0.8140 - val_loss: 0.3317 - val_acc: 0.8855\n",
            "Epoch 5/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.4864 - acc: 0.8255 - val_loss: 0.3083 - val_acc: 0.8931\n",
            "Epoch 6/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.4531 - acc: 0.8388 - val_loss: 0.2807 - val_acc: 0.9021\n",
            "Epoch 7/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.4344 - acc: 0.8458 - val_loss: 0.2691 - val_acc: 0.9024\n",
            "Epoch 8/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.4078 - acc: 0.8554 - val_loss: 0.2504 - val_acc: 0.9109\n",
            "Epoch 9/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.3899 - acc: 0.8627 - val_loss: 0.2434 - val_acc: 0.9166\n",
            "Epoch 10/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.3875 - acc: 0.8643 - val_loss: 0.2360 - val_acc: 0.9130\n",
            "Epoch 11/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.3739 - acc: 0.8695 - val_loss: 0.2234 - val_acc: 0.9253\n",
            "Epoch 12/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.3597 - acc: 0.8751 - val_loss: 0.2143 - val_acc: 0.9226\n",
            "Epoch 13/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.3529 - acc: 0.8772 - val_loss: 0.2148 - val_acc: 0.9248\n",
            "Epoch 14/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.3463 - acc: 0.8784 - val_loss: 0.2017 - val_acc: 0.9338\n",
            "Epoch 15/100\n",
            "33029/33029 [==============================] - 2s 57us/sample - loss: 0.3428 - acc: 0.8792 - val_loss: 0.1979 - val_acc: 0.9308\n",
            "Epoch 16/100\n",
            "33029/33029 [==============================] - 2s 60us/sample - loss: 0.3307 - acc: 0.8836 - val_loss: 0.1948 - val_acc: 0.9294\n",
            "Epoch 17/100\n",
            "33029/33029 [==============================] - 2s 57us/sample - loss: 0.3260 - acc: 0.8858 - val_loss: 0.1876 - val_acc: 0.9362\n",
            "Epoch 18/100\n",
            "33029/33029 [==============================] - 2s 57us/sample - loss: 0.3196 - acc: 0.8888 - val_loss: 0.1840 - val_acc: 0.9359\n",
            "Epoch 19/100\n",
            "33029/33029 [==============================] - 2s 56us/sample - loss: 0.3180 - acc: 0.8892 - val_loss: 0.1817 - val_acc: 0.9359\n",
            "Epoch 20/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.3142 - acc: 0.8921 - val_loss: 0.1790 - val_acc: 0.9395\n",
            "Epoch 21/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3170 - acc: 0.8918 - val_loss: 0.1802 - val_acc: 0.9397\n",
            "Epoch 22/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.3079 - acc: 0.8935 - val_loss: 0.1718 - val_acc: 0.9441\n",
            "Epoch 23/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3008 - acc: 0.8967 - val_loss: 0.1691 - val_acc: 0.9381\n",
            "Epoch 24/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2994 - acc: 0.8971 - val_loss: 0.1651 - val_acc: 0.9457\n",
            "Epoch 25/100\n",
            "33029/33029 [==============================] - 2s 56us/sample - loss: 0.2945 - acc: 0.8992 - val_loss: 0.1632 - val_acc: 0.9403\n",
            "Epoch 26/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2969 - acc: 0.8977 - val_loss: 0.1650 - val_acc: 0.9471\n",
            "Epoch 27/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2896 - acc: 0.8991 - val_loss: 0.1622 - val_acc: 0.9477\n",
            "Epoch 28/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2880 - acc: 0.9006 - val_loss: 0.1607 - val_acc: 0.9433\n",
            "Epoch 29/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2875 - acc: 0.8995 - val_loss: 0.1590 - val_acc: 0.9466\n",
            "Epoch 30/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2854 - acc: 0.9034 - val_loss: 0.1593 - val_acc: 0.9447\n",
            "Epoch 31/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2820 - acc: 0.9043 - val_loss: 0.1581 - val_acc: 0.9449\n",
            "Epoch 32/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2799 - acc: 0.9030 - val_loss: 0.1571 - val_acc: 0.9444\n",
            "Epoch 33/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2773 - acc: 0.9049 - val_loss: 0.1518 - val_acc: 0.9455\n",
            "Epoch 34/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2786 - acc: 0.9031 - val_loss: 0.1511 - val_acc: 0.9479\n",
            "Epoch 35/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2754 - acc: 0.9047 - val_loss: 0.1503 - val_acc: 0.9493\n",
            "Epoch 36/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2755 - acc: 0.9063 - val_loss: 0.1529 - val_acc: 0.9460\n",
            "Epoch 37/100\n",
            "33029/33029 [==============================] - 2s 59us/sample - loss: 0.2816 - acc: 0.9043 - val_loss: 0.1532 - val_acc: 0.9493\n",
            "Epoch 38/100\n",
            "33029/33029 [==============================] - 2s 62us/sample - loss: 0.2681 - acc: 0.9085 - val_loss: 0.1487 - val_acc: 0.9493\n",
            "Epoch 39/100\n",
            "33029/33029 [==============================] - 2s 61us/sample - loss: 0.2724 - acc: 0.9060 - val_loss: 0.1500 - val_acc: 0.9537\n",
            "Epoch 40/100\n",
            "33029/33029 [==============================] - 2s 57us/sample - loss: 0.2673 - acc: 0.9085 - val_loss: 0.1437 - val_acc: 0.9490\n",
            "Epoch 41/100\n",
            "33029/33029 [==============================] - 2s 66us/sample - loss: 0.2690 - acc: 0.9078 - val_loss: 0.1526 - val_acc: 0.9471\n",
            "Epoch 42/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2609 - acc: 0.9109 - val_loss: 0.1410 - val_acc: 0.9572\n",
            "Epoch 43/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2594 - acc: 0.9104 - val_loss: 0.1407 - val_acc: 0.9561\n",
            "Epoch 44/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2605 - acc: 0.9112 - val_loss: 0.1481 - val_acc: 0.9487\n",
            "Epoch 45/100\n",
            "33029/33029 [==============================] - 2s 56us/sample - loss: 0.2579 - acc: 0.9123 - val_loss: 0.1435 - val_acc: 0.9507\n",
            "Epoch 46/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2575 - acc: 0.9125 - val_loss: 0.1413 - val_acc: 0.9545\n",
            "Epoch 47/100\n",
            "33029/33029 [==============================] - 2s 56us/sample - loss: 0.2556 - acc: 0.9134 - val_loss: 0.1389 - val_acc: 0.9528\n",
            "Epoch 48/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2551 - acc: 0.9127 - val_loss: 0.1445 - val_acc: 0.9517\n",
            "Epoch 49/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2617 - acc: 0.9099 - val_loss: 0.1407 - val_acc: 0.9517\n",
            "Epoch 50/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2549 - acc: 0.9139 - val_loss: 0.1372 - val_acc: 0.9523\n",
            "Epoch 51/100\n",
            "33029/33029 [==============================] - 2s 59us/sample - loss: 0.2555 - acc: 0.9135 - val_loss: 0.1413 - val_acc: 0.9507\n",
            "Epoch 52/100\n",
            "33029/33029 [==============================] - 2s 62us/sample - loss: 0.2523 - acc: 0.9136 - val_loss: 0.1367 - val_acc: 0.9558\n",
            "Epoch 53/100\n",
            "33029/33029 [==============================] - 2s 60us/sample - loss: 0.2510 - acc: 0.9152 - val_loss: 0.1361 - val_acc: 0.9561\n",
            "Epoch 54/100\n",
            "33029/33029 [==============================] - 2s 58us/sample - loss: 0.2523 - acc: 0.9116 - val_loss: 0.1358 - val_acc: 0.9545\n",
            "Epoch 55/100\n",
            "33029/33029 [==============================] - 2s 57us/sample - loss: 0.2423 - acc: 0.9190 - val_loss: 0.1382 - val_acc: 0.9528\n",
            "Epoch 56/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2492 - acc: 0.9148 - val_loss: 0.1351 - val_acc: 0.9572\n",
            "Epoch 57/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2437 - acc: 0.9159 - val_loss: 0.1360 - val_acc: 0.9531\n",
            "Epoch 58/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2490 - acc: 0.9147 - val_loss: 0.1384 - val_acc: 0.9523\n",
            "Epoch 59/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2435 - acc: 0.9170 - val_loss: 0.1353 - val_acc: 0.9542\n",
            "Epoch 60/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2470 - acc: 0.9155 - val_loss: 0.1313 - val_acc: 0.9564\n",
            "Epoch 61/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2431 - acc: 0.9170 - val_loss: 0.1302 - val_acc: 0.9583\n",
            "Epoch 62/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2387 - acc: 0.9211 - val_loss: 0.1306 - val_acc: 0.9561\n",
            "Epoch 63/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2397 - acc: 0.9179 - val_loss: 0.1338 - val_acc: 0.9569\n",
            "Epoch 64/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2484 - acc: 0.9168 - val_loss: 0.1356 - val_acc: 0.9567\n",
            "Epoch 65/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2389 - acc: 0.9186 - val_loss: 0.1286 - val_acc: 0.9594\n",
            "Epoch 66/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2466 - acc: 0.9178 - val_loss: 0.1306 - val_acc: 0.9564\n",
            "Epoch 67/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2433 - acc: 0.9177 - val_loss: 0.1279 - val_acc: 0.9577\n",
            "Epoch 68/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2416 - acc: 0.9178 - val_loss: 0.1297 - val_acc: 0.9577\n",
            "Epoch 69/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2328 - acc: 0.9215 - val_loss: 0.1276 - val_acc: 0.9577\n",
            "Epoch 70/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2352 - acc: 0.9191 - val_loss: 0.1286 - val_acc: 0.9572\n",
            "Epoch 71/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2343 - acc: 0.9197 - val_loss: 0.1306 - val_acc: 0.9572\n",
            "Epoch 72/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2414 - acc: 0.9174 - val_loss: 0.1274 - val_acc: 0.9586\n",
            "Epoch 73/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2355 - acc: 0.9194 - val_loss: 0.1291 - val_acc: 0.9558\n",
            "Epoch 74/100\n",
            "33029/33029 [==============================] - 2s 56us/sample - loss: 0.2347 - acc: 0.9208 - val_loss: 0.1277 - val_acc: 0.9586\n",
            "Epoch 75/100\n",
            "33029/33029 [==============================] - 2s 56us/sample - loss: 0.2375 - acc: 0.9169 - val_loss: 0.1293 - val_acc: 0.9583\n",
            "Epoch 76/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2349 - acc: 0.9216 - val_loss: 0.1244 - val_acc: 0.9594\n",
            "Epoch 77/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2327 - acc: 0.9200 - val_loss: 0.1274 - val_acc: 0.9564\n",
            "Epoch 78/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2356 - acc: 0.9209 - val_loss: 0.1278 - val_acc: 0.9567\n",
            "Epoch 79/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2286 - acc: 0.9232 - val_loss: 0.1229 - val_acc: 0.9583\n",
            "Epoch 80/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2327 - acc: 0.9207 - val_loss: 0.1250 - val_acc: 0.9586\n",
            "Epoch 81/100\n",
            "33029/33029 [==============================] - 2s 59us/sample - loss: 0.2322 - acc: 0.9222 - val_loss: 0.1247 - val_acc: 0.9607\n",
            "Epoch 82/100\n",
            "33029/33029 [==============================] - 2s 59us/sample - loss: 0.2356 - acc: 0.9199 - val_loss: 0.1222 - val_acc: 0.9588\n",
            "Epoch 83/100\n",
            "33029/33029 [==============================] - 2s 64us/sample - loss: 0.2283 - acc: 0.9219 - val_loss: 0.1294 - val_acc: 0.9586\n",
            "Epoch 84/100\n",
            "33029/33029 [==============================] - 2s 63us/sample - loss: 0.2307 - acc: 0.9221 - val_loss: 0.1269 - val_acc: 0.9567\n",
            "Epoch 85/100\n",
            "33029/33029 [==============================] - 2s 57us/sample - loss: 0.2293 - acc: 0.9210 - val_loss: 0.1259 - val_acc: 0.9621\n",
            "Epoch 86/100\n",
            "33029/33029 [==============================] - 2s 60us/sample - loss: 0.2249 - acc: 0.9232 - val_loss: 0.1268 - val_acc: 0.9580\n",
            "Epoch 87/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2301 - acc: 0.9229 - val_loss: 0.1238 - val_acc: 0.9591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-7-q5bOstri",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FoHKjaqpGQub"
      },
      "source": [
        "#### Save the model as a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vixNeoaEGW9n",
        "colab": {}
      },
      "source": [
        "model.save('{}.model'.format(os.path.basename(dataPath)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1VG5cxSplyA",
        "colab_type": "text"
      },
      "source": [
        "### Model Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p3xvC6ZlG9Wz",
        "outputId": "3bd5a41f-69fe-47c4-b944-6e112a7be670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "scores = model.evaluate(X,encoded_y, batch_size=batch_size,verbose=1)\n",
        "print(model.metrics_names)\n",
        "acc, loss = scores[1]*100, scores[0]\n",
        "print('Baseline: accuracy: {:.2f}%: loss: {:.2f}'.format(acc, loss))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36697/36697 [==============================] - 1s 23us/sample - loss: 0.1127 - acc: 0.9637\n",
            "['loss', 'acc']\n",
            "Baseline: accuracy: 96.37%: loss: 0.11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlNdQsyiplyF",
        "colab_type": "text"
      },
      "source": [
        "#### Generate predictions from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0jLyQXHplyG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "54a4371f-8a49-4949-85f9-221a36547a12"
      },
      "source": [
        "prediction_y = model.predict_classes(X, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36697/36697 [==============================] - 1s 29us/sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44-OsF7tplyK",
        "colab_type": "text"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjsWSmWvplyM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "7e0b4c06-b9f1-4eb4-db52-14fe8cdb2c18"
      },
      "source": [
        "y=LabelEncoder().fit_transform(categorical_df[dep_var].values)\n",
        "cm = confusion_matrix(y, prediction_y)\n",
        "sn.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Defacement','Benign','Malware','Phishing','Spam'],\n",
        "           yticklabels=['Defacement','Benign','Malware','Phishing','Spam'])"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fea293d39e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAD8CAYAAABO3GKQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUF3XcQZm0AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeVn3oV7plyQ",
        "colab_type": "text"
      },
      "source": [
        "#### Graph of Categorical Cross-Entropy Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzH_f_T3plyU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "8a97c626-152a-4520-cf1b-0ec66863b0e2"
      },
      "source": [
        "plot('Categorical Model Accuracy')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtWNPIlMplyY",
        "colab_type": "text"
      },
      "source": [
        "### Write results to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YP-tODMplyZ",
        "colab": {}
      },
      "source": [
        "resultFile = os.path.join(resultPath, dataFile)\n",
        "with open('{}.result'.format(resultFile), 'a') as fout:\n",
        "  fout.write('{} results...'.format(model_name+model_extension))\n",
        "  fout.write('\\taccuracy: {:.2f} loss: {:.2f}\\n'.format(acc, loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOJNLEDfplyc",
        "colab_type": "text"
      },
      "source": [
        "## Binary Classification of Labels\n",
        "> Change all malicious labels to value 1 and benign label to 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH5SPeyD1zUX",
        "colab_type": "text"
      },
      "source": [
        "### Binarize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzpGC1TYplyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lblTypes = list(lblTypes)\n",
        "lblTypes = dict(zip(lblTypes, [1, 1, 1, 1, 1]))\n",
        "lblTypes['benign'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ5kYPnTplyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "binary_df = df1.copy()\n",
        "binary_df[label] = binary_df[label].map(lblTypes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taziMECuplyj",
        "colab_type": "text"
      },
      "source": [
        "### Train the Binary Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llXRdiQ1plyk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "293e8abd-c1e5-4a0d-f921-d2a39d6f9df2"
      },
      "source": [
        "model, history, X , encoded_y = experiment(binary_df)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running fold #1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0716 21:48:34.614425 140645152642944 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33026 samples, validate on 3671 samples\n",
            "Epoch 1/100\n",
            "33026/33026 [==============================] - 3s 104us/sample - loss: 0.3633 - acc: 0.8567 - val_loss: 0.1621 - val_acc: 0.9308\n",
            "Epoch 2/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.2082 - acc: 0.9139 - val_loss: 0.1350 - val_acc: 0.9442\n",
            "Epoch 3/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.1710 - acc: 0.9294 - val_loss: 0.1162 - val_acc: 0.9504\n",
            "Epoch 4/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.1511 - acc: 0.9383 - val_loss: 0.1073 - val_acc: 0.9570\n",
            "Epoch 5/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.1383 - acc: 0.9446 - val_loss: 0.0947 - val_acc: 0.9646\n",
            "Epoch 6/100\n",
            "33026/33026 [==============================] - 2s 51us/sample - loss: 0.1289 - acc: 0.9488 - val_loss: 0.0880 - val_acc: 0.9670\n",
            "Epoch 7/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.1193 - acc: 0.9529 - val_loss: 0.0841 - val_acc: 0.9709\n",
            "Epoch 8/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.1165 - acc: 0.9550 - val_loss: 0.0815 - val_acc: 0.9695\n",
            "Epoch 9/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.1104 - acc: 0.9577 - val_loss: 0.0774 - val_acc: 0.9703\n",
            "Epoch 10/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.1059 - acc: 0.9602 - val_loss: 0.0750 - val_acc: 0.9744\n",
            "Epoch 11/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.1011 - acc: 0.9619 - val_loss: 0.0766 - val_acc: 0.9698\n",
            "Epoch 12/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.0978 - acc: 0.9627 - val_loss: 0.0719 - val_acc: 0.9749\n",
            "Epoch 13/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0952 - acc: 0.9646 - val_loss: 0.0702 - val_acc: 0.9738\n",
            "Epoch 14/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.0912 - acc: 0.9657 - val_loss: 0.0668 - val_acc: 0.9793\n",
            "Epoch 15/100\n",
            "33026/33026 [==============================] - 2s 57us/sample - loss: 0.0969 - acc: 0.9632 - val_loss: 0.0671 - val_acc: 0.9793\n",
            "Epoch 16/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0905 - acc: 0.9667 - val_loss: 0.0669 - val_acc: 0.9788\n",
            "Epoch 17/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0886 - acc: 0.9674 - val_loss: 0.0650 - val_acc: 0.9796\n",
            "Epoch 18/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0845 - acc: 0.9701 - val_loss: 0.0612 - val_acc: 0.9801\n",
            "Epoch 19/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.0870 - acc: 0.9674 - val_loss: 0.0630 - val_acc: 0.9798\n",
            "Epoch 20/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0812 - acc: 0.9704 - val_loss: 0.0627 - val_acc: 0.9793\n",
            "Epoch 21/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0823 - acc: 0.9698 - val_loss: 0.0606 - val_acc: 0.9809\n",
            "Epoch 22/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.0816 - acc: 0.9712 - val_loss: 0.0597 - val_acc: 0.9801\n",
            "Epoch 23/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0775 - acc: 0.9724 - val_loss: 0.0589 - val_acc: 0.9807\n",
            "Epoch 24/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.0787 - acc: 0.9712 - val_loss: 0.0626 - val_acc: 0.9779\n",
            "Epoch 25/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0783 - acc: 0.9716 - val_loss: 0.0568 - val_acc: 0.9817\n",
            "Epoch 26/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0747 - acc: 0.9728 - val_loss: 0.0570 - val_acc: 0.9820\n",
            "Epoch 27/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0719 - acc: 0.9736 - val_loss: 0.0612 - val_acc: 0.9793\n",
            "Epoch 28/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.0759 - acc: 0.9722 - val_loss: 0.0564 - val_acc: 0.9828\n",
            "Epoch 29/100\n",
            "33026/33026 [==============================] - 2s 56us/sample - loss: 0.0726 - acc: 0.9739 - val_loss: 0.0590 - val_acc: 0.9788\n",
            "Epoch 30/100\n",
            "33026/33026 [==============================] - 2s 56us/sample - loss: 0.0712 - acc: 0.9747 - val_loss: 0.0559 - val_acc: 0.9826\n",
            "Epoch 31/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0727 - acc: 0.9741 - val_loss: 0.0560 - val_acc: 0.9804\n",
            "Epoch 32/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0696 - acc: 0.9750 - val_loss: 0.0573 - val_acc: 0.9820\n",
            "Epoch 33/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0713 - acc: 0.9745 - val_loss: 0.0571 - val_acc: 0.9815\n",
            "Epoch 34/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0687 - acc: 0.9754 - val_loss: 0.0538 - val_acc: 0.9828\n",
            "Epoch 35/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.0652 - acc: 0.9765 - val_loss: 0.0584 - val_acc: 0.9817\n",
            "Epoch 36/100\n",
            "33026/33026 [==============================] - 2s 56us/sample - loss: 0.0672 - acc: 0.9754 - val_loss: 0.0578 - val_acc: 0.9820\n",
            "Epoch 37/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0709 - acc: 0.9743 - val_loss: 0.0552 - val_acc: 0.9812\n",
            "Epoch 38/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0678 - acc: 0.9753 - val_loss: 0.0526 - val_acc: 0.9837\n",
            "Epoch 39/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0652 - acc: 0.9767 - val_loss: 0.0539 - val_acc: 0.9834\n",
            "Epoch 40/100\n",
            "33026/33026 [==============================] - 2s 56us/sample - loss: 0.0692 - acc: 0.9753 - val_loss: 0.0564 - val_acc: 0.9823\n",
            "Epoch 41/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0673 - acc: 0.9769 - val_loss: 0.0518 - val_acc: 0.9842\n",
            "Epoch 42/100\n",
            "33026/33026 [==============================] - 2s 57us/sample - loss: 0.0675 - acc: 0.9763 - val_loss: 0.0531 - val_acc: 0.9826\n",
            "Epoch 43/100\n",
            "33026/33026 [==============================] - 2s 56us/sample - loss: 0.0663 - acc: 0.9764 - val_loss: 0.0563 - val_acc: 0.9812\n",
            "Epoch 44/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0633 - acc: 0.9777 - val_loss: 0.0535 - val_acc: 0.9828\n",
            "Epoch 45/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0647 - acc: 0.9768 - val_loss: 0.0527 - val_acc: 0.9837\n",
            "Epoch 46/100\n",
            "33026/33026 [==============================] - 2s 60us/sample - loss: 0.0633 - acc: 0.9777 - val_loss: 0.0545 - val_acc: 0.9815\n",
            "Running fold #2\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 4s 110us/sample - loss: 0.3446 - acc: 0.8655 - val_loss: 0.1652 - val_acc: 0.9322\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1940 - acc: 0.9213 - val_loss: 0.1340 - val_acc: 0.9477\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1620 - acc: 0.9342 - val_loss: 0.1156 - val_acc: 0.9572\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.1450 - acc: 0.9434 - val_loss: 0.1036 - val_acc: 0.9586\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1341 - acc: 0.9470 - val_loss: 0.1010 - val_acc: 0.9635\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.1256 - acc: 0.9507 - val_loss: 0.0935 - val_acc: 0.9662\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1156 - acc: 0.9559 - val_loss: 0.0851 - val_acc: 0.9700\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1134 - acc: 0.9574 - val_loss: 0.0843 - val_acc: 0.9714\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1058 - acc: 0.9600 - val_loss: 0.0814 - val_acc: 0.9700\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1037 - acc: 0.9607 - val_loss: 0.0770 - val_acc: 0.9722\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1015 - acc: 0.9624 - val_loss: 0.0729 - val_acc: 0.9749\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0963 - acc: 0.9641 - val_loss: 0.0694 - val_acc: 0.9755\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0955 - acc: 0.9650 - val_loss: 0.0711 - val_acc: 0.9733\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0933 - acc: 0.9672 - val_loss: 0.0691 - val_acc: 0.9760\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0931 - acc: 0.9670 - val_loss: 0.0673 - val_acc: 0.9763\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0883 - acc: 0.9680 - val_loss: 0.0642 - val_acc: 0.9763\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0868 - acc: 0.9689 - val_loss: 0.0642 - val_acc: 0.9768\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0859 - acc: 0.9692 - val_loss: 0.0651 - val_acc: 0.9777\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0839 - acc: 0.9700 - val_loss: 0.0629 - val_acc: 0.9763\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0817 - acc: 0.9707 - val_loss: 0.0615 - val_acc: 0.9779\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0837 - acc: 0.9699 - val_loss: 0.0623 - val_acc: 0.9779\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0819 - acc: 0.9714 - val_loss: 0.0594 - val_acc: 0.9787\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0781 - acc: 0.9721 - val_loss: 0.0591 - val_acc: 0.9787\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0774 - acc: 0.9713 - val_loss: 0.0604 - val_acc: 0.9766\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0745 - acc: 0.9736 - val_loss: 0.0592 - val_acc: 0.9779\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0722 - acc: 0.9743 - val_loss: 0.0552 - val_acc: 0.9801\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0757 - acc: 0.9732 - val_loss: 0.0572 - val_acc: 0.9798\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0758 - acc: 0.9722 - val_loss: 0.0585 - val_acc: 0.9790\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0756 - acc: 0.9727 - val_loss: 0.0574 - val_acc: 0.9782\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0746 - acc: 0.9736 - val_loss: 0.0579 - val_acc: 0.9793\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0746 - acc: 0.9735 - val_loss: 0.0577 - val_acc: 0.9782\n",
            "Running fold #3\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 4s 109us/sample - loss: 0.3438 - acc: 0.8665 - val_loss: 0.1523 - val_acc: 0.9371\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1893 - acc: 0.9223 - val_loss: 0.1177 - val_acc: 0.9591\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1622 - acc: 0.9339 - val_loss: 0.1030 - val_acc: 0.9651\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.1443 - acc: 0.9417 - val_loss: 0.0958 - val_acc: 0.9670\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.1346 - acc: 0.9482 - val_loss: 0.0886 - val_acc: 0.9703\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.1259 - acc: 0.9517 - val_loss: 0.0803 - val_acc: 0.9728\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1186 - acc: 0.9535 - val_loss: 0.0788 - val_acc: 0.9706\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.1124 - acc: 0.9582 - val_loss: 0.0727 - val_acc: 0.9719\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.1084 - acc: 0.9593 - val_loss: 0.0696 - val_acc: 0.9722\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.1039 - acc: 0.9619 - val_loss: 0.0697 - val_acc: 0.9728\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0991 - acc: 0.9636 - val_loss: 0.0664 - val_acc: 0.9760\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.0987 - acc: 0.9630 - val_loss: 0.0662 - val_acc: 0.9766\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0951 - acc: 0.9651 - val_loss: 0.0639 - val_acc: 0.9790\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0952 - acc: 0.9650 - val_loss: 0.0637 - val_acc: 0.9757\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0963 - acc: 0.9638 - val_loss: 0.0613 - val_acc: 0.9779\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0912 - acc: 0.9669 - val_loss: 0.0597 - val_acc: 0.9785\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0873 - acc: 0.9675 - val_loss: 0.0574 - val_acc: 0.9796\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0879 - acc: 0.9678 - val_loss: 0.0565 - val_acc: 0.9790\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0855 - acc: 0.9691 - val_loss: 0.0573 - val_acc: 0.9787\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0840 - acc: 0.9694 - val_loss: 0.0552 - val_acc: 0.9804\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0823 - acc: 0.9712 - val_loss: 0.0551 - val_acc: 0.9790\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0826 - acc: 0.9693 - val_loss: 0.0541 - val_acc: 0.9801\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0840 - acc: 0.9703 - val_loss: 0.0549 - val_acc: 0.9817\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0799 - acc: 0.9724 - val_loss: 0.0527 - val_acc: 0.9812\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0808 - acc: 0.9705 - val_loss: 0.0561 - val_acc: 0.9801\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0743 - acc: 0.9727 - val_loss: 0.0514 - val_acc: 0.9823\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0775 - acc: 0.9722 - val_loss: 0.0489 - val_acc: 0.9834\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0782 - acc: 0.9723 - val_loss: 0.0481 - val_acc: 0.9834\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0732 - acc: 0.9742 - val_loss: 0.0524 - val_acc: 0.9817\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0717 - acc: 0.9744 - val_loss: 0.0511 - val_acc: 0.9809\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0747 - acc: 0.9741 - val_loss: 0.0500 - val_acc: 0.9820\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0743 - acc: 0.9737 - val_loss: 0.0477 - val_acc: 0.9817\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0721 - acc: 0.9750 - val_loss: 0.0477 - val_acc: 0.9823\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0744 - acc: 0.9742 - val_loss: 0.0486 - val_acc: 0.9831\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0730 - acc: 0.9749 - val_loss: 0.0461 - val_acc: 0.9823\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0689 - acc: 0.9751 - val_loss: 0.0463 - val_acc: 0.9837\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0715 - acc: 0.9749 - val_loss: 0.0486 - val_acc: 0.9828\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0703 - acc: 0.9749 - val_loss: 0.0450 - val_acc: 0.9831\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0697 - acc: 0.9755 - val_loss: 0.0464 - val_acc: 0.9828\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0683 - acc: 0.9769 - val_loss: 0.0501 - val_acc: 0.9817\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.0677 - acc: 0.9762 - val_loss: 0.0483 - val_acc: 0.9837\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0663 - acc: 0.9765 - val_loss: 0.0450 - val_acc: 0.9834\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0659 - acc: 0.9758 - val_loss: 0.0439 - val_acc: 0.9853\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0665 - acc: 0.9769 - val_loss: 0.0450 - val_acc: 0.9847\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0676 - acc: 0.9771 - val_loss: 0.0455 - val_acc: 0.9837\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0686 - acc: 0.9765 - val_loss: 0.0452 - val_acc: 0.9853\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0662 - acc: 0.9773 - val_loss: 0.0437 - val_acc: 0.9847\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0648 - acc: 0.9766 - val_loss: 0.0432 - val_acc: 0.9856\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0630 - acc: 0.9780 - val_loss: 0.0426 - val_acc: 0.9845\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0620 - acc: 0.9784 - val_loss: 0.0433 - val_acc: 0.9850\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0618 - acc: 0.9785 - val_loss: 0.0430 - val_acc: 0.9858\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0657 - acc: 0.9774 - val_loss: 0.0433 - val_acc: 0.9839\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0638 - acc: 0.9769 - val_loss: 0.0434 - val_acc: 0.9842\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0640 - acc: 0.9768 - val_loss: 0.0431 - val_acc: 0.9853\n",
            "Running fold #4\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 4s 116us/sample - loss: 0.3459 - acc: 0.8661 - val_loss: 0.1615 - val_acc: 0.9286\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.1974 - acc: 0.9205 - val_loss: 0.1271 - val_acc: 0.9518\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1662 - acc: 0.9316 - val_loss: 0.1098 - val_acc: 0.9613\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1526 - acc: 0.9395 - val_loss: 0.0998 - val_acc: 0.9646\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.1373 - acc: 0.9450 - val_loss: 0.0930 - val_acc: 0.9654\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.1290 - acc: 0.9497 - val_loss: 0.0848 - val_acc: 0.9711\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1210 - acc: 0.9525 - val_loss: 0.0793 - val_acc: 0.9730\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1175 - acc: 0.9542 - val_loss: 0.0776 - val_acc: 0.9747\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1101 - acc: 0.9581 - val_loss: 0.0726 - val_acc: 0.9768\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1040 - acc: 0.9605 - val_loss: 0.0740 - val_acc: 0.9755\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1056 - acc: 0.9598 - val_loss: 0.0704 - val_acc: 0.9777\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0988 - acc: 0.9635 - val_loss: 0.0645 - val_acc: 0.9801\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0983 - acc: 0.9629 - val_loss: 0.0621 - val_acc: 0.9801\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0950 - acc: 0.9652 - val_loss: 0.0653 - val_acc: 0.9777\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0930 - acc: 0.9660 - val_loss: 0.0592 - val_acc: 0.9831\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0907 - acc: 0.9667 - val_loss: 0.0643 - val_acc: 0.9801\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0892 - acc: 0.9681 - val_loss: 0.0618 - val_acc: 0.9815\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0899 - acc: 0.9675 - val_loss: 0.0565 - val_acc: 0.9815\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0875 - acc: 0.9669 - val_loss: 0.0601 - val_acc: 0.9809\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0850 - acc: 0.9688 - val_loss: 0.0546 - val_acc: 0.9831\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0818 - acc: 0.9701 - val_loss: 0.0543 - val_acc: 0.9839\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0818 - acc: 0.9704 - val_loss: 0.0538 - val_acc: 0.9828\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0811 - acc: 0.9704 - val_loss: 0.0535 - val_acc: 0.9826\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0798 - acc: 0.9708 - val_loss: 0.0512 - val_acc: 0.9837\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0785 - acc: 0.9714 - val_loss: 0.0522 - val_acc: 0.9845\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0780 - acc: 0.9737 - val_loss: 0.0522 - val_acc: 0.9847\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0771 - acc: 0.9721 - val_loss: 0.0508 - val_acc: 0.9850\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0804 - acc: 0.9717 - val_loss: 0.0517 - val_acc: 0.9861\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0733 - acc: 0.9734 - val_loss: 0.0531 - val_acc: 0.9831\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0735 - acc: 0.9725 - val_loss: 0.0530 - val_acc: 0.9845\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0730 - acc: 0.9735 - val_loss: 0.0478 - val_acc: 0.9847\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0733 - acc: 0.9733 - val_loss: 0.0491 - val_acc: 0.9858\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0707 - acc: 0.9754 - val_loss: 0.0488 - val_acc: 0.9856\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0736 - acc: 0.9730 - val_loss: 0.0497 - val_acc: 0.9856\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0698 - acc: 0.9756 - val_loss: 0.0495 - val_acc: 0.9856\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0705 - acc: 0.9751 - val_loss: 0.0506 - val_acc: 0.9856\n",
            "Running fold #5\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 4s 122us/sample - loss: 0.3632 - acc: 0.8601 - val_loss: 0.1646 - val_acc: 0.9300\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.2067 - acc: 0.9140 - val_loss: 0.1359 - val_acc: 0.9455\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1713 - acc: 0.9311 - val_loss: 0.1198 - val_acc: 0.9515\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1535 - acc: 0.9377 - val_loss: 0.1123 - val_acc: 0.9569\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1436 - acc: 0.9420 - val_loss: 0.1003 - val_acc: 0.9627\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1321 - acc: 0.9481 - val_loss: 0.0965 - val_acc: 0.9649\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1218 - acc: 0.9530 - val_loss: 0.0865 - val_acc: 0.9692\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1154 - acc: 0.9549 - val_loss: 0.0822 - val_acc: 0.9706\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1110 - acc: 0.9584 - val_loss: 0.0736 - val_acc: 0.9733\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1107 - acc: 0.9577 - val_loss: 0.0771 - val_acc: 0.9736\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1045 - acc: 0.9608 - val_loss: 0.0745 - val_acc: 0.9744\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1002 - acc: 0.9634 - val_loss: 0.0710 - val_acc: 0.9782\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1003 - acc: 0.9617 - val_loss: 0.0688 - val_acc: 0.9768\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0944 - acc: 0.9648 - val_loss: 0.0678 - val_acc: 0.9763\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0990 - acc: 0.9638 - val_loss: 0.0669 - val_acc: 0.9763\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0946 - acc: 0.9647 - val_loss: 0.0637 - val_acc: 0.9790\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0905 - acc: 0.9657 - val_loss: 0.0605 - val_acc: 0.9815\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0893 - acc: 0.9673 - val_loss: 0.0618 - val_acc: 0.9801\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0864 - acc: 0.9688 - val_loss: 0.0609 - val_acc: 0.9785\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0867 - acc: 0.9675 - val_loss: 0.0634 - val_acc: 0.9785\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0871 - acc: 0.9693 - val_loss: 0.0602 - val_acc: 0.9804\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0841 - acc: 0.9706 - val_loss: 0.0569 - val_acc: 0.9828\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0839 - acc: 0.9689 - val_loss: 0.0559 - val_acc: 0.9839\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.0795 - acc: 0.9713 - val_loss: 0.0590 - val_acc: 0.9817\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0809 - acc: 0.9698 - val_loss: 0.0543 - val_acc: 0.9817\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0817 - acc: 0.9698 - val_loss: 0.0568 - val_acc: 0.9845\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0775 - acc: 0.9717 - val_loss: 0.0565 - val_acc: 0.9823\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0793 - acc: 0.9710 - val_loss: 0.0551 - val_acc: 0.9839\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0760 - acc: 0.9728 - val_loss: 0.0521 - val_acc: 0.9837\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0746 - acc: 0.9731 - val_loss: 0.0515 - val_acc: 0.9853\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0749 - acc: 0.9733 - val_loss: 0.0524 - val_acc: 0.9837\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0735 - acc: 0.9743 - val_loss: 0.0489 - val_acc: 0.9847\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0738 - acc: 0.9744 - val_loss: 0.0564 - val_acc: 0.9820\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0736 - acc: 0.9737 - val_loss: 0.0509 - val_acc: 0.9850\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0731 - acc: 0.9735 - val_loss: 0.0536 - val_acc: 0.9847\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0707 - acc: 0.9746 - val_loss: 0.0493 - val_acc: 0.9856\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0716 - acc: 0.9750 - val_loss: 0.0503 - val_acc: 0.9853\n",
            "Running fold #6\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 4s 125us/sample - loss: 0.3418 - acc: 0.8653 - val_loss: 0.1602 - val_acc: 0.9341\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.1924 - acc: 0.9208 - val_loss: 0.1257 - val_acc: 0.9469\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.1627 - acc: 0.9334 - val_loss: 0.1064 - val_acc: 0.9580\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1461 - acc: 0.9427 - val_loss: 0.0977 - val_acc: 0.9616\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1337 - acc: 0.9469 - val_loss: 0.0897 - val_acc: 0.9676\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1273 - acc: 0.9522 - val_loss: 0.0840 - val_acc: 0.9689\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1154 - acc: 0.9562 - val_loss: 0.0778 - val_acc: 0.9708\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.1126 - acc: 0.9565 - val_loss: 0.0755 - val_acc: 0.9725\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1054 - acc: 0.9604 - val_loss: 0.0758 - val_acc: 0.9725\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.1032 - acc: 0.9609 - val_loss: 0.0689 - val_acc: 0.9733\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.1012 - acc: 0.9615 - val_loss: 0.0672 - val_acc: 0.9733\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.0984 - acc: 0.9629 - val_loss: 0.0663 - val_acc: 0.9757\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.0959 - acc: 0.9648 - val_loss: 0.0645 - val_acc: 0.9763\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.0930 - acc: 0.9666 - val_loss: 0.0607 - val_acc: 0.9768\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0887 - acc: 0.9678 - val_loss: 0.0604 - val_acc: 0.9787\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0843 - acc: 0.9703 - val_loss: 0.0666 - val_acc: 0.9749\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0867 - acc: 0.9684 - val_loss: 0.0592 - val_acc: 0.9787\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.0845 - acc: 0.9705 - val_loss: 0.0573 - val_acc: 0.9790\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.0827 - acc: 0.9706 - val_loss: 0.0571 - val_acc: 0.9812\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 72us/sample - loss: 0.0797 - acc: 0.9714 - val_loss: 0.0546 - val_acc: 0.9804\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 67us/sample - loss: 0.0793 - acc: 0.9711 - val_loss: 0.0564 - val_acc: 0.9771\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0749 - acc: 0.9733 - val_loss: 0.0559 - val_acc: 0.9798\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0739 - acc: 0.9737 - val_loss: 0.0537 - val_acc: 0.9823\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0754 - acc: 0.9727 - val_loss: 0.0556 - val_acc: 0.9782\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0735 - acc: 0.9744 - val_loss: 0.0547 - val_acc: 0.9785\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0747 - acc: 0.9735 - val_loss: 0.0553 - val_acc: 0.9815\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0737 - acc: 0.9744 - val_loss: 0.0536 - val_acc: 0.9812\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.0730 - acc: 0.9744 - val_loss: 0.0512 - val_acc: 0.9804\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0744 - acc: 0.9730 - val_loss: 0.0537 - val_acc: 0.9782\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.0700 - acc: 0.9750 - val_loss: 0.0525 - val_acc: 0.9804\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 69us/sample - loss: 0.0705 - acc: 0.9749 - val_loss: 0.0549 - val_acc: 0.9798\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 66us/sample - loss: 0.0722 - acc: 0.9748 - val_loss: 0.0537 - val_acc: 0.9801\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.0693 - acc: 0.9759 - val_loss: 0.0494 - val_acc: 0.9812\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0687 - acc: 0.9752 - val_loss: 0.0535 - val_acc: 0.9798\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0715 - acc: 0.9750 - val_loss: 0.0529 - val_acc: 0.9812\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0679 - acc: 0.9764 - val_loss: 0.0507 - val_acc: 0.9815\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0662 - acc: 0.9768 - val_loss: 0.0526 - val_acc: 0.9801\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0659 - acc: 0.9771 - val_loss: 0.0498 - val_acc: 0.9798\n",
            "Running fold #7\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 4s 130us/sample - loss: 0.3620 - acc: 0.8583 - val_loss: 0.1671 - val_acc: 0.9275\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.1943 - acc: 0.9202 - val_loss: 0.1285 - val_acc: 0.9447\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.1613 - acc: 0.9351 - val_loss: 0.1082 - val_acc: 0.9588\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.1452 - acc: 0.9423 - val_loss: 0.0989 - val_acc: 0.9654\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1337 - acc: 0.9464 - val_loss: 0.0909 - val_acc: 0.9654\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1290 - acc: 0.9493 - val_loss: 0.0945 - val_acc: 0.9667\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.1186 - acc: 0.9537 - val_loss: 0.0861 - val_acc: 0.9684\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.1145 - acc: 0.9582 - val_loss: 0.0783 - val_acc: 0.9703\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.1101 - acc: 0.9588 - val_loss: 0.0804 - val_acc: 0.9711\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.1067 - acc: 0.9597 - val_loss: 0.0739 - val_acc: 0.9730\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.1042 - acc: 0.9613 - val_loss: 0.0727 - val_acc: 0.9738\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.1028 - acc: 0.9612 - val_loss: 0.0676 - val_acc: 0.9736\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0967 - acc: 0.9641 - val_loss: 0.0653 - val_acc: 0.9782\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0945 - acc: 0.9657 - val_loss: 0.0620 - val_acc: 0.9771\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0928 - acc: 0.9652 - val_loss: 0.0617 - val_acc: 0.9793\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0910 - acc: 0.9671 - val_loss: 0.0624 - val_acc: 0.9777\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0879 - acc: 0.9682 - val_loss: 0.0606 - val_acc: 0.9785\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0866 - acc: 0.9681 - val_loss: 0.0605 - val_acc: 0.9801\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0873 - acc: 0.9690 - val_loss: 0.0588 - val_acc: 0.9793\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0841 - acc: 0.9685 - val_loss: 0.0561 - val_acc: 0.9804\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0809 - acc: 0.9711 - val_loss: 0.0582 - val_acc: 0.9801\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0807 - acc: 0.9709 - val_loss: 0.0577 - val_acc: 0.9812\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0791 - acc: 0.9718 - val_loss: 0.0620 - val_acc: 0.9779\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0818 - acc: 0.9704 - val_loss: 0.0568 - val_acc: 0.9820\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0786 - acc: 0.9721 - val_loss: 0.0553 - val_acc: 0.9801\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0791 - acc: 0.9716 - val_loss: 0.0565 - val_acc: 0.9815\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0782 - acc: 0.9715 - val_loss: 0.0521 - val_acc: 0.9823\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0750 - acc: 0.9734 - val_loss: 0.0518 - val_acc: 0.9820\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.0753 - acc: 0.9729 - val_loss: 0.0528 - val_acc: 0.9834\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.0756 - acc: 0.9725 - val_loss: 0.0502 - val_acc: 0.9839\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.0721 - acc: 0.9742 - val_loss: 0.0526 - val_acc: 0.9815\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.0723 - acc: 0.9731 - val_loss: 0.0516 - val_acc: 0.9831\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0750 - acc: 0.9744 - val_loss: 0.0521 - val_acc: 0.9812\n",
            "Epoch 34/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0731 - acc: 0.9737 - val_loss: 0.0512 - val_acc: 0.9847\n",
            "Epoch 35/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0716 - acc: 0.9754 - val_loss: 0.0512 - val_acc: 0.9831\n",
            "Running fold #8\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 4s 124us/sample - loss: 0.3582 - acc: 0.8627 - val_loss: 0.1637 - val_acc: 0.9300\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 67us/sample - loss: 0.1957 - acc: 0.9210 - val_loss: 0.1264 - val_acc: 0.9419\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.1659 - acc: 0.9319 - val_loss: 0.1081 - val_acc: 0.9599\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.1418 - acc: 0.9430 - val_loss: 0.0976 - val_acc: 0.9602\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.1344 - acc: 0.9470 - val_loss: 0.0919 - val_acc: 0.9638\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1234 - acc: 0.9510 - val_loss: 0.0832 - val_acc: 0.9665\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.1188 - acc: 0.9553 - val_loss: 0.0813 - val_acc: 0.9711\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 66us/sample - loss: 0.1135 - acc: 0.9557 - val_loss: 0.0786 - val_acc: 0.9697\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.1091 - acc: 0.9592 - val_loss: 0.0747 - val_acc: 0.9736\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.1025 - acc: 0.9607 - val_loss: 0.0725 - val_acc: 0.9727\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0998 - acc: 0.9625 - val_loss: 0.0671 - val_acc: 0.9747\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0982 - acc: 0.9618 - val_loss: 0.0647 - val_acc: 0.9768\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0970 - acc: 0.9634 - val_loss: 0.0656 - val_acc: 0.9741\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0952 - acc: 0.9645 - val_loss: 0.0609 - val_acc: 0.9768\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0912 - acc: 0.9661 - val_loss: 0.0631 - val_acc: 0.9757\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0932 - acc: 0.9667 - val_loss: 0.0591 - val_acc: 0.9774\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.0872 - acc: 0.9684 - val_loss: 0.0644 - val_acc: 0.9763\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0852 - acc: 0.9672 - val_loss: 0.0568 - val_acc: 0.9801\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0836 - acc: 0.9704 - val_loss: 0.0567 - val_acc: 0.9782\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.0865 - acc: 0.9701 - val_loss: 0.0578 - val_acc: 0.9796\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.0811 - acc: 0.9702 - val_loss: 0.0576 - val_acc: 0.9779\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0788 - acc: 0.9714 - val_loss: 0.0570 - val_acc: 0.9796\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0832 - acc: 0.9696 - val_loss: 0.0532 - val_acc: 0.9812\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0784 - acc: 0.9724 - val_loss: 0.0512 - val_acc: 0.9809\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0787 - acc: 0.9718 - val_loss: 0.0531 - val_acc: 0.9798\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0739 - acc: 0.9732 - val_loss: 0.0504 - val_acc: 0.9831\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0817 - acc: 0.9706 - val_loss: 0.0516 - val_acc: 0.9823\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0756 - acc: 0.9743 - val_loss: 0.0480 - val_acc: 0.9828\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0751 - acc: 0.9737 - val_loss: 0.0492 - val_acc: 0.9809\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0728 - acc: 0.9739 - val_loss: 0.0490 - val_acc: 0.9815\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0743 - acc: 0.9735 - val_loss: 0.0507 - val_acc: 0.9790\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0739 - acc: 0.9741 - val_loss: 0.0499 - val_acc: 0.9817\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 65us/sample - loss: 0.0695 - acc: 0.9757 - val_loss: 0.0503 - val_acc: 0.9820\n",
            "Running fold #9\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 4s 134us/sample - loss: 0.3414 - acc: 0.8606 - val_loss: 0.1622 - val_acc: 0.9286\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.1953 - acc: 0.9193 - val_loss: 0.1270 - val_acc: 0.9433\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.1621 - acc: 0.9338 - val_loss: 0.1092 - val_acc: 0.9602\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.1466 - acc: 0.9409 - val_loss: 0.1025 - val_acc: 0.9624\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.1348 - acc: 0.9468 - val_loss: 0.0945 - val_acc: 0.9646\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.1247 - acc: 0.9517 - val_loss: 0.0856 - val_acc: 0.9667\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.1224 - acc: 0.9533 - val_loss: 0.0834 - val_acc: 0.9711\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.1159 - acc: 0.9547 - val_loss: 0.0768 - val_acc: 0.9711\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.1091 - acc: 0.9584 - val_loss: 0.0745 - val_acc: 0.9760\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 67us/sample - loss: 0.1056 - acc: 0.9618 - val_loss: 0.0734 - val_acc: 0.9727\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 76us/sample - loss: 0.1031 - acc: 0.9624 - val_loss: 0.0714 - val_acc: 0.9706\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 70us/sample - loss: 0.0974 - acc: 0.9639 - val_loss: 0.0704 - val_acc: 0.9771\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 70us/sample - loss: 0.1001 - acc: 0.9635 - val_loss: 0.0672 - val_acc: 0.9766\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 75us/sample - loss: 0.0970 - acc: 0.9632 - val_loss: 0.0668 - val_acc: 0.9763\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.0948 - acc: 0.9661 - val_loss: 0.0651 - val_acc: 0.9779\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.0901 - acc: 0.9668 - val_loss: 0.0647 - val_acc: 0.9779\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.0888 - acc: 0.9676 - val_loss: 0.0611 - val_acc: 0.9782\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 67us/sample - loss: 0.0870 - acc: 0.9689 - val_loss: 0.0603 - val_acc: 0.9785\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 65us/sample - loss: 0.0841 - acc: 0.9705 - val_loss: 0.0596 - val_acc: 0.9801\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.0874 - acc: 0.9681 - val_loss: 0.0604 - val_acc: 0.9779\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0841 - acc: 0.9693 - val_loss: 0.0582 - val_acc: 0.9785\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0819 - acc: 0.9714 - val_loss: 0.0579 - val_acc: 0.9798\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0799 - acc: 0.9719 - val_loss: 0.0584 - val_acc: 0.9798\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.0809 - acc: 0.9713 - val_loss: 0.0555 - val_acc: 0.9812\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.0786 - acc: 0.9712 - val_loss: 0.0541 - val_acc: 0.9804\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.0790 - acc: 0.9724 - val_loss: 0.0544 - val_acc: 0.9804\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.0756 - acc: 0.9726 - val_loss: 0.0542 - val_acc: 0.9823\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 69us/sample - loss: 0.0754 - acc: 0.9728 - val_loss: 0.0562 - val_acc: 0.9809\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 66us/sample - loss: 0.0742 - acc: 0.9732 - val_loss: 0.0548 - val_acc: 0.9806\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0753 - acc: 0.9729 - val_loss: 0.0512 - val_acc: 0.9831\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0716 - acc: 0.9751 - val_loss: 0.0515 - val_acc: 0.9834\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0735 - acc: 0.9739 - val_loss: 0.0518 - val_acc: 0.9836\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0730 - acc: 0.9739 - val_loss: 0.0491 - val_acc: 0.9839\n",
            "Epoch 34/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0698 - acc: 0.9755 - val_loss: 0.0530 - val_acc: 0.9820\n",
            "Epoch 35/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0713 - acc: 0.9752 - val_loss: 0.0517 - val_acc: 0.9839\n",
            "Epoch 36/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0711 - acc: 0.9749 - val_loss: 0.0503 - val_acc: 0.9839\n",
            "Epoch 37/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0688 - acc: 0.9768 - val_loss: 0.0511 - val_acc: 0.9842\n",
            "Epoch 38/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0677 - acc: 0.9761 - val_loss: 0.0508 - val_acc: 0.9836\n",
            "Running fold #10\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 4s 134us/sample - loss: 0.3572 - acc: 0.8591 - val_loss: 0.1650 - val_acc: 0.9297\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.1993 - acc: 0.9179 - val_loss: 0.1332 - val_acc: 0.9428\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 65us/sample - loss: 0.1667 - acc: 0.9334 - val_loss: 0.1122 - val_acc: 0.9558\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.1465 - acc: 0.9406 - val_loss: 0.1006 - val_acc: 0.9597\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.1327 - acc: 0.9470 - val_loss: 0.0955 - val_acc: 0.9586\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.1237 - acc: 0.9516 - val_loss: 0.0847 - val_acc: 0.9706\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.1165 - acc: 0.9548 - val_loss: 0.0764 - val_acc: 0.9733\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.1135 - acc: 0.9559 - val_loss: 0.0765 - val_acc: 0.9747\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.1089 - acc: 0.9574 - val_loss: 0.0701 - val_acc: 0.9749\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.1057 - acc: 0.9606 - val_loss: 0.0659 - val_acc: 0.9766\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.1013 - acc: 0.9616 - val_loss: 0.0683 - val_acc: 0.9733\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 65us/sample - loss: 0.0970 - acc: 0.9642 - val_loss: 0.0662 - val_acc: 0.9766\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.0955 - acc: 0.9645 - val_loss: 0.0625 - val_acc: 0.9779\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.0945 - acc: 0.9652 - val_loss: 0.0620 - val_acc: 0.9793\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 67us/sample - loss: 0.0935 - acc: 0.9660 - val_loss: 0.0584 - val_acc: 0.9801\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0907 - acc: 0.9667 - val_loss: 0.0587 - val_acc: 0.9785\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0895 - acc: 0.9674 - val_loss: 0.0630 - val_acc: 0.9777\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0855 - acc: 0.9692 - val_loss: 0.0554 - val_acc: 0.9817\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.0800 - acc: 0.9708 - val_loss: 0.0548 - val_acc: 0.9804\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 69us/sample - loss: 0.0830 - acc: 0.9704 - val_loss: 0.0559 - val_acc: 0.9809\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 69us/sample - loss: 0.0794 - acc: 0.9707 - val_loss: 0.0530 - val_acc: 0.9826\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 67us/sample - loss: 0.0818 - acc: 0.9690 - val_loss: 0.0550 - val_acc: 0.9831\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 65us/sample - loss: 0.0789 - acc: 0.9705 - val_loss: 0.0539 - val_acc: 0.9836\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 67us/sample - loss: 0.0791 - acc: 0.9719 - val_loss: 0.0543 - val_acc: 0.9836\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0781 - acc: 0.9716 - val_loss: 0.0523 - val_acc: 0.9834\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.0807 - acc: 0.9702 - val_loss: 0.0514 - val_acc: 0.9831\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 64us/sample - loss: 0.0765 - acc: 0.9727 - val_loss: 0.0520 - val_acc: 0.9826\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0821 - acc: 0.9701 - val_loss: 0.0512 - val_acc: 0.9836\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 65us/sample - loss: 0.0767 - acc: 0.9733 - val_loss: 0.0511 - val_acc: 0.9826\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0762 - acc: 0.9721 - val_loss: 0.0473 - val_acc: 0.9823\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0736 - acc: 0.9737 - val_loss: 0.0458 - val_acc: 0.9858\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0717 - acc: 0.9743 - val_loss: 0.0451 - val_acc: 0.9856\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0728 - acc: 0.9731 - val_loss: 0.0476 - val_acc: 0.9845\n",
            "Epoch 34/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.0707 - acc: 0.9753 - val_loss: 0.0498 - val_acc: 0.9831\n",
            "Epoch 35/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.0686 - acc: 0.9752 - val_loss: 0.0474 - val_acc: 0.9836\n",
            "Epoch 36/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.0728 - acc: 0.9743 - val_loss: 0.0499 - val_acc: 0.9831\n",
            "Epoch 37/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0729 - acc: 0.9737 - val_loss: 0.0487 - val_acc: 0.9847\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta-ruG0K1eVN",
        "colab_type": "text"
      },
      "source": [
        "### Model Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FV5LefXplyt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c6372ec0-5078-496d-e46e-0098a872aae4"
      },
      "source": [
        "scores = model.evaluate(X,encoded_y, verbose=1)\n",
        "print(model.metrics_names)\n",
        "acc, loss = scores[1]*100, scores[0]\n",
        "print('Baseline: accuracy: {:.2f}%: loss: {:.2f}'.format(acc, loss))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36697/36697 [==============================] - 2s 49us/sample - loss: 0.0413 - acc: 0.9862\n",
            "['loss', 'acc']\n",
            "Baseline: accuracy: 98.62%: loss: 0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DviMoEPX1hxZ",
        "colab_type": "text"
      },
      "source": [
        "#### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kU7Ncq7ply2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c1ce7cf-fabb-485e-c504-d31d2ebed1f7"
      },
      "source": [
        "prediction_y = model.predict_classes(X, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36697/36697 [==============================] - 1s 37us/sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T2dZA9Nply8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "5765c2b9-3aa2-4941-cceb-2fb4cc3aca58"
      },
      "source": [
        "y=LabelEncoder().fit_transform(binary_df[dep_var].values)\n",
        "cm = confusion_matrix(y, prediction_y)\n",
        "sn.heatmap(cm, cmap='Blues', annot=True, fmt='g', xticklabels=['Benign', 'Malicious'],\n",
        "        yticklabels=['Benign', 'Malicious'])"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7fea1ed1e588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAD8CAYAAAC8TPVwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIO2mVmGOGibmWXI/wMlPkTavNGExwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YdW3jb8plzB",
        "colab_type": "text"
      },
      "source": [
        "#### Graph of Binary Cross-Entropy Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzeLN1BSplzC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ff9fb35f-baf9-4398-c467-893b5ef9e39b"
      },
      "source": [
        "plot('Binary Model Accuracy')"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Il5_kL1oQt",
        "colab_type": "text"
      },
      "source": [
        "### Write Results to File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz6wRy6GplzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resultFile = os.path.join(resultPath, dataFile)\n",
        "with open('{}.result'.format(resultFile), 'a') as fout:\n",
        "  fout.write('{} results...'.format(model_name+model_extension))\n",
        "  fout.write('\\taccuracy: {:.2f} loss: {:.2f}\\n'.format(acc, loss))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}