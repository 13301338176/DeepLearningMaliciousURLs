{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras-Tensorflow-Experiments.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rambasnet/DeepLearningMaliciousURLs/blob/master/Keras-Tensorflow-Experiments.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFAQJfeWplvv",
        "colab_type": "text"
      },
      "source": [
        "# Keras-Tensorflow Experiments\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CQ6Ngesk0F9d",
        "colab_type": "text"
      },
      "source": [
        "##### Sources:\n",
        " + https://www.tensorflow.org/tutorials/keras/overfit_and_underfit\n",
        " + https://www.kaggle.com/grafiszti/98-59-acc-on-10-fold-with-testing-7-keras-models\n",
        " + https://keras.io/visualization/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47GgPW0G0Pi7",
        "colab_type": "text"
      },
      "source": [
        "## Initial Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GbTWr3JeuzLm"
      },
      "source": [
        "### Include needed files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sBMU4MElu9GB",
        "outputId": "63a5f6e3-a9c2-431b-f328-e7501b9e5eeb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import operator\n",
        "import time\n",
        "\n",
        "import seaborn as sn\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils.np_utils import to_categorical, normalize\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "07jwatic7-Jk"
      },
      "source": [
        "### Include Dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ngdiiVOJ8Bt4",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "URL=https://iscxdownloads.cs.unb.ca/iscxdownloads/ISCX-URL-2016/\n",
        "FILES=(ISCXURL2016.zip) \n",
        "for FILE in ${FILES[*]}; do\n",
        "    if [ ! -f \"$FILE\" ]; then\n",
        "        printf \"downloading %s\\n\" $FILE\n",
        "        curl -O $URL$FILE\n",
        "        # unzip files\n",
        "        echo 'unzipping ' $FILE\n",
        "        unzip -o $FILE #overwrite exiting files/folders if exists\n",
        "    fi\n",
        "done"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MIqy2Xmc8HoD"
      },
      "source": [
        "### Check Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zl2T8MRq8PNT",
        "outputId": "ed21cb73-66cd-4b33-ae90-d476e3eda809",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "! ls FinalDataset"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All_BestFirst.csv\t      Malware_Infogain.csv\n",
            "All_BestFirst_test.csv\t      Malware_Infogain_test.csv\n",
            "All.csv\t\t\t      Phishing_BestFirst.csv\n",
            "All.csv.pickle\t\t      Phishing.csv\n",
            "All_Infogain.csv\t      Phishing_Infogain.csv\n",
            "All_Infogain_test.csv\t      Phishing_Infogain_test.csv\n",
            "Defacement_BestFirst.csv      Spam_BestFirst.csv\n",
            "Defacement.csv\t\t      Spam_BestFirst_test.csv\n",
            "Defacement_Infogain.csv       Spam.csv\n",
            "Defacement_Infogain_test.csv  Spam_Infogain.csv\n",
            "Malware_BestFirst.csv\t      Spam_Infogain_test.csv\n",
            "Malware.csv\t\t      URL\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IeCm3LP08XOy"
      },
      "source": [
        "### Set some data\n",
        "> Some data needs to be set, we need to ensure that constants are set properly. These are important but will not be used until later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qJuZTgd0u_WG",
        "colab": {}
      },
      "source": [
        "resultPath = 'results_keras_tensorflow'\n",
        "if not os.path.exists(resultPath):\n",
        "   print('result path {} created.'.format(resultPath))\n",
        "   os.mkdir(resultPath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EHnzBhcavSR-",
        "colab": {}
      },
      "source": [
        "model_name=\"init\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6Vo0Cwne9wRJ"
      },
      "source": [
        "## Functions for Testing\n",
        "> Now that our data has been collected it is time to create functions that will be used in later tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DFtoJCs_7T8p",
        "colab": {}
      },
      "source": [
        "def loadData(csvFile):\n",
        "    pickleDump = '{}.pickle'.format(csvFile)\n",
        "    if os.path.exists(pickleDump):\n",
        "        df = pd.read_pickle(pickleDump)\n",
        "    else:\n",
        "        df = pd.read_csv(csvFile, low_memory=False, na_values='NaN')\n",
        "        # clean data\n",
        "        # strip the whitspaces from column names\n",
        "        df = df.rename(str.strip, axis='columns')\n",
        "        #df.drop(columns=[], inplace=True)\n",
        "        # drop missing values/NaN etc.\n",
        "        #df.dropna(inplace=True)\n",
        "        # drop Infinity rows and NaN string from each column\n",
        "        for col in df.columns:\n",
        "            indexNames = df[df[col]=='Infinity'].index\n",
        "            if not indexNames.empty:\n",
        "                print('deleting {} rows with Infinity in column {}'.format(len(indexNames), col))\n",
        "                df.drop(indexNames, inplace=True)\n",
        "        \n",
        "        df.to_pickle(pickleDump)\n",
        "    \n",
        "    return df\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GB51N0iLCK9J",
        "colab": {}
      },
      "source": [
        "def baseline_model(inputDim=-1,batch_size=32,outputDim=-1):\n",
        "    global model_name, model_extension\n",
        "    model = tf.keras.Sequential([\n",
        "        Dense(128, activation='relu', input_shape=(inputDim,)),\n",
        "        BatchNormalization(),\n",
        "        Dropout(.5),\n",
        "        Dense(batch_size, activation='relu'),\n",
        "        BatchNormalization(),\n",
        "        Dropout(.5),\n",
        "        Dense(outputDim, activation='softmax')\n",
        "    ]) #This is the output layer\n",
        "\n",
        "    if outputDim > 2:\n",
        "        print('Categorical Cross-Entropy Loss Function')\n",
        "        model_extension = \"_categorical\"\n",
        "        model.compile(optimizer='adam',\n",
        "                 loss='categorical_crossentropy',\n",
        "                 metrics=['accuracy'])\n",
        "    else:\n",
        "        model_extension = \"_binary\"\n",
        "        print('Binary Cross-Entropy Loss Function')\n",
        "        model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbiT-Sf9plwd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_labels(dataframe):\n",
        "    dataframe=dataframe.copy()\n",
        "    data_y=dataframe.pop(dep_var)\n",
        "    encoder = LabelEncoder()\n",
        "    encoder.fit(data_y)\n",
        "    data_y = encoder.transform(data_y)\n",
        "    dummy_y = to_categorical(data_y)\n",
        "    return dummy_y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aQvhWPU1H8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot(title):\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Test'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1bG2p566m8V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to determine train and validation indexes, \n",
        "# and fit the data to the model we constructed\n",
        "def experiment(dataframe):\n",
        "    \n",
        "    #10-fold cross validation, choosing random indices for training and validation\n",
        "    kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "    \n",
        "    # Encode the label column for model fitting\n",
        "    encoded_y = dataframe.copy()\n",
        "    encoded_y = encode_labels(encoded_y)\n",
        "    \n",
        "    # X is our data/features to train the model with\n",
        "    X=StandardScaler().fit_transform(dataframe.drop(dep_var, axis=1))\n",
        "    \n",
        "    \n",
        "    # Y is our labels to classify the data\n",
        "    y=LabelEncoder().fit_transform(dataframe[dep_var].values)\n",
        "    start_time = time.time()\n",
        "    for index, (train_indices, val_indices) in enumerate(kfold.split(X, y)):\n",
        "        \n",
        "        xtrain, xval = X[train_indices], X[val_indices]\n",
        "        ytrain, yval = encoded_y[train_indices], encoded_y[val_indices]\n",
        "\n",
        "        inputDim=xtrain.shape[1]\n",
        "        outputDim=ytrain.shape[1]\n",
        "        print(\"Running fold #\" + str(index+1))\n",
        "\n",
        "        model = baseline_model(inputDim,batch_size,outputDim)\n",
        "        history = model.fit(xtrain, ytrain, epochs=epochs, validation_data=(xval,yval), callbacks=[tensorboard, early_stop], batch_size=batch_size)\n",
        "       \n",
        "    end_time = time.time() - start_time\n",
        "    remain_seconds = 60%end_time\n",
        "    Minutes = int(end_time/60)\n",
        "    print(\"Time to complete {} min {} sec\".format(Minutes, end_time))\n",
        "        \n",
        "    return model, history, X, encoded_y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GkZ-uZuz9_pi"
      },
      "source": [
        "## Data\n",
        "### Load and Clean\n",
        "> First we will load our data, scan the columns for Infinity and NaN values, and remove those columns from testing. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h_7Y2Ytp7xa-",
        "colab": {}
      },
      "source": [
        "df1 = loadData('FinalDataset/All.csv')\n",
        "df1=df1.dropna(axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mekPFv6lplwm",
        "colab_type": "text"
      },
      "source": [
        "### Display Data Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ZwM-GC1-LPj",
        "outputId": "10991986-aea1-47ba-db77-91518c7c6884",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df1.columns"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Querylength', 'domain_token_count', 'path_token_count',\n",
              "       'avgdomaintokenlen', 'longdomaintokenlen', 'tld', 'charcompvowels',\n",
              "       'charcompace', 'ldl_url', 'ldl_domain', 'ldl_path', 'ldl_filename',\n",
              "       'ldl_getArg', 'dld_url', 'dld_domain', 'dld_path', 'dld_filename',\n",
              "       'dld_getArg', 'urlLen', 'domainlength', 'pathLength', 'subDirLen',\n",
              "       'fileNameLen', 'this.fileExtLen', 'ArgLen', 'pathurlRatio',\n",
              "       'ArgUrlRatio', 'argDomanRatio', 'domainUrlRatio', 'pathDomainRatio',\n",
              "       'argPathRatio', 'executable', 'isPortEighty', 'NumberofDotsinURL',\n",
              "       'ISIpAddressInDomainName', 'CharacterContinuityRate',\n",
              "       'LongestVariableValue', 'URL_DigitCount', 'host_DigitCount',\n",
              "       'Directory_DigitCount', 'File_name_DigitCount', 'Extension_DigitCount',\n",
              "       'Query_DigitCount', 'URL_Letter_Count', 'host_letter_count',\n",
              "       'Directory_LetterCount', 'Filename_LetterCount',\n",
              "       'Extension_LetterCount', 'Query_LetterCount', 'LongestPathTokenLength',\n",
              "       'Domain_LongestWordLength', 'Path_LongestWordLength',\n",
              "       'sub-Directory_LongestWordLength', 'Arguments_LongestWordLength',\n",
              "       'URL_sensitiveWord', 'URLQueries_variable', 'spcharUrl',\n",
              "       'delimeter_Domain', 'delimeter_path', 'delimeter_Count',\n",
              "       'NumberRate_URL', 'NumberRate_Domain', 'NumberRate_DirectoryName',\n",
              "       'NumberRate_FileName', 'SymbolCount_URL', 'SymbolCount_Domain',\n",
              "       'SymbolCount_Directoryname', 'SymbolCount_FileName',\n",
              "       'SymbolCount_Extension', 'SymbolCount_Afterpath', 'Entropy_URL',\n",
              "       'Entropy_Domain', 'URL_Type_obf_Type'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX24a-Woplwt",
        "colab_type": "text"
      },
      "source": [
        "### Display Matrix Shape of Data\n",
        "> In the format (samples, columns)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NBvgfW-J--g8",
        "outputId": "87bacb43-db8f-4b27-ad20-89d4efe8892a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df1.shape"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(36697, 73)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSjQXVYKplw0",
        "colab_type": "text"
      },
      "source": [
        "### Display First Samples in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9eYtvDf5AAR5",
        "outputId": "64bf116c-a9ad-4f87-ad21-1913b73f6b39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df1.head()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Querylength</th>\n",
              "      <th>domain_token_count</th>\n",
              "      <th>path_token_count</th>\n",
              "      <th>avgdomaintokenlen</th>\n",
              "      <th>longdomaintokenlen</th>\n",
              "      <th>tld</th>\n",
              "      <th>charcompvowels</th>\n",
              "      <th>charcompace</th>\n",
              "      <th>ldl_url</th>\n",
              "      <th>ldl_domain</th>\n",
              "      <th>ldl_path</th>\n",
              "      <th>ldl_filename</th>\n",
              "      <th>ldl_getArg</th>\n",
              "      <th>dld_url</th>\n",
              "      <th>dld_domain</th>\n",
              "      <th>dld_path</th>\n",
              "      <th>dld_filename</th>\n",
              "      <th>dld_getArg</th>\n",
              "      <th>urlLen</th>\n",
              "      <th>domainlength</th>\n",
              "      <th>pathLength</th>\n",
              "      <th>subDirLen</th>\n",
              "      <th>fileNameLen</th>\n",
              "      <th>this.fileExtLen</th>\n",
              "      <th>ArgLen</th>\n",
              "      <th>pathurlRatio</th>\n",
              "      <th>ArgUrlRatio</th>\n",
              "      <th>argDomanRatio</th>\n",
              "      <th>domainUrlRatio</th>\n",
              "      <th>pathDomainRatio</th>\n",
              "      <th>argPathRatio</th>\n",
              "      <th>executable</th>\n",
              "      <th>isPortEighty</th>\n",
              "      <th>NumberofDotsinURL</th>\n",
              "      <th>ISIpAddressInDomainName</th>\n",
              "      <th>CharacterContinuityRate</th>\n",
              "      <th>LongestVariableValue</th>\n",
              "      <th>URL_DigitCount</th>\n",
              "      <th>host_DigitCount</th>\n",
              "      <th>Directory_DigitCount</th>\n",
              "      <th>File_name_DigitCount</th>\n",
              "      <th>Extension_DigitCount</th>\n",
              "      <th>Query_DigitCount</th>\n",
              "      <th>URL_Letter_Count</th>\n",
              "      <th>host_letter_count</th>\n",
              "      <th>Directory_LetterCount</th>\n",
              "      <th>Filename_LetterCount</th>\n",
              "      <th>Extension_LetterCount</th>\n",
              "      <th>Query_LetterCount</th>\n",
              "      <th>LongestPathTokenLength</th>\n",
              "      <th>Domain_LongestWordLength</th>\n",
              "      <th>Path_LongestWordLength</th>\n",
              "      <th>sub-Directory_LongestWordLength</th>\n",
              "      <th>Arguments_LongestWordLength</th>\n",
              "      <th>URL_sensitiveWord</th>\n",
              "      <th>URLQueries_variable</th>\n",
              "      <th>spcharUrl</th>\n",
              "      <th>delimeter_Domain</th>\n",
              "      <th>delimeter_path</th>\n",
              "      <th>delimeter_Count</th>\n",
              "      <th>NumberRate_URL</th>\n",
              "      <th>NumberRate_Domain</th>\n",
              "      <th>NumberRate_DirectoryName</th>\n",
              "      <th>NumberRate_FileName</th>\n",
              "      <th>SymbolCount_URL</th>\n",
              "      <th>SymbolCount_Domain</th>\n",
              "      <th>SymbolCount_Directoryname</th>\n",
              "      <th>SymbolCount_FileName</th>\n",
              "      <th>SymbolCount_Extension</th>\n",
              "      <th>SymbolCount_Afterpath</th>\n",
              "      <th>Entropy_URL</th>\n",
              "      <th>Entropy_Domain</th>\n",
              "      <th>URL_Type_obf_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>58</td>\n",
              "      <td>25</td>\n",
              "      <td>26</td>\n",
              "      <td>26</td>\n",
              "      <td>13</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0.448276</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.431034</td>\n",
              "      <td>1.04</td>\n",
              "      <td>0.07692308</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>5</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>47</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.017241</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.726298</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>66</td>\n",
              "      <td>25</td>\n",
              "      <td>34</td>\n",
              "      <td>34</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.515151</td>\n",
              "      <td>0.030303</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.378788</td>\n",
              "      <td>1.36</td>\n",
              "      <td>0.05882353</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>56</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>9</td>\n",
              "      <td>-1</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.688635</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>65</td>\n",
              "      <td>25</td>\n",
              "      <td>33</td>\n",
              "      <td>33</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.507692</td>\n",
              "      <td>0.030769</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.384615</td>\n",
              "      <td>1.32</td>\n",
              "      <td>0.060606062</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>55</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>8</td>\n",
              "      <td>-1</td>\n",
              "      <td>13</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>5</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.695049</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>12</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>32</td>\n",
              "      <td>16</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>109</td>\n",
              "      <td>25</td>\n",
              "      <td>77</td>\n",
              "      <td>77</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.706422</td>\n",
              "      <td>0.018349</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.229358</td>\n",
              "      <td>3.08</td>\n",
              "      <td>0.025974026</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>92</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>45</td>\n",
              "      <td>-1</td>\n",
              "      <td>52</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.640130</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>5.5</td>\n",
              "      <td>14</td>\n",
              "      <td>4</td>\n",
              "      <td>18</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>81</td>\n",
              "      <td>25</td>\n",
              "      <td>49</td>\n",
              "      <td>49</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.604938</td>\n",
              "      <td>0.024691</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.308642</td>\n",
              "      <td>1.96</td>\n",
              "      <td>0.040816326</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>4</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.6</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>70</td>\n",
              "      <td>22</td>\n",
              "      <td>8</td>\n",
              "      <td>13</td>\n",
              "      <td>23</td>\n",
              "      <td>-1</td>\n",
              "      <td>24</td>\n",
              "      <td>14</td>\n",
              "      <td>13</td>\n",
              "      <td>13</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.681307</td>\n",
              "      <td>0.784493</td>\n",
              "      <td>Defacement</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Querylength  domain_token_count  ...  Entropy_Domain  URL_Type_obf_Type\n",
              "0            0                   4  ...        0.784493         Defacement\n",
              "1            0                   4  ...        0.784493         Defacement\n",
              "2            0                   4  ...        0.784493         Defacement\n",
              "3            0                   4  ...        0.784493         Defacement\n",
              "4            0                   4  ...        0.784493         Defacement\n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OLiGVjzOplw6",
        "colab_type": "text"
      },
      "source": [
        "### Display Last Samples in Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsqLE8Wnplw7",
        "colab_type": "code",
        "outputId": "c6d145b0-aeb0-4141-f8d8-376c56ce7fd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "df1.tail()"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Querylength</th>\n",
              "      <th>domain_token_count</th>\n",
              "      <th>path_token_count</th>\n",
              "      <th>avgdomaintokenlen</th>\n",
              "      <th>longdomaintokenlen</th>\n",
              "      <th>tld</th>\n",
              "      <th>charcompvowels</th>\n",
              "      <th>charcompace</th>\n",
              "      <th>ldl_url</th>\n",
              "      <th>ldl_domain</th>\n",
              "      <th>ldl_path</th>\n",
              "      <th>ldl_filename</th>\n",
              "      <th>ldl_getArg</th>\n",
              "      <th>dld_url</th>\n",
              "      <th>dld_domain</th>\n",
              "      <th>dld_path</th>\n",
              "      <th>dld_filename</th>\n",
              "      <th>dld_getArg</th>\n",
              "      <th>urlLen</th>\n",
              "      <th>domainlength</th>\n",
              "      <th>pathLength</th>\n",
              "      <th>subDirLen</th>\n",
              "      <th>fileNameLen</th>\n",
              "      <th>this.fileExtLen</th>\n",
              "      <th>ArgLen</th>\n",
              "      <th>pathurlRatio</th>\n",
              "      <th>ArgUrlRatio</th>\n",
              "      <th>argDomanRatio</th>\n",
              "      <th>domainUrlRatio</th>\n",
              "      <th>pathDomainRatio</th>\n",
              "      <th>argPathRatio</th>\n",
              "      <th>executable</th>\n",
              "      <th>isPortEighty</th>\n",
              "      <th>NumberofDotsinURL</th>\n",
              "      <th>ISIpAddressInDomainName</th>\n",
              "      <th>CharacterContinuityRate</th>\n",
              "      <th>LongestVariableValue</th>\n",
              "      <th>URL_DigitCount</th>\n",
              "      <th>host_DigitCount</th>\n",
              "      <th>Directory_DigitCount</th>\n",
              "      <th>File_name_DigitCount</th>\n",
              "      <th>Extension_DigitCount</th>\n",
              "      <th>Query_DigitCount</th>\n",
              "      <th>URL_Letter_Count</th>\n",
              "      <th>host_letter_count</th>\n",
              "      <th>Directory_LetterCount</th>\n",
              "      <th>Filename_LetterCount</th>\n",
              "      <th>Extension_LetterCount</th>\n",
              "      <th>Query_LetterCount</th>\n",
              "      <th>LongestPathTokenLength</th>\n",
              "      <th>Domain_LongestWordLength</th>\n",
              "      <th>Path_LongestWordLength</th>\n",
              "      <th>sub-Directory_LongestWordLength</th>\n",
              "      <th>Arguments_LongestWordLength</th>\n",
              "      <th>URL_sensitiveWord</th>\n",
              "      <th>URLQueries_variable</th>\n",
              "      <th>spcharUrl</th>\n",
              "      <th>delimeter_Domain</th>\n",
              "      <th>delimeter_path</th>\n",
              "      <th>delimeter_Count</th>\n",
              "      <th>NumberRate_URL</th>\n",
              "      <th>NumberRate_Domain</th>\n",
              "      <th>NumberRate_DirectoryName</th>\n",
              "      <th>NumberRate_FileName</th>\n",
              "      <th>SymbolCount_URL</th>\n",
              "      <th>SymbolCount_Domain</th>\n",
              "      <th>SymbolCount_Directoryname</th>\n",
              "      <th>SymbolCount_FileName</th>\n",
              "      <th>SymbolCount_Extension</th>\n",
              "      <th>SymbolCount_Afterpath</th>\n",
              "      <th>Entropy_URL</th>\n",
              "      <th>Entropy_Domain</th>\n",
              "      <th>URL_Type_obf_Type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>36702</th>\n",
              "      <td>29</td>\n",
              "      <td>4</td>\n",
              "      <td>14</td>\n",
              "      <td>5.750000</td>\n",
              "      <td>12</td>\n",
              "      <td>4</td>\n",
              "      <td>20</td>\n",
              "      <td>24</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>146</td>\n",
              "      <td>26</td>\n",
              "      <td>113</td>\n",
              "      <td>113</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>85</td>\n",
              "      <td>0.773973</td>\n",
              "      <td>0.582192</td>\n",
              "      <td>3.269231</td>\n",
              "      <td>0.178082</td>\n",
              "      <td>4.346154</td>\n",
              "      <td>0.7522124</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>5</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>23</td>\n",
              "      <td>31</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>27</td>\n",
              "      <td>3</td>\n",
              "      <td>94</td>\n",
              "      <td>23</td>\n",
              "      <td>46</td>\n",
              "      <td>7</td>\n",
              "      <td>14</td>\n",
              "      <td>24</td>\n",
              "      <td>43</td>\n",
              "      <td>12</td>\n",
              "      <td>11</td>\n",
              "      <td>11</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>0.212329</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.064516</td>\n",
              "      <td>0.529412</td>\n",
              "      <td>19</td>\n",
              "      <td>3</td>\n",
              "      <td>11</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>7</td>\n",
              "      <td>0.690555</td>\n",
              "      <td>0.791265</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36703</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>8</td>\n",
              "      <td>4</td>\n",
              "      <td>24</td>\n",
              "      <td>23</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>147</td>\n",
              "      <td>18</td>\n",
              "      <td>122</td>\n",
              "      <td>122</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0.829932</td>\n",
              "      <td>0.013605</td>\n",
              "      <td>0.111111</td>\n",
              "      <td>0.122449</td>\n",
              "      <td>6.777778</td>\n",
              "      <td>0.016393442</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>5</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>-1</td>\n",
              "      <td>21</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>21</td>\n",
              "      <td>-1</td>\n",
              "      <td>101</td>\n",
              "      <td>15</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>69</td>\n",
              "      <td>-1</td>\n",
              "      <td>105</td>\n",
              "      <td>8</td>\n",
              "      <td>9</td>\n",
              "      <td>9</td>\n",
              "      <td>-1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.187500</td>\n",
              "      <td>23</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>16</td>\n",
              "      <td>15</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.665492</td>\n",
              "      <td>0.820010</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36704</th>\n",
              "      <td>58</td>\n",
              "      <td>3</td>\n",
              "      <td>27</td>\n",
              "      <td>6.666666</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>41</td>\n",
              "      <td>34</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>18</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>12</td>\n",
              "      <td>246</td>\n",
              "      <td>22</td>\n",
              "      <td>217</td>\n",
              "      <td>217</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>182</td>\n",
              "      <td>0.882114</td>\n",
              "      <td>0.739837</td>\n",
              "      <td>8.272727</td>\n",
              "      <td>0.089431</td>\n",
              "      <td>9.863636</td>\n",
              "      <td>0.83870965</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>7</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.772727</td>\n",
              "      <td>58</td>\n",
              "      <td>57</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>51</td>\n",
              "      <td>1</td>\n",
              "      <td>156</td>\n",
              "      <td>20</td>\n",
              "      <td>71</td>\n",
              "      <td>3</td>\n",
              "      <td>58</td>\n",
              "      <td>48</td>\n",
              "      <td>118</td>\n",
              "      <td>16</td>\n",
              "      <td>12</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>12</td>\n",
              "      <td>0</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.231707</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073171</td>\n",
              "      <td>0.377778</td>\n",
              "      <td>26</td>\n",
              "      <td>2</td>\n",
              "      <td>14</td>\n",
              "      <td>8</td>\n",
              "      <td>7</td>\n",
              "      <td>9</td>\n",
              "      <td>0.656807</td>\n",
              "      <td>0.801139</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36705</th>\n",
              "      <td>35</td>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "      <td>4.333334</td>\n",
              "      <td>9</td>\n",
              "      <td>3</td>\n",
              "      <td>15</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>116</td>\n",
              "      <td>15</td>\n",
              "      <td>94</td>\n",
              "      <td>94</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>71</td>\n",
              "      <td>0.810345</td>\n",
              "      <td>0.612069</td>\n",
              "      <td>4.733333</td>\n",
              "      <td>0.129310</td>\n",
              "      <td>6.266667</td>\n",
              "      <td>0.7553192</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>3</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>32</td>\n",
              "      <td>25</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>25</td>\n",
              "      <td>23</td>\n",
              "      <td>73</td>\n",
              "      <td>13</td>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>41</td>\n",
              "      <td>12</td>\n",
              "      <td>75</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>8</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0.215517</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.284091</td>\n",
              "      <td>14</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>9</td>\n",
              "      <td>8</td>\n",
              "      <td>3</td>\n",
              "      <td>0.725963</td>\n",
              "      <td>0.897617</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36706</th>\n",
              "      <td>40</td>\n",
              "      <td>3</td>\n",
              "      <td>25</td>\n",
              "      <td>6.666666</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>35</td>\n",
              "      <td>31</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>17</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>227</td>\n",
              "      <td>22</td>\n",
              "      <td>198</td>\n",
              "      <td>198</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>164</td>\n",
              "      <td>0.872247</td>\n",
              "      <td>0.722467</td>\n",
              "      <td>7.454546</td>\n",
              "      <td>0.096916</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>0.82828283</td>\n",
              "      <td>0</td>\n",
              "      <td>-1</td>\n",
              "      <td>6</td>\n",
              "      <td>-1</td>\n",
              "      <td>0.772727</td>\n",
              "      <td>40</td>\n",
              "      <td>52</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>45</td>\n",
              "      <td>2</td>\n",
              "      <td>144</td>\n",
              "      <td>20</td>\n",
              "      <td>50</td>\n",
              "      <td>6</td>\n",
              "      <td>64</td>\n",
              "      <td>31</td>\n",
              "      <td>118</td>\n",
              "      <td>16</td>\n",
              "      <td>10</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>11</td>\n",
              "      <td>0</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>0.229075</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.083333</td>\n",
              "      <td>0.365079</td>\n",
              "      <td>24</td>\n",
              "      <td>2</td>\n",
              "      <td>13</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>7</td>\n",
              "      <td>0.674351</td>\n",
              "      <td>0.801139</td>\n",
              "      <td>spam</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       Querylength  domain_token_count  ...  Entropy_Domain  URL_Type_obf_Type\n",
              "36702           29                   4  ...        0.791265               spam\n",
              "36703            0                   4  ...        0.820010               spam\n",
              "36704           58                   3  ...        0.801139               spam\n",
              "36705           35                   3  ...        0.897617               spam\n",
              "36706           40                   3  ...        0.801139               spam\n",
              "\n",
              "[5 rows x 73 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "f2zcKS_DAGcx"
      },
      "source": [
        "  ## Experimenting with Final Dataset/All.csv\n",
        "  \n",
        "  #### Total Samples for each Type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NuFeBCWJABj9",
        "outputId": "e775b6b3-a7cb-476b-aec7-ece7e343525c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "label = 'URL_Type_obf_Type'\n",
        "lblTypes=set(df1[label])\n",
        "for lbl in lblTypes:\n",
        "    print('| {} | {} |'.format(lbl, len(df1[df1[label] == lbl].index)))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| phishing | 7577 |\n",
            "| Defacement | 7930 |\n",
            "| benign | 7781 |\n",
            "| malware | 6711 |\n",
            "| spam | 6698 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KlTOWuf8A_Yy",
        "colab": {}
      },
      "source": [
        "dataPath = 'FinalDataset'\n",
        "dep_var = label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B1keV97eplxW",
        "colab_type": "text"
      },
      "source": [
        "### Cast column values to float\n",
        "> Values in this column register as object type, which isn't valid for testing, so cast them to float"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2nFBzqafCKkG",
        "colab": {}
      },
      "source": [
        "df1.argPathRatio = df1['argPathRatio'].astype('float')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AaUqSRUiItig"
      },
      "source": [
        "## Experimenting with Tensorflow Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4c-GxIVIIZAR"
      },
      "source": [
        "#### Globals for Testing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x-fMOCE2I7MI",
        "colab": {}
      },
      "source": [
        "dataFile = 'All.csv'\n",
        "epochs=100\n",
        "batch_size=64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxNlAPk2plxh",
        "colab_type": "text"
      },
      "source": [
        "#### Show uncoded label column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfKJek8splxi",
        "colab_type": "code",
        "outputId": "9f09b1f2-21e9-4f81-907d-067462efe694",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "df1[dep_var]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        Defacement\n",
              "1        Defacement\n",
              "2        Defacement\n",
              "3        Defacement\n",
              "4        Defacement\n",
              "5        Defacement\n",
              "6        Defacement\n",
              "7        Defacement\n",
              "8        Defacement\n",
              "9        Defacement\n",
              "10       Defacement\n",
              "11       Defacement\n",
              "12       Defacement\n",
              "13       Defacement\n",
              "14       Defacement\n",
              "15       Defacement\n",
              "16       Defacement\n",
              "17       Defacement\n",
              "18       Defacement\n",
              "19       Defacement\n",
              "20       Defacement\n",
              "21       Defacement\n",
              "22       Defacement\n",
              "23       Defacement\n",
              "24       Defacement\n",
              "25       Defacement\n",
              "26       Defacement\n",
              "27       Defacement\n",
              "28       Defacement\n",
              "29       Defacement\n",
              "            ...    \n",
              "36677          spam\n",
              "36678          spam\n",
              "36679          spam\n",
              "36680          spam\n",
              "36681          spam\n",
              "36682          spam\n",
              "36683          spam\n",
              "36684          spam\n",
              "36685          spam\n",
              "36686          spam\n",
              "36687          spam\n",
              "36688          spam\n",
              "36689          spam\n",
              "36690          spam\n",
              "36691          spam\n",
              "36692          spam\n",
              "36693          spam\n",
              "36694          spam\n",
              "36695          spam\n",
              "36696          spam\n",
              "36697          spam\n",
              "36698          spam\n",
              "36699          spam\n",
              "36700          spam\n",
              "36701          spam\n",
              "36702          spam\n",
              "36703          spam\n",
              "36704          spam\n",
              "36705          spam\n",
              "36706          spam\n",
              "Name: URL_Type_obf_Type, Length: 36697, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ED-PijliFjJL"
      },
      "source": [
        "#### Logging Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCau-nq_plxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "time_gen = int(time.time())\n",
        "\n",
        "global model_name\n",
        "model_name = f\"{dataFile}_{time_gen}\"\n",
        "\n",
        "tensorboard = TensorBoard(log_dir='keras_tensorflow_logs/{}'.format(model_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_kcEzRL7ULC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikcFoxzcplxt",
        "colab_type": "text"
      },
      "source": [
        "#### Random seed for splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41We5rvNplxu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxvO-5wqplx1",
        "colab_type": "text"
      },
      "source": [
        "### Run the experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wDKDVrhZplx2",
        "colab_type": "code",
        "outputId": "dff06dfc-8280-4961-fcc5-331eba0dd01e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "categorical_df = df1.copy()\n",
        "model, history, X , encoded_y = experiment(categorical_df)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0716 22:41:32.811184 139875824584576 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Running fold #1\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33025 samples, validate on 3672 samples\n",
            "Epoch 1/100\n",
            "33025/33025 [==============================] - 2s 64us/sample - loss: 1.0173 - acc: 0.6345 - val_loss: 0.5092 - val_acc: 0.8292\n",
            "Epoch 2/100\n",
            "33025/33025 [==============================] - 1s 44us/sample - loss: 0.6688 - acc: 0.7536 - val_loss: 0.4146 - val_acc: 0.8486\n",
            "Epoch 3/100\n",
            "33025/33025 [==============================] - 1s 44us/sample - loss: 0.5798 - acc: 0.7878 - val_loss: 0.3631 - val_acc: 0.8769\n",
            "Epoch 4/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.5179 - acc: 0.8111 - val_loss: 0.3235 - val_acc: 0.8968\n",
            "Epoch 5/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.4758 - acc: 0.8272 - val_loss: 0.2938 - val_acc: 0.8998\n",
            "Epoch 6/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.4486 - acc: 0.8386 - val_loss: 0.2701 - val_acc: 0.9082\n",
            "Epoch 7/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.4262 - acc: 0.8478 - val_loss: 0.2562 - val_acc: 0.9145\n",
            "Epoch 8/100\n",
            "33025/33025 [==============================] - 1s 44us/sample - loss: 0.4125 - acc: 0.8532 - val_loss: 0.2486 - val_acc: 0.9115\n",
            "Epoch 9/100\n",
            "33025/33025 [==============================] - 1s 45us/sample - loss: 0.3966 - acc: 0.8614 - val_loss: 0.2413 - val_acc: 0.9188\n",
            "Epoch 10/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.3777 - acc: 0.8669 - val_loss: 0.2287 - val_acc: 0.9210\n",
            "Epoch 11/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.3615 - acc: 0.8744 - val_loss: 0.2320 - val_acc: 0.9145\n",
            "Epoch 12/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.3577 - acc: 0.8746 - val_loss: 0.2148 - val_acc: 0.9218\n",
            "Epoch 13/100\n",
            "33025/33025 [==============================] - 1s 41us/sample - loss: 0.3512 - acc: 0.8764 - val_loss: 0.2096 - val_acc: 0.9259\n",
            "Epoch 14/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.3363 - acc: 0.8833 - val_loss: 0.2012 - val_acc: 0.9286\n",
            "Epoch 15/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.3324 - acc: 0.8847 - val_loss: 0.1963 - val_acc: 0.9341\n",
            "Epoch 16/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.3341 - acc: 0.8853 - val_loss: 0.1952 - val_acc: 0.9344\n",
            "Epoch 17/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.3201 - acc: 0.8879 - val_loss: 0.1898 - val_acc: 0.9341\n",
            "Epoch 18/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.3173 - acc: 0.8880 - val_loss: 0.1840 - val_acc: 0.9368\n",
            "Epoch 19/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.3133 - acc: 0.8903 - val_loss: 0.1856 - val_acc: 0.9344\n",
            "Epoch 20/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.3088 - acc: 0.8916 - val_loss: 0.1800 - val_acc: 0.9376\n",
            "Epoch 21/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.3069 - acc: 0.8944 - val_loss: 0.1813 - val_acc: 0.9387\n",
            "Epoch 22/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2998 - acc: 0.8948 - val_loss: 0.1797 - val_acc: 0.9382\n",
            "Epoch 23/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2998 - acc: 0.8983 - val_loss: 0.1777 - val_acc: 0.9382\n",
            "Epoch 24/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2953 - acc: 0.8983 - val_loss: 0.1723 - val_acc: 0.9390\n",
            "Epoch 25/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2865 - acc: 0.9020 - val_loss: 0.1677 - val_acc: 0.9412\n",
            "Epoch 26/100\n",
            "33025/33025 [==============================] - 1s 41us/sample - loss: 0.2914 - acc: 0.8990 - val_loss: 0.1677 - val_acc: 0.9453\n",
            "Epoch 27/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2897 - acc: 0.9022 - val_loss: 0.1680 - val_acc: 0.9436\n",
            "Epoch 28/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2858 - acc: 0.9015 - val_loss: 0.1642 - val_acc: 0.9450\n",
            "Epoch 29/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2791 - acc: 0.9043 - val_loss: 0.1621 - val_acc: 0.9493\n",
            "Epoch 30/100\n",
            "33025/33025 [==============================] - 1s 41us/sample - loss: 0.2815 - acc: 0.9020 - val_loss: 0.1670 - val_acc: 0.9417\n",
            "Epoch 31/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2739 - acc: 0.9075 - val_loss: 0.1581 - val_acc: 0.9469\n",
            "Epoch 32/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2711 - acc: 0.9071 - val_loss: 0.1565 - val_acc: 0.9444\n",
            "Epoch 33/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2704 - acc: 0.9079 - val_loss: 0.1588 - val_acc: 0.9469\n",
            "Epoch 34/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2714 - acc: 0.9075 - val_loss: 0.1521 - val_acc: 0.9493\n",
            "Epoch 35/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2719 - acc: 0.9093 - val_loss: 0.1513 - val_acc: 0.9496\n",
            "Epoch 36/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2618 - acc: 0.9106 - val_loss: 0.1537 - val_acc: 0.9477\n",
            "Epoch 37/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2625 - acc: 0.9101 - val_loss: 0.1488 - val_acc: 0.9504\n",
            "Epoch 38/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2607 - acc: 0.9120 - val_loss: 0.1560 - val_acc: 0.9477\n",
            "Epoch 39/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2589 - acc: 0.9128 - val_loss: 0.1501 - val_acc: 0.9496\n",
            "Epoch 40/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2540 - acc: 0.9132 - val_loss: 0.1520 - val_acc: 0.9469\n",
            "Epoch 41/100\n",
            "33025/33025 [==============================] - 1s 41us/sample - loss: 0.2544 - acc: 0.9137 - val_loss: 0.1486 - val_acc: 0.9513\n",
            "Epoch 42/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2537 - acc: 0.9140 - val_loss: 0.1469 - val_acc: 0.9513\n",
            "Epoch 43/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2568 - acc: 0.9119 - val_loss: 0.1435 - val_acc: 0.9510\n",
            "Epoch 44/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2505 - acc: 0.9133 - val_loss: 0.1446 - val_acc: 0.9523\n",
            "Epoch 45/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2531 - acc: 0.9139 - val_loss: 0.1459 - val_acc: 0.9502\n",
            "Epoch 46/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2550 - acc: 0.9127 - val_loss: 0.1449 - val_acc: 0.9507\n",
            "Epoch 47/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2476 - acc: 0.9162 - val_loss: 0.1443 - val_acc: 0.9496\n",
            "Epoch 48/100\n",
            "33025/33025 [==============================] - 1s 41us/sample - loss: 0.2503 - acc: 0.9158 - val_loss: 0.1407 - val_acc: 0.9504\n",
            "Epoch 49/100\n",
            "33025/33025 [==============================] - 1s 41us/sample - loss: 0.2503 - acc: 0.9153 - val_loss: 0.1398 - val_acc: 0.9518\n",
            "Epoch 50/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2494 - acc: 0.9158 - val_loss: 0.1436 - val_acc: 0.9521\n",
            "Epoch 51/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2443 - acc: 0.9179 - val_loss: 0.1387 - val_acc: 0.9542\n",
            "Epoch 52/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2504 - acc: 0.9150 - val_loss: 0.1486 - val_acc: 0.9485\n",
            "Epoch 53/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2417 - acc: 0.9180 - val_loss: 0.1405 - val_acc: 0.9545\n",
            "Epoch 54/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2416 - acc: 0.9167 - val_loss: 0.1412 - val_acc: 0.9548\n",
            "Epoch 55/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2442 - acc: 0.9172 - val_loss: 0.1355 - val_acc: 0.9556\n",
            "Epoch 56/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2390 - acc: 0.9194 - val_loss: 0.1427 - val_acc: 0.9521\n",
            "Epoch 57/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2367 - acc: 0.9186 - val_loss: 0.1432 - val_acc: 0.9507\n",
            "Epoch 58/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2407 - acc: 0.9192 - val_loss: 0.1411 - val_acc: 0.9534\n",
            "Epoch 59/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2432 - acc: 0.9176 - val_loss: 0.1380 - val_acc: 0.9540\n",
            "Epoch 60/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2433 - acc: 0.9183 - val_loss: 0.1336 - val_acc: 0.9553\n",
            "Epoch 61/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2385 - acc: 0.9200 - val_loss: 0.1300 - val_acc: 0.9559\n",
            "Epoch 62/100\n",
            "33025/33025 [==============================] - 1s 43us/sample - loss: 0.2389 - acc: 0.9190 - val_loss: 0.1345 - val_acc: 0.9559\n",
            "Epoch 63/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2333 - acc: 0.9207 - val_loss: 0.1404 - val_acc: 0.9504\n",
            "Epoch 64/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2357 - acc: 0.9196 - val_loss: 0.1376 - val_acc: 0.9556\n",
            "Epoch 65/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2348 - acc: 0.9208 - val_loss: 0.1340 - val_acc: 0.9521\n",
            "Epoch 66/100\n",
            "33025/33025 [==============================] - 1s 42us/sample - loss: 0.2320 - acc: 0.9219 - val_loss: 0.1341 - val_acc: 0.9545\n",
            "Running fold #2\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 1.0029 - acc: 0.6404 - val_loss: 0.5490 - val_acc: 0.8030\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.6695 - acc: 0.7569 - val_loss: 0.4532 - val_acc: 0.8384\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.5782 - acc: 0.7908 - val_loss: 0.3903 - val_acc: 0.8629\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.5222 - acc: 0.8115 - val_loss: 0.3572 - val_acc: 0.8695\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.4820 - acc: 0.8279 - val_loss: 0.3327 - val_acc: 0.8847\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4589 - acc: 0.8348 - val_loss: 0.3107 - val_acc: 0.8891\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.4432 - acc: 0.8429 - val_loss: 0.2914 - val_acc: 0.8981\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.4204 - acc: 0.8489 - val_loss: 0.2791 - val_acc: 0.9022\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.4029 - acc: 0.8552 - val_loss: 0.2598 - val_acc: 0.9109\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3880 - acc: 0.8631 - val_loss: 0.2608 - val_acc: 0.9109\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3811 - acc: 0.8666 - val_loss: 0.2471 - val_acc: 0.9093\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.3700 - acc: 0.8676 - val_loss: 0.2379 - val_acc: 0.9177\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3586 - acc: 0.8747 - val_loss: 0.2297 - val_acc: 0.9232\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3532 - acc: 0.8766 - val_loss: 0.2250 - val_acc: 0.9215\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3370 - acc: 0.8818 - val_loss: 0.2223 - val_acc: 0.9183\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3361 - acc: 0.8812 - val_loss: 0.2075 - val_acc: 0.9292\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3332 - acc: 0.8836 - val_loss: 0.2036 - val_acc: 0.9294\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.3223 - acc: 0.8875 - val_loss: 0.2013 - val_acc: 0.9286\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3200 - acc: 0.8881 - val_loss: 0.2066 - val_acc: 0.9300\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.3144 - acc: 0.8905 - val_loss: 0.2006 - val_acc: 0.9330\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3073 - acc: 0.8937 - val_loss: 0.1968 - val_acc: 0.9327\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3142 - acc: 0.8915 - val_loss: 0.1931 - val_acc: 0.9322\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.3060 - acc: 0.8926 - val_loss: 0.1882 - val_acc: 0.9390\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3044 - acc: 0.8956 - val_loss: 0.1891 - val_acc: 0.9384\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2955 - acc: 0.8985 - val_loss: 0.1836 - val_acc: 0.9422\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3055 - acc: 0.8952 - val_loss: 0.1862 - val_acc: 0.9351\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2925 - acc: 0.8985 - val_loss: 0.1867 - val_acc: 0.9335\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2884 - acc: 0.9003 - val_loss: 0.1810 - val_acc: 0.9390\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2860 - acc: 0.9011 - val_loss: 0.1829 - val_acc: 0.9401\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2833 - acc: 0.9011 - val_loss: 0.1773 - val_acc: 0.9414\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2855 - acc: 0.9024 - val_loss: 0.1796 - val_acc: 0.9384\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2862 - acc: 0.9015 - val_loss: 0.1803 - val_acc: 0.9401\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2831 - acc: 0.9036 - val_loss: 0.1805 - val_acc: 0.9395\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2785 - acc: 0.9047 - val_loss: 0.1728 - val_acc: 0.9431\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2753 - acc: 0.9047 - val_loss: 0.1774 - val_acc: 0.9387\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2779 - acc: 0.9035 - val_loss: 0.1683 - val_acc: 0.9447\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2725 - acc: 0.9065 - val_loss: 0.1763 - val_acc: 0.9373\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2657 - acc: 0.9090 - val_loss: 0.1696 - val_acc: 0.9414\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2750 - acc: 0.9064 - val_loss: 0.1674 - val_acc: 0.9447\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2695 - acc: 0.9070 - val_loss: 0.1627 - val_acc: 0.9469\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2670 - acc: 0.9096 - val_loss: 0.1627 - val_acc: 0.9439\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2733 - acc: 0.9056 - val_loss: 0.1656 - val_acc: 0.9436\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2705 - acc: 0.9067 - val_loss: 0.1641 - val_acc: 0.9455\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2633 - acc: 0.9111 - val_loss: 0.1577 - val_acc: 0.9455\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2640 - acc: 0.9092 - val_loss: 0.1650 - val_acc: 0.9439\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2636 - acc: 0.9105 - val_loss: 0.1584 - val_acc: 0.9447\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2562 - acc: 0.9121 - val_loss: 0.1595 - val_acc: 0.9469\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2674 - acc: 0.9077 - val_loss: 0.1592 - val_acc: 0.9480\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2629 - acc: 0.9101 - val_loss: 0.1567 - val_acc: 0.9447\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2634 - acc: 0.9112 - val_loss: 0.1558 - val_acc: 0.9460\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2498 - acc: 0.9143 - val_loss: 0.1565 - val_acc: 0.9477\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2605 - acc: 0.9107 - val_loss: 0.1579 - val_acc: 0.9455\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2571 - acc: 0.9128 - val_loss: 0.1580 - val_acc: 0.9433\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2534 - acc: 0.9140 - val_loss: 0.1540 - val_acc: 0.9501\n",
            "Epoch 55/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2500 - acc: 0.9146 - val_loss: 0.1453 - val_acc: 0.9485\n",
            "Epoch 56/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2581 - acc: 0.9117 - val_loss: 0.1548 - val_acc: 0.9439\n",
            "Epoch 57/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2473 - acc: 0.9148 - val_loss: 0.1491 - val_acc: 0.9485\n",
            "Epoch 58/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2444 - acc: 0.9152 - val_loss: 0.1520 - val_acc: 0.9493\n",
            "Epoch 59/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2480 - acc: 0.9157 - val_loss: 0.1571 - val_acc: 0.9452\n",
            "Epoch 60/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2522 - acc: 0.9153 - val_loss: 0.1534 - val_acc: 0.9439\n",
            "Running fold #3\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 1.0020 - acc: 0.6386 - val_loss: 0.5369 - val_acc: 0.8207\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.6666 - acc: 0.7586 - val_loss: 0.4368 - val_acc: 0.8460\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.5763 - acc: 0.7907 - val_loss: 0.3829 - val_acc: 0.8681\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.5204 - acc: 0.8132 - val_loss: 0.3450 - val_acc: 0.8681\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.4873 - acc: 0.8240 - val_loss: 0.3188 - val_acc: 0.8850\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.4546 - acc: 0.8385 - val_loss: 0.3096 - val_acc: 0.8891\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.4299 - acc: 0.8450 - val_loss: 0.2817 - val_acc: 0.9027\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.4159 - acc: 0.8546 - val_loss: 0.2679 - val_acc: 0.9025\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3959 - acc: 0.8621 - val_loss: 0.2645 - val_acc: 0.9027\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.3919 - acc: 0.8634 - val_loss: 0.2559 - val_acc: 0.9106\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3862 - acc: 0.8648 - val_loss: 0.2508 - val_acc: 0.9098\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.3669 - acc: 0.8739 - val_loss: 0.2417 - val_acc: 0.9128\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3574 - acc: 0.8758 - val_loss: 0.2370 - val_acc: 0.9147\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3428 - acc: 0.8804 - val_loss: 0.2272 - val_acc: 0.9210\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3480 - acc: 0.8771 - val_loss: 0.2216 - val_acc: 0.9218\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3422 - acc: 0.8791 - val_loss: 0.2157 - val_acc: 0.9229\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3289 - acc: 0.8856 - val_loss: 0.2222 - val_acc: 0.9221\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3224 - acc: 0.8884 - val_loss: 0.2198 - val_acc: 0.9174\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3171 - acc: 0.8909 - val_loss: 0.2131 - val_acc: 0.9259\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3308 - acc: 0.8860 - val_loss: 0.2075 - val_acc: 0.9311\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.3265 - acc: 0.8888 - val_loss: 0.2092 - val_acc: 0.9234\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3141 - acc: 0.8914 - val_loss: 0.2015 - val_acc: 0.9289\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3104 - acc: 0.8941 - val_loss: 0.2017 - val_acc: 0.9330\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3159 - acc: 0.8917 - val_loss: 0.2012 - val_acc: 0.9335\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2989 - acc: 0.8978 - val_loss: 0.1927 - val_acc: 0.9379\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3005 - acc: 0.8966 - val_loss: 0.1987 - val_acc: 0.9349\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2941 - acc: 0.8999 - val_loss: 0.1913 - val_acc: 0.9343\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2950 - acc: 0.9005 - val_loss: 0.1914 - val_acc: 0.9346\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2917 - acc: 0.9000 - val_loss: 0.1882 - val_acc: 0.9349\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2883 - acc: 0.9024 - val_loss: 0.1839 - val_acc: 0.9384\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2861 - acc: 0.9017 - val_loss: 0.1817 - val_acc: 0.9387\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2822 - acc: 0.9046 - val_loss: 0.1883 - val_acc: 0.9357\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2773 - acc: 0.9058 - val_loss: 0.1826 - val_acc: 0.9351\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2722 - acc: 0.9069 - val_loss: 0.1824 - val_acc: 0.9327\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2765 - acc: 0.9060 - val_loss: 0.1732 - val_acc: 0.9433\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2790 - acc: 0.9056 - val_loss: 0.1731 - val_acc: 0.9417\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2819 - acc: 0.9054 - val_loss: 0.1739 - val_acc: 0.9406\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2766 - acc: 0.9054 - val_loss: 0.1765 - val_acc: 0.9390\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2922 - acc: 0.8994 - val_loss: 0.1850 - val_acc: 0.9379\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2736 - acc: 0.9072 - val_loss: 0.1786 - val_acc: 0.9401\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2716 - acc: 0.9071 - val_loss: 0.1682 - val_acc: 0.9428\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2730 - acc: 0.9072 - val_loss: 0.1686 - val_acc: 0.9409\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2674 - acc: 0.9084 - val_loss: 0.1748 - val_acc: 0.9401\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2692 - acc: 0.9092 - val_loss: 0.1688 - val_acc: 0.9398\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2617 - acc: 0.9102 - val_loss: 0.1660 - val_acc: 0.9444\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2635 - acc: 0.9113 - val_loss: 0.1678 - val_acc: 0.9395\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2616 - acc: 0.9123 - val_loss: 0.1632 - val_acc: 0.9422\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2673 - acc: 0.9099 - val_loss: 0.1654 - val_acc: 0.9425\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 1s 42us/sample - loss: 0.2608 - acc: 0.9118 - val_loss: 0.1639 - val_acc: 0.9409\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2541 - acc: 0.9148 - val_loss: 0.1598 - val_acc: 0.9471\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2521 - acc: 0.9150 - val_loss: 0.1584 - val_acc: 0.9477\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2552 - acc: 0.9136 - val_loss: 0.1639 - val_acc: 0.9417\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2525 - acc: 0.9132 - val_loss: 0.1672 - val_acc: 0.9444\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2500 - acc: 0.9134 - val_loss: 0.1664 - val_acc: 0.9409\n",
            "Epoch 55/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.2506 - acc: 0.9167 - val_loss: 0.1621 - val_acc: 0.9439\n",
            "Epoch 56/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2496 - acc: 0.9174 - val_loss: 0.1600 - val_acc: 0.9444\n",
            "Running fold #4\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.9967 - acc: 0.6415 - val_loss: 0.5510 - val_acc: 0.8123\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.6647 - acc: 0.7559 - val_loss: 0.4499 - val_acc: 0.8311\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 45us/sample - loss: 0.5787 - acc: 0.7898 - val_loss: 0.3933 - val_acc: 0.8523\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.5187 - acc: 0.8136 - val_loss: 0.3517 - val_acc: 0.8749\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.4832 - acc: 0.8293 - val_loss: 0.3312 - val_acc: 0.8861\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.4586 - acc: 0.8360 - val_loss: 0.3094 - val_acc: 0.8965\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.4335 - acc: 0.8467 - val_loss: 0.2923 - val_acc: 0.9005\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.4190 - acc: 0.8515 - val_loss: 0.2820 - val_acc: 0.9052\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.4052 - acc: 0.8579 - val_loss: 0.2674 - val_acc: 0.9033\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 1s 43us/sample - loss: 0.3859 - acc: 0.8660 - val_loss: 0.2591 - val_acc: 0.9106\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3753 - acc: 0.8697 - val_loss: 0.2484 - val_acc: 0.9117\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3694 - acc: 0.8700 - val_loss: 0.2498 - val_acc: 0.9134\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3550 - acc: 0.8766 - val_loss: 0.2350 - val_acc: 0.9215\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3552 - acc: 0.8752 - val_loss: 0.2326 - val_acc: 0.9204\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3356 - acc: 0.8852 - val_loss: 0.2225 - val_acc: 0.9262\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3406 - acc: 0.8834 - val_loss: 0.2251 - val_acc: 0.9278\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3336 - acc: 0.8841 - val_loss: 0.2295 - val_acc: 0.9215\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3222 - acc: 0.8902 - val_loss: 0.2202 - val_acc: 0.9272\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.3225 - acc: 0.8882 - val_loss: 0.2071 - val_acc: 0.9316\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3147 - acc: 0.8921 - val_loss: 0.2043 - val_acc: 0.9330\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3099 - acc: 0.8941 - val_loss: 0.1996 - val_acc: 0.9330\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3060 - acc: 0.8946 - val_loss: 0.2015 - val_acc: 0.9330\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3058 - acc: 0.8954 - val_loss: 0.1954 - val_acc: 0.9362\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3011 - acc: 0.8958 - val_loss: 0.1943 - val_acc: 0.9373\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2953 - acc: 0.8971 - val_loss: 0.1896 - val_acc: 0.9373\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2920 - acc: 0.9006 - val_loss: 0.1867 - val_acc: 0.9343\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2924 - acc: 0.9009 - val_loss: 0.1883 - val_acc: 0.9354\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2870 - acc: 0.9009 - val_loss: 0.1823 - val_acc: 0.9365\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2822 - acc: 0.9020 - val_loss: 0.1862 - val_acc: 0.9381\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2932 - acc: 0.9009 - val_loss: 0.1894 - val_acc: 0.9351\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2892 - acc: 0.9012 - val_loss: 0.1778 - val_acc: 0.9395\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2809 - acc: 0.9051 - val_loss: 0.1764 - val_acc: 0.9373\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2787 - acc: 0.9067 - val_loss: 0.1820 - val_acc: 0.9406\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2763 - acc: 0.9068 - val_loss: 0.1783 - val_acc: 0.9392\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2677 - acc: 0.9058 - val_loss: 0.1750 - val_acc: 0.9406\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2728 - acc: 0.9058 - val_loss: 0.1723 - val_acc: 0.9439\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 1s 44us/sample - loss: 0.2823 - acc: 0.9025 - val_loss: 0.1798 - val_acc: 0.9387\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2676 - acc: 0.9084 - val_loss: 0.1715 - val_acc: 0.9444\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2672 - acc: 0.9110 - val_loss: 0.1695 - val_acc: 0.9436\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2685 - acc: 0.9107 - val_loss: 0.1667 - val_acc: 0.9428\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2616 - acc: 0.9107 - val_loss: 0.1681 - val_acc: 0.9436\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2773 - acc: 0.9048 - val_loss: 0.1730 - val_acc: 0.9441\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2666 - acc: 0.9107 - val_loss: 0.1677 - val_acc: 0.9439\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2575 - acc: 0.9110 - val_loss: 0.1719 - val_acc: 0.9455\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2546 - acc: 0.9146 - val_loss: 0.1637 - val_acc: 0.9436\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2684 - acc: 0.9090 - val_loss: 0.1709 - val_acc: 0.9452\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2585 - acc: 0.9125 - val_loss: 0.1676 - val_acc: 0.9455\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2592 - acc: 0.9131 - val_loss: 0.1638 - val_acc: 0.9477\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2558 - acc: 0.9129 - val_loss: 0.1634 - val_acc: 0.9477\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 2s 45us/sample - loss: 0.2505 - acc: 0.9160 - val_loss: 0.1642 - val_acc: 0.9452\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2524 - acc: 0.9156 - val_loss: 0.1641 - val_acc: 0.9471\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2483 - acc: 0.9159 - val_loss: 0.1619 - val_acc: 0.9450\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2503 - acc: 0.9146 - val_loss: 0.1558 - val_acc: 0.9510\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2527 - acc: 0.9119 - val_loss: 0.1595 - val_acc: 0.9460\n",
            "Epoch 55/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2495 - acc: 0.9172 - val_loss: 0.1600 - val_acc: 0.9490\n",
            "Epoch 56/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2489 - acc: 0.9151 - val_loss: 0.1569 - val_acc: 0.9469\n",
            "Epoch 57/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2493 - acc: 0.9153 - val_loss: 0.1568 - val_acc: 0.9474\n",
            "Epoch 58/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2480 - acc: 0.9158 - val_loss: 0.1567 - val_acc: 0.9458\n",
            "Running fold #5\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 1.0195 - acc: 0.6344 - val_loss: 0.5435 - val_acc: 0.8025\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.6682 - acc: 0.7570 - val_loss: 0.4439 - val_acc: 0.8414\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.5769 - acc: 0.7935 - val_loss: 0.3878 - val_acc: 0.8564\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.5229 - acc: 0.8129 - val_loss: 0.3411 - val_acc: 0.8828\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4790 - acc: 0.8288 - val_loss: 0.3197 - val_acc: 0.8801\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4526 - acc: 0.8391 - val_loss: 0.2994 - val_acc: 0.8894\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.4271 - acc: 0.8469 - val_loss: 0.2897 - val_acc: 0.8951\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.4073 - acc: 0.8559 - val_loss: 0.2736 - val_acc: 0.9049\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3991 - acc: 0.8592 - val_loss: 0.2606 - val_acc: 0.9057\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3869 - acc: 0.8632 - val_loss: 0.2448 - val_acc: 0.9131\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3692 - acc: 0.8712 - val_loss: 0.2400 - val_acc: 0.9120\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3557 - acc: 0.8766 - val_loss: 0.2361 - val_acc: 0.9161\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3452 - acc: 0.8808 - val_loss: 0.2264 - val_acc: 0.9213\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3440 - acc: 0.8806 - val_loss: 0.2176 - val_acc: 0.9193\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3393 - acc: 0.8824 - val_loss: 0.2149 - val_acc: 0.9292\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3339 - acc: 0.8815 - val_loss: 0.2110 - val_acc: 0.9262\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3214 - acc: 0.8895 - val_loss: 0.2154 - val_acc: 0.9202\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3186 - acc: 0.8889 - val_loss: 0.1968 - val_acc: 0.9316\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3133 - acc: 0.8895 - val_loss: 0.2035 - val_acc: 0.9259\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3090 - acc: 0.8941 - val_loss: 0.1974 - val_acc: 0.9281\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3049 - acc: 0.8931 - val_loss: 0.1960 - val_acc: 0.9300\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3065 - acc: 0.8937 - val_loss: 0.1945 - val_acc: 0.9294\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3049 - acc: 0.8944 - val_loss: 0.1981 - val_acc: 0.9300\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3054 - acc: 0.8959 - val_loss: 0.1886 - val_acc: 0.9327\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2922 - acc: 0.8995 - val_loss: 0.1814 - val_acc: 0.9401\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2871 - acc: 0.9019 - val_loss: 0.1820 - val_acc: 0.9343\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2809 - acc: 0.9030 - val_loss: 0.1858 - val_acc: 0.9371\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2855 - acc: 0.9014 - val_loss: 0.1824 - val_acc: 0.9411\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2757 - acc: 0.9067 - val_loss: 0.1765 - val_acc: 0.9371\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2746 - acc: 0.9054 - val_loss: 0.1748 - val_acc: 0.9376\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2751 - acc: 0.9054 - val_loss: 0.1770 - val_acc: 0.9371\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2775 - acc: 0.9058 - val_loss: 0.1716 - val_acc: 0.9417\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2721 - acc: 0.9083 - val_loss: 0.1717 - val_acc: 0.9379\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2738 - acc: 0.9057 - val_loss: 0.1707 - val_acc: 0.9431\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.2834 - acc: 0.9032 - val_loss: 0.1818 - val_acc: 0.9346\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.2750 - acc: 0.9074 - val_loss: 0.1739 - val_acc: 0.9403\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.2759 - acc: 0.9056 - val_loss: 0.1761 - val_acc: 0.9384\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2691 - acc: 0.9074 - val_loss: 0.1706 - val_acc: 0.9411\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2748 - acc: 0.9054 - val_loss: 0.1725 - val_acc: 0.9390\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2661 - acc: 0.9093 - val_loss: 0.1692 - val_acc: 0.9433\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2681 - acc: 0.9076 - val_loss: 0.1724 - val_acc: 0.9395\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2630 - acc: 0.9109 - val_loss: 0.1668 - val_acc: 0.9439\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2600 - acc: 0.9108 - val_loss: 0.1631 - val_acc: 0.9471\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2584 - acc: 0.9113 - val_loss: 0.1623 - val_acc: 0.9469\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2524 - acc: 0.9131 - val_loss: 0.1610 - val_acc: 0.9417\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2537 - acc: 0.9145 - val_loss: 0.1583 - val_acc: 0.9474\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2504 - acc: 0.9158 - val_loss: 0.1575 - val_acc: 0.9477\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2527 - acc: 0.9156 - val_loss: 0.1553 - val_acc: 0.9482\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2478 - acc: 0.9163 - val_loss: 0.1560 - val_acc: 0.9444\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2532 - acc: 0.9141 - val_loss: 0.1554 - val_acc: 0.9482\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2479 - acc: 0.9163 - val_loss: 0.1550 - val_acc: 0.9469\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2479 - acc: 0.9160 - val_loss: 0.1561 - val_acc: 0.9455\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2453 - acc: 0.9161 - val_loss: 0.1559 - val_acc: 0.9488\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2496 - acc: 0.9153 - val_loss: 0.1566 - val_acc: 0.9450\n",
            "Epoch 55/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2502 - acc: 0.9146 - val_loss: 0.1585 - val_acc: 0.9444\n",
            "Epoch 56/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2439 - acc: 0.9179 - val_loss: 0.1582 - val_acc: 0.9460\n",
            "Running fold #6\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 1.0098 - acc: 0.6318 - val_loss: 0.5342 - val_acc: 0.8202\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.6627 - acc: 0.7570 - val_loss: 0.4377 - val_acc: 0.8436\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.5755 - acc: 0.7935 - val_loss: 0.3699 - val_acc: 0.8717\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.5227 - acc: 0.8142 - val_loss: 0.3390 - val_acc: 0.8888\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.4856 - acc: 0.8277 - val_loss: 0.3148 - val_acc: 0.8888\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.4524 - acc: 0.8403 - val_loss: 0.2948 - val_acc: 0.8973\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.4324 - acc: 0.8487 - val_loss: 0.2839 - val_acc: 0.8992\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.4090 - acc: 0.8551 - val_loss: 0.2663 - val_acc: 0.9114\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4043 - acc: 0.8584 - val_loss: 0.2558 - val_acc: 0.9090\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3833 - acc: 0.8667 - val_loss: 0.2431 - val_acc: 0.9172\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3677 - acc: 0.8705 - val_loss: 0.2415 - val_acc: 0.9153\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3613 - acc: 0.8739 - val_loss: 0.2277 - val_acc: 0.9202\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3519 - acc: 0.8777 - val_loss: 0.2208 - val_acc: 0.9256\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3493 - acc: 0.8782 - val_loss: 0.2211 - val_acc: 0.9213\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 45us/sample - loss: 0.3483 - acc: 0.8794 - val_loss: 0.2122 - val_acc: 0.9275\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3328 - acc: 0.8834 - val_loss: 0.2134 - val_acc: 0.9292\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 45us/sample - loss: 0.3286 - acc: 0.8853 - val_loss: 0.2112 - val_acc: 0.9264\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3191 - acc: 0.8902 - val_loss: 0.2111 - val_acc: 0.9319\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 45us/sample - loss: 0.3215 - acc: 0.8884 - val_loss: 0.2081 - val_acc: 0.9327\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3190 - acc: 0.8911 - val_loss: 0.1951 - val_acc: 0.9349\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.3108 - acc: 0.8933 - val_loss: 0.1917 - val_acc: 0.9414\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3026 - acc: 0.8956 - val_loss: 0.1988 - val_acc: 0.9365\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3026 - acc: 0.8967 - val_loss: 0.1877 - val_acc: 0.9401\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2967 - acc: 0.8956 - val_loss: 0.1939 - val_acc: 0.9354\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3004 - acc: 0.8983 - val_loss: 0.1915 - val_acc: 0.9384\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2895 - acc: 0.9030 - val_loss: 0.1816 - val_acc: 0.9447\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2934 - acc: 0.8995 - val_loss: 0.1873 - val_acc: 0.9417\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2901 - acc: 0.9000 - val_loss: 0.1830 - val_acc: 0.9425\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2891 - acc: 0.9023 - val_loss: 0.1826 - val_acc: 0.9431\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2814 - acc: 0.9030 - val_loss: 0.1839 - val_acc: 0.9436\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2809 - acc: 0.9031 - val_loss: 0.1759 - val_acc: 0.9441\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2823 - acc: 0.9032 - val_loss: 0.1812 - val_acc: 0.9425\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2740 - acc: 0.9046 - val_loss: 0.1775 - val_acc: 0.9458\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2799 - acc: 0.9035 - val_loss: 0.1686 - val_acc: 0.9441\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2696 - acc: 0.9096 - val_loss: 0.1706 - val_acc: 0.9450\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2735 - acc: 0.9057 - val_loss: 0.1668 - val_acc: 0.9458\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2703 - acc: 0.9066 - val_loss: 0.1763 - val_acc: 0.9420\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2706 - acc: 0.9077 - val_loss: 0.1713 - val_acc: 0.9452\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2698 - acc: 0.9083 - val_loss: 0.1679 - val_acc: 0.9477\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2633 - acc: 0.9117 - val_loss: 0.1727 - val_acc: 0.9433\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2626 - acc: 0.9105 - val_loss: 0.1656 - val_acc: 0.9431\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.2689 - acc: 0.9086 - val_loss: 0.1686 - val_acc: 0.9455\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.2660 - acc: 0.9090 - val_loss: 0.1686 - val_acc: 0.9496\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2584 - acc: 0.9108 - val_loss: 0.1660 - val_acc: 0.9460\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2606 - acc: 0.9108 - val_loss: 0.1637 - val_acc: 0.9428\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2541 - acc: 0.9126 - val_loss: 0.1651 - val_acc: 0.9441\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2548 - acc: 0.9128 - val_loss: 0.1596 - val_acc: 0.9485\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2627 - acc: 0.9113 - val_loss: 0.1671 - val_acc: 0.9469\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2554 - acc: 0.9139 - val_loss: 0.1563 - val_acc: 0.9501\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2531 - acc: 0.9148 - val_loss: 0.1619 - val_acc: 0.9455\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2491 - acc: 0.9151 - val_loss: 0.1580 - val_acc: 0.9471\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2559 - acc: 0.9130 - val_loss: 0.1621 - val_acc: 0.9463\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2513 - acc: 0.9150 - val_loss: 0.1627 - val_acc: 0.9485\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2443 - acc: 0.9171 - val_loss: 0.1602 - val_acc: 0.9482\n",
            "Running fold #7\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 2s 72us/sample - loss: 1.0163 - acc: 0.6306 - val_loss: 0.5339 - val_acc: 0.8082\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.6762 - acc: 0.7514 - val_loss: 0.4326 - val_acc: 0.8450\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.5776 - acc: 0.7878 - val_loss: 0.3711 - val_acc: 0.8736\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.5259 - acc: 0.8094 - val_loss: 0.3348 - val_acc: 0.8837\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.4939 - acc: 0.8223 - val_loss: 0.3192 - val_acc: 0.8978\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 45us/sample - loss: 0.4632 - acc: 0.8338 - val_loss: 0.2913 - val_acc: 0.9038\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.4357 - acc: 0.8468 - val_loss: 0.2782 - val_acc: 0.9123\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4251 - acc: 0.8483 - val_loss: 0.2667 - val_acc: 0.9120\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.4002 - acc: 0.8596 - val_loss: 0.2492 - val_acc: 0.9131\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.3908 - acc: 0.8616 - val_loss: 0.2431 - val_acc: 0.9177\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 50us/sample - loss: 0.3750 - acc: 0.8679 - val_loss: 0.2321 - val_acc: 0.9199\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3683 - acc: 0.8703 - val_loss: 0.2263 - val_acc: 0.9207\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3564 - acc: 0.8751 - val_loss: 0.2210 - val_acc: 0.9283\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3438 - acc: 0.8789 - val_loss: 0.2148 - val_acc: 0.9256\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3372 - acc: 0.8813 - val_loss: 0.2085 - val_acc: 0.9294\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3354 - acc: 0.8817 - val_loss: 0.2067 - val_acc: 0.9338\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3337 - acc: 0.8841 - val_loss: 0.2030 - val_acc: 0.9332\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3246 - acc: 0.8871 - val_loss: 0.1960 - val_acc: 0.9332\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.3196 - acc: 0.8892 - val_loss: 0.1927 - val_acc: 0.9360\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.3186 - acc: 0.8903 - val_loss: 0.1845 - val_acc: 0.9390\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.3144 - acc: 0.8915 - val_loss: 0.1873 - val_acc: 0.9368\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3197 - acc: 0.8892 - val_loss: 0.1854 - val_acc: 0.9379\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3026 - acc: 0.8947 - val_loss: 0.1807 - val_acc: 0.9384\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.3002 - acc: 0.8964 - val_loss: 0.1798 - val_acc: 0.9436\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2960 - acc: 0.8974 - val_loss: 0.1781 - val_acc: 0.9444\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2897 - acc: 0.9017 - val_loss: 0.1840 - val_acc: 0.9371\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2910 - acc: 0.9009 - val_loss: 0.1771 - val_acc: 0.9447\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2893 - acc: 0.9007 - val_loss: 0.1703 - val_acc: 0.9433\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2890 - acc: 0.9006 - val_loss: 0.1692 - val_acc: 0.9425\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2837 - acc: 0.9034 - val_loss: 0.1725 - val_acc: 0.9428\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2817 - acc: 0.9033 - val_loss: 0.1692 - val_acc: 0.9420\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2858 - acc: 0.9014 - val_loss: 0.1687 - val_acc: 0.9439\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2812 - acc: 0.9041 - val_loss: 0.1663 - val_acc: 0.9441\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2854 - acc: 0.9016 - val_loss: 0.1618 - val_acc: 0.9441\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2726 - acc: 0.9070 - val_loss: 0.1607 - val_acc: 0.9417\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2742 - acc: 0.9064 - val_loss: 0.1618 - val_acc: 0.9450\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2730 - acc: 0.9057 - val_loss: 0.1604 - val_acc: 0.9441\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2758 - acc: 0.9060 - val_loss: 0.1609 - val_acc: 0.9471\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2672 - acc: 0.9097 - val_loss: 0.1566 - val_acc: 0.9485\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2624 - acc: 0.9106 - val_loss: 0.1581 - val_acc: 0.9455\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2679 - acc: 0.9076 - val_loss: 0.1585 - val_acc: 0.9444\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2646 - acc: 0.9078 - val_loss: 0.1572 - val_acc: 0.9485\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2681 - acc: 0.9073 - val_loss: 0.1553 - val_acc: 0.9485\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2584 - acc: 0.9124 - val_loss: 0.1532 - val_acc: 0.9501\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2570 - acc: 0.9121 - val_loss: 0.1532 - val_acc: 0.9471\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2569 - acc: 0.9123 - val_loss: 0.1516 - val_acc: 0.9471\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2557 - acc: 0.9124 - val_loss: 0.1604 - val_acc: 0.9474\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2547 - acc: 0.9150 - val_loss: 0.1499 - val_acc: 0.9507\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2505 - acc: 0.9133 - val_loss: 0.1487 - val_acc: 0.9510\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2586 - acc: 0.9133 - val_loss: 0.1472 - val_acc: 0.9501\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2467 - acc: 0.9163 - val_loss: 0.1500 - val_acc: 0.9496\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2671 - acc: 0.9109 - val_loss: 0.1491 - val_acc: 0.9496\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 1s 45us/sample - loss: 0.2539 - acc: 0.9156 - val_loss: 0.1499 - val_acc: 0.9493\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2506 - acc: 0.9164 - val_loss: 0.1452 - val_acc: 0.9537\n",
            "Epoch 55/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2430 - acc: 0.9178 - val_loss: 0.1435 - val_acc: 0.9499\n",
            "Epoch 56/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2471 - acc: 0.9156 - val_loss: 0.1430 - val_acc: 0.9534\n",
            "Epoch 57/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2632 - acc: 0.9105 - val_loss: 0.1446 - val_acc: 0.9529\n",
            "Epoch 58/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2548 - acc: 0.9129 - val_loss: 0.1438 - val_acc: 0.9542\n",
            "Epoch 59/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2468 - acc: 0.9164 - val_loss: 0.1418 - val_acc: 0.9518\n",
            "Epoch 60/100\n",
            "33027/33027 [==============================] - 2s 46us/sample - loss: 0.2580 - acc: 0.9099 - val_loss: 0.1417 - val_acc: 0.9567\n",
            "Epoch 61/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2476 - acc: 0.9157 - val_loss: 0.1383 - val_acc: 0.9569\n",
            "Epoch 62/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2493 - acc: 0.9153 - val_loss: 0.1391 - val_acc: 0.9545\n",
            "Epoch 63/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2415 - acc: 0.9174 - val_loss: 0.1362 - val_acc: 0.9561\n",
            "Epoch 64/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2397 - acc: 0.9183 - val_loss: 0.1439 - val_acc: 0.9548\n",
            "Epoch 65/100\n",
            "33027/33027 [==============================] - 2s 48us/sample - loss: 0.2403 - acc: 0.9191 - val_loss: 0.1437 - val_acc: 0.9501\n",
            "Epoch 66/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2345 - acc: 0.9204 - val_loss: 0.1453 - val_acc: 0.9501\n",
            "Epoch 67/100\n",
            "33027/33027 [==============================] - 2s 47us/sample - loss: 0.2359 - acc: 0.9201 - val_loss: 0.1410 - val_acc: 0.9518\n",
            "Epoch 68/100\n",
            "33027/33027 [==============================] - 2s 49us/sample - loss: 0.2401 - acc: 0.9187 - val_loss: 0.1399 - val_acc: 0.9534\n",
            "Running fold #8\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 2s 72us/sample - loss: 0.9993 - acc: 0.6375 - val_loss: 0.5686 - val_acc: 0.7811\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.6683 - acc: 0.7540 - val_loss: 0.4651 - val_acc: 0.8373\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.5762 - acc: 0.7890 - val_loss: 0.4051 - val_acc: 0.8621\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.5222 - acc: 0.8110 - val_loss: 0.3656 - val_acc: 0.8686\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.4833 - acc: 0.8229 - val_loss: 0.3363 - val_acc: 0.8798\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.4514 - acc: 0.8367 - val_loss: 0.3211 - val_acc: 0.8874\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.4365 - acc: 0.8431 - val_loss: 0.3022 - val_acc: 0.8956\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.4143 - acc: 0.8509 - val_loss: 0.2856 - val_acc: 0.9035\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.3964 - acc: 0.8585 - val_loss: 0.2798 - val_acc: 0.9024\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.3796 - acc: 0.8671 - val_loss: 0.2655 - val_acc: 0.9057\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.3721 - acc: 0.8684 - val_loss: 0.2596 - val_acc: 0.9092\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.3639 - acc: 0.8714 - val_loss: 0.2445 - val_acc: 0.9171\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3482 - acc: 0.8779 - val_loss: 0.2440 - val_acc: 0.9166\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3434 - acc: 0.8805 - val_loss: 0.2359 - val_acc: 0.9207\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3397 - acc: 0.8811 - val_loss: 0.2290 - val_acc: 0.9218\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3311 - acc: 0.8839 - val_loss: 0.2218 - val_acc: 0.9275\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.3285 - acc: 0.8860 - val_loss: 0.2136 - val_acc: 0.9324\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.3178 - acc: 0.8894 - val_loss: 0.2167 - val_acc: 0.9286\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3139 - acc: 0.8918 - val_loss: 0.2097 - val_acc: 0.9308\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.3185 - acc: 0.8895 - val_loss: 0.2103 - val_acc: 0.9330\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3117 - acc: 0.8915 - val_loss: 0.2005 - val_acc: 0.9335\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3121 - acc: 0.8939 - val_loss: 0.2047 - val_acc: 0.9332\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.3036 - acc: 0.8950 - val_loss: 0.2018 - val_acc: 0.9324\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.3004 - acc: 0.8983 - val_loss: 0.1969 - val_acc: 0.9338\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.2993 - acc: 0.8975 - val_loss: 0.2030 - val_acc: 0.9316\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.2907 - acc: 0.8995 - val_loss: 0.1966 - val_acc: 0.9338\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.2874 - acc: 0.9004 - val_loss: 0.1962 - val_acc: 0.9349\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.2865 - acc: 0.9024 - val_loss: 0.1934 - val_acc: 0.9365\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2858 - acc: 0.9021 - val_loss: 0.1862 - val_acc: 0.9370\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.2772 - acc: 0.9048 - val_loss: 0.1971 - val_acc: 0.9338\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.2828 - acc: 0.9032 - val_loss: 0.1925 - val_acc: 0.9338\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2799 - acc: 0.9035 - val_loss: 0.1924 - val_acc: 0.9362\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 49us/sample - loss: 0.2774 - acc: 0.9048 - val_loss: 0.1834 - val_acc: 0.9384\n",
            "Epoch 34/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.2781 - acc: 0.9057 - val_loss: 0.1832 - val_acc: 0.9376\n",
            "Epoch 35/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2760 - acc: 0.9063 - val_loss: 0.1853 - val_acc: 0.9384\n",
            "Epoch 36/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2680 - acc: 0.9086 - val_loss: 0.1810 - val_acc: 0.9419\n",
            "Epoch 37/100\n",
            "33028/33028 [==============================] - 2s 51us/sample - loss: 0.2679 - acc: 0.9073 - val_loss: 0.1768 - val_acc: 0.9411\n",
            "Epoch 38/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2675 - acc: 0.9088 - val_loss: 0.1758 - val_acc: 0.9447\n",
            "Epoch 39/100\n",
            "33028/33028 [==============================] - 2s 50us/sample - loss: 0.2764 - acc: 0.9070 - val_loss: 0.1795 - val_acc: 0.9376\n",
            "Epoch 40/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.2748 - acc: 0.9061 - val_loss: 0.1786 - val_acc: 0.9414\n",
            "Epoch 41/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.2688 - acc: 0.9084 - val_loss: 0.1741 - val_acc: 0.9428\n",
            "Epoch 42/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.2605 - acc: 0.9102 - val_loss: 0.1670 - val_acc: 0.9463\n",
            "Epoch 43/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.2621 - acc: 0.9112 - val_loss: 0.1700 - val_acc: 0.9458\n",
            "Epoch 44/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.2642 - acc: 0.9087 - val_loss: 0.1698 - val_acc: 0.9449\n",
            "Epoch 45/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.2564 - acc: 0.9120 - val_loss: 0.1687 - val_acc: 0.9455\n",
            "Epoch 46/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.2545 - acc: 0.9121 - val_loss: 0.1720 - val_acc: 0.9449\n",
            "Epoch 47/100\n",
            "33028/33028 [==============================] - 2s 48us/sample - loss: 0.2589 - acc: 0.9114 - val_loss: 0.1728 - val_acc: 0.9414\n",
            "Running fold #9\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33029 samples, validate on 3668 samples\n",
            "Epoch 1/100\n",
            "33029/33029 [==============================] - 2s 73us/sample - loss: 1.0143 - acc: 0.6308 - val_loss: 0.5401 - val_acc: 0.8138\n",
            "Epoch 2/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.6700 - acc: 0.7507 - val_loss: 0.4411 - val_acc: 0.8367\n",
            "Epoch 3/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.5834 - acc: 0.7885 - val_loss: 0.3892 - val_acc: 0.8626\n",
            "Epoch 4/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.5283 - acc: 0.8082 - val_loss: 0.3496 - val_acc: 0.8803\n",
            "Epoch 5/100\n",
            "33029/33029 [==============================] - 2s 48us/sample - loss: 0.4899 - acc: 0.8254 - val_loss: 0.3187 - val_acc: 0.8915\n",
            "Epoch 6/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.4585 - acc: 0.8357 - val_loss: 0.2977 - val_acc: 0.8972\n",
            "Epoch 7/100\n",
            "33029/33029 [==============================] - 2s 48us/sample - loss: 0.4396 - acc: 0.8434 - val_loss: 0.2831 - val_acc: 0.9027\n",
            "Epoch 8/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.4218 - acc: 0.8506 - val_loss: 0.2705 - val_acc: 0.9073\n",
            "Epoch 9/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3957 - acc: 0.8568 - val_loss: 0.2510 - val_acc: 0.9177\n",
            "Epoch 10/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3916 - acc: 0.8627 - val_loss: 0.2461 - val_acc: 0.9144\n",
            "Epoch 11/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3761 - acc: 0.8682 - val_loss: 0.2365 - val_acc: 0.9171\n",
            "Epoch 12/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3661 - acc: 0.8717 - val_loss: 0.2327 - val_acc: 0.9190\n",
            "Epoch 13/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3486 - acc: 0.8789 - val_loss: 0.2151 - val_acc: 0.9245\n",
            "Epoch 14/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.3445 - acc: 0.8817 - val_loss: 0.2111 - val_acc: 0.9324\n",
            "Epoch 15/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3418 - acc: 0.8793 - val_loss: 0.2150 - val_acc: 0.9275\n",
            "Epoch 16/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.3335 - acc: 0.8828 - val_loss: 0.2027 - val_acc: 0.9308\n",
            "Epoch 17/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.3255 - acc: 0.8870 - val_loss: 0.2046 - val_acc: 0.9327\n",
            "Epoch 18/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3225 - acc: 0.8887 - val_loss: 0.2014 - val_acc: 0.9351\n",
            "Epoch 19/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3190 - acc: 0.8877 - val_loss: 0.1921 - val_acc: 0.9357\n",
            "Epoch 20/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3072 - acc: 0.8950 - val_loss: 0.1882 - val_acc: 0.9368\n",
            "Epoch 21/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.3005 - acc: 0.8943 - val_loss: 0.1824 - val_acc: 0.9414\n",
            "Epoch 22/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3029 - acc: 0.8966 - val_loss: 0.1827 - val_acc: 0.9417\n",
            "Epoch 23/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2971 - acc: 0.8975 - val_loss: 0.1782 - val_acc: 0.9406\n",
            "Epoch 24/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2989 - acc: 0.8981 - val_loss: 0.1779 - val_acc: 0.9400\n",
            "Epoch 25/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2921 - acc: 0.8989 - val_loss: 0.1760 - val_acc: 0.9403\n",
            "Epoch 26/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2881 - acc: 0.8996 - val_loss: 0.1734 - val_acc: 0.9427\n",
            "Epoch 27/100\n",
            "33029/33029 [==============================] - 2s 48us/sample - loss: 0.2848 - acc: 0.9010 - val_loss: 0.1791 - val_acc: 0.9387\n",
            "Epoch 28/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2819 - acc: 0.9029 - val_loss: 0.1745 - val_acc: 0.9419\n",
            "Epoch 29/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2908 - acc: 0.9018 - val_loss: 0.1664 - val_acc: 0.9455\n",
            "Epoch 30/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2843 - acc: 0.9020 - val_loss: 0.1715 - val_acc: 0.9400\n",
            "Epoch 31/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2782 - acc: 0.9033 - val_loss: 0.1579 - val_acc: 0.9507\n",
            "Epoch 32/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2740 - acc: 0.9059 - val_loss: 0.1554 - val_acc: 0.9498\n",
            "Epoch 33/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2703 - acc: 0.9070 - val_loss: 0.1681 - val_acc: 0.9419\n",
            "Epoch 34/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2724 - acc: 0.9060 - val_loss: 0.1570 - val_acc: 0.9471\n",
            "Epoch 35/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2770 - acc: 0.9061 - val_loss: 0.1583 - val_acc: 0.9487\n",
            "Epoch 36/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2654 - acc: 0.9090 - val_loss: 0.1618 - val_acc: 0.9444\n",
            "Epoch 37/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2637 - acc: 0.9087 - val_loss: 0.1546 - val_acc: 0.9482\n",
            "Epoch 38/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2652 - acc: 0.9099 - val_loss: 0.1591 - val_acc: 0.9477\n",
            "Epoch 39/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2616 - acc: 0.9108 - val_loss: 0.1645 - val_acc: 0.9436\n",
            "Epoch 40/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2583 - acc: 0.9118 - val_loss: 0.1528 - val_acc: 0.9496\n",
            "Epoch 41/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2567 - acc: 0.9115 - val_loss: 0.1553 - val_acc: 0.9468\n",
            "Epoch 42/100\n",
            "33029/33029 [==============================] - 2s 58us/sample - loss: 0.2578 - acc: 0.9107 - val_loss: 0.1475 - val_acc: 0.9528\n",
            "Epoch 43/100\n",
            "33029/33029 [==============================] - 2s 58us/sample - loss: 0.2567 - acc: 0.9125 - val_loss: 0.1499 - val_acc: 0.9517\n",
            "Epoch 44/100\n",
            "33029/33029 [==============================] - 2s 59us/sample - loss: 0.2578 - acc: 0.9123 - val_loss: 0.1466 - val_acc: 0.9496\n",
            "Epoch 45/100\n",
            "33029/33029 [==============================] - 2s 57us/sample - loss: 0.2503 - acc: 0.9140 - val_loss: 0.1465 - val_acc: 0.9558\n",
            "Epoch 46/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2561 - acc: 0.9133 - val_loss: 0.1483 - val_acc: 0.9537\n",
            "Epoch 47/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2529 - acc: 0.9155 - val_loss: 0.1482 - val_acc: 0.9504\n",
            "Epoch 48/100\n",
            "33029/33029 [==============================] - 2s 54us/sample - loss: 0.2632 - acc: 0.9093 - val_loss: 0.1435 - val_acc: 0.9523\n",
            "Epoch 49/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2495 - acc: 0.9156 - val_loss: 0.1520 - val_acc: 0.9485\n",
            "Epoch 50/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2472 - acc: 0.9165 - val_loss: 0.1478 - val_acc: 0.9517\n",
            "Epoch 51/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2477 - acc: 0.9153 - val_loss: 0.1420 - val_acc: 0.9526\n",
            "Epoch 52/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2451 - acc: 0.9187 - val_loss: 0.1463 - val_acc: 0.9517\n",
            "Epoch 53/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2422 - acc: 0.9170 - val_loss: 0.1392 - val_acc: 0.9528\n",
            "Epoch 54/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2501 - acc: 0.9157 - val_loss: 0.1462 - val_acc: 0.9507\n",
            "Epoch 55/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2416 - acc: 0.9173 - val_loss: 0.1424 - val_acc: 0.9550\n",
            "Epoch 56/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2438 - acc: 0.9177 - val_loss: 0.1355 - val_acc: 0.9572\n",
            "Epoch 57/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2418 - acc: 0.9181 - val_loss: 0.1420 - val_acc: 0.9542\n",
            "Epoch 58/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2432 - acc: 0.9175 - val_loss: 0.1441 - val_acc: 0.9542\n",
            "Epoch 59/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2494 - acc: 0.9140 - val_loss: 0.1414 - val_acc: 0.9534\n",
            "Epoch 60/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2433 - acc: 0.9186 - val_loss: 0.1418 - val_acc: 0.9553\n",
            "Epoch 61/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2394 - acc: 0.9193 - val_loss: 0.1405 - val_acc: 0.9572\n",
            "Running fold #10\n",
            "Categorical Cross-Entropy Loss Function\n",
            "Train on 33029 samples, validate on 3668 samples\n",
            "Epoch 1/100\n",
            "33029/33029 [==============================] - 3s 79us/sample - loss: 1.0138 - acc: 0.6380 - val_loss: 0.5287 - val_acc: 0.8173\n",
            "Epoch 2/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.6771 - acc: 0.7525 - val_loss: 0.4318 - val_acc: 0.8541\n",
            "Epoch 3/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.5815 - acc: 0.7905 - val_loss: 0.3730 - val_acc: 0.8713\n",
            "Epoch 4/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.5254 - acc: 0.8117 - val_loss: 0.3324 - val_acc: 0.8855\n",
            "Epoch 5/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.4872 - acc: 0.8273 - val_loss: 0.3113 - val_acc: 0.8915\n",
            "Epoch 6/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.4563 - acc: 0.8385 - val_loss: 0.2975 - val_acc: 0.9013\n",
            "Epoch 7/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.4343 - acc: 0.8457 - val_loss: 0.2685 - val_acc: 0.9062\n",
            "Epoch 8/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.4140 - acc: 0.8518 - val_loss: 0.2606 - val_acc: 0.9117\n",
            "Epoch 9/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.3965 - acc: 0.8608 - val_loss: 0.2399 - val_acc: 0.9171\n",
            "Epoch 10/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.3846 - acc: 0.8632 - val_loss: 0.2373 - val_acc: 0.9109\n",
            "Epoch 11/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3781 - acc: 0.8699 - val_loss: 0.2209 - val_acc: 0.9242\n",
            "Epoch 12/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.3647 - acc: 0.8718 - val_loss: 0.2164 - val_acc: 0.9204\n",
            "Epoch 13/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3537 - acc: 0.8750 - val_loss: 0.2144 - val_acc: 0.9275\n",
            "Epoch 14/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3481 - acc: 0.8792 - val_loss: 0.2054 - val_acc: 0.9256\n",
            "Epoch 15/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3396 - acc: 0.8811 - val_loss: 0.2022 - val_acc: 0.9335\n",
            "Epoch 16/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.3336 - acc: 0.8844 - val_loss: 0.1999 - val_acc: 0.9302\n",
            "Epoch 17/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.3267 - acc: 0.8866 - val_loss: 0.1902 - val_acc: 0.9318\n",
            "Epoch 18/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3210 - acc: 0.8884 - val_loss: 0.1855 - val_acc: 0.9362\n",
            "Epoch 19/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3176 - acc: 0.8913 - val_loss: 0.1863 - val_acc: 0.9316\n",
            "Epoch 20/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3122 - acc: 0.8919 - val_loss: 0.1818 - val_acc: 0.9357\n",
            "Epoch 21/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.3116 - acc: 0.8920 - val_loss: 0.1772 - val_acc: 0.9381\n",
            "Epoch 22/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.3106 - acc: 0.8925 - val_loss: 0.1768 - val_acc: 0.9378\n",
            "Epoch 23/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2993 - acc: 0.8948 - val_loss: 0.1721 - val_acc: 0.9378\n",
            "Epoch 24/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2944 - acc: 0.8984 - val_loss: 0.1707 - val_acc: 0.9392\n",
            "Epoch 25/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2935 - acc: 0.8976 - val_loss: 0.1699 - val_acc: 0.9370\n",
            "Epoch 26/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2948 - acc: 0.9000 - val_loss: 0.1711 - val_acc: 0.9351\n",
            "Epoch 27/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2968 - acc: 0.8978 - val_loss: 0.1621 - val_acc: 0.9474\n",
            "Epoch 28/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2877 - acc: 0.9001 - val_loss: 0.1598 - val_acc: 0.9430\n",
            "Epoch 29/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2845 - acc: 0.9025 - val_loss: 0.1565 - val_acc: 0.9466\n",
            "Epoch 30/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2827 - acc: 0.9035 - val_loss: 0.1578 - val_acc: 0.9471\n",
            "Epoch 31/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2855 - acc: 0.9008 - val_loss: 0.1544 - val_acc: 0.9477\n",
            "Epoch 32/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2836 - acc: 0.9026 - val_loss: 0.1515 - val_acc: 0.9498\n",
            "Epoch 33/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2763 - acc: 0.9053 - val_loss: 0.1522 - val_acc: 0.9485\n",
            "Epoch 34/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2779 - acc: 0.9034 - val_loss: 0.1581 - val_acc: 0.9438\n",
            "Epoch 35/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2682 - acc: 0.9064 - val_loss: 0.1536 - val_acc: 0.9411\n",
            "Epoch 36/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2702 - acc: 0.9071 - val_loss: 0.1472 - val_acc: 0.9498\n",
            "Epoch 37/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2668 - acc: 0.9088 - val_loss: 0.1463 - val_acc: 0.9512\n",
            "Epoch 38/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2666 - acc: 0.9081 - val_loss: 0.1443 - val_acc: 0.9512\n",
            "Epoch 39/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2716 - acc: 0.9074 - val_loss: 0.1458 - val_acc: 0.9515\n",
            "Epoch 40/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2625 - acc: 0.9108 - val_loss: 0.1401 - val_acc: 0.9528\n",
            "Epoch 41/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2617 - acc: 0.9114 - val_loss: 0.1427 - val_acc: 0.9520\n",
            "Epoch 42/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2651 - acc: 0.9095 - val_loss: 0.1461 - val_acc: 0.9455\n",
            "Epoch 43/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2651 - acc: 0.9090 - val_loss: 0.1424 - val_acc: 0.9531\n",
            "Epoch 44/100\n",
            "33029/33029 [==============================] - 2s 55us/sample - loss: 0.2617 - acc: 0.9110 - val_loss: 0.1399 - val_acc: 0.9504\n",
            "Epoch 45/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2641 - acc: 0.9095 - val_loss: 0.1422 - val_acc: 0.9490\n",
            "Epoch 46/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2564 - acc: 0.9128 - val_loss: 0.1369 - val_acc: 0.9545\n",
            "Epoch 47/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2572 - acc: 0.9125 - val_loss: 0.1344 - val_acc: 0.9545\n",
            "Epoch 48/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2600 - acc: 0.9108 - val_loss: 0.1375 - val_acc: 0.9520\n",
            "Epoch 49/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2537 - acc: 0.9127 - val_loss: 0.1348 - val_acc: 0.9534\n",
            "Epoch 50/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2526 - acc: 0.9146 - val_loss: 0.1390 - val_acc: 0.9512\n",
            "Epoch 51/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2494 - acc: 0.9146 - val_loss: 0.1325 - val_acc: 0.9556\n",
            "Epoch 52/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2474 - acc: 0.9144 - val_loss: 0.1328 - val_acc: 0.9556\n",
            "Epoch 53/100\n",
            "33029/33029 [==============================] - 2s 52us/sample - loss: 0.2510 - acc: 0.9147 - val_loss: 0.1374 - val_acc: 0.9512\n",
            "Epoch 54/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2544 - acc: 0.9125 - val_loss: 0.1310 - val_acc: 0.9586\n",
            "Epoch 55/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2584 - acc: 0.9122 - val_loss: 0.1341 - val_acc: 0.9537\n",
            "Epoch 56/100\n",
            "33029/33029 [==============================] - 2s 51us/sample - loss: 0.2433 - acc: 0.9177 - val_loss: 0.1252 - val_acc: 0.9577\n",
            "Epoch 57/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2473 - acc: 0.9161 - val_loss: 0.1342 - val_acc: 0.9542\n",
            "Epoch 58/100\n",
            "33029/33029 [==============================] - 2s 50us/sample - loss: 0.2426 - acc: 0.9194 - val_loss: 0.1357 - val_acc: 0.9542\n",
            "Epoch 59/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2437 - acc: 0.9158 - val_loss: 0.1274 - val_acc: 0.9572\n",
            "Epoch 60/100\n",
            "33029/33029 [==============================] - 2s 49us/sample - loss: 0.2462 - acc: 0.9163 - val_loss: 0.1279 - val_acc: 0.9556\n",
            "Epoch 61/100\n",
            "33029/33029 [==============================] - 2s 53us/sample - loss: 0.2363 - acc: 0.9187 - val_loss: 0.1260 - val_acc: 0.9599\n",
            "Time to complete 15 min 940.0597233772278 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-7-q5bOstri",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FoHKjaqpGQub"
      },
      "source": [
        "#### Save the model as a file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vixNeoaEGW9n",
        "colab": {}
      },
      "source": [
        "model.save('{}.model'.format(os.path.basename(dataPath)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1VG5cxSplyA",
        "colab_type": "text"
      },
      "source": [
        "### Model Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p3xvC6ZlG9Wz",
        "outputId": "5633da7a-c0db-43a7-95bf-9616981e6993",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "scores = model.evaluate(X,encoded_y, batch_size=batch_size,verbose=1)\n",
        "print(model.metrics_names)\n",
        "acc, loss = scores[1]*100, scores[0]\n",
        "print('Baseline: accuracy: {:.2f}%: loss: {:.2f}'.format(acc, loss))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36697/36697 [==============================] - 1s 22us/sample - loss: 0.1238 - acc: 0.9585\n",
            "['loss', 'acc']\n",
            "Baseline: accuracy: 95.85%: loss: 0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlNdQsyiplyF",
        "colab_type": "text"
      },
      "source": [
        "#### Generate predictions from the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0jLyQXHplyG",
        "colab_type": "code",
        "outputId": "e4a65036-9d64-4a10-b1e3-c362922fa1d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prediction_y = model.predict_classes(X, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36697/36697 [==============================] - 1s 22us/sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "44-OsF7tplyK",
        "colab_type": "text"
      },
      "source": [
        "#### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjsWSmWvplyM",
        "colab_type": "code",
        "outputId": "6ba2fe22-4b36-40a2-e4a1-ce1e20973143",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "y=LabelEncoder().fit_transform(categorical_df[dep_var].values)\n",
        "cm = confusion_matrix(y, prediction_y)\n",
        "sn.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Defacement','Benign','Malware','Phishing','Spam'],\n",
        "           yticklabels=['Defacement','Benign','Malware','Phishing','Spam'])"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f370fb3b7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAD8CAYAAABO3GKQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3Xd8FFXXwPHfSQNCkxqq0iJIURQQ\nKyBKB6lSLIDlxUKRJiIoTVEUFB9BEHhAQVFERUF6F5TepAnSlRo6JNQk5/1jh7ggSTaQzW72OV8+\n82H2zp2ZM7vZnL13bvaKqmKMMcZ4S5CvAzDGGBPYLNEYY4zxKks0xhhjvMoSjTHGGK+yRGOMMcar\nLNEYY4zxKks0xhhjvMoSjTHGGK+yRGOMMf8DRKSkiGxwW86ISGcR6SciB9zK67rt84aI7BSR7SJS\ny628tlO2U0R6Jntu+2YAz2W6u0NAPlnHVw3zdQipLkjE1yF4xeXYeF+HkOpCQwLz827GEG76hzAl\nv3POrx/u8flEJBg4AFQGngWiVXXINXVKA98A9wIFgPnA7c7mP4EawH5gNdBKVbcmdr4QTwMzxhgT\nMB4FdqnqPkn8Q1lDYJKqXgT2iMhOXEkHYKeq7gYQkUlO3UQTTWB+lDDGmEAgQZ4vKdMSV2vlig4i\nslFExolIDqesIPC3W539Tlli5YmyRGOMMf4qKNjjRUTaicgat6Xd9Q4pImHA48B3TtFIoDhQHjgE\nfJjal2FdZ8YY469ScK9RVUcDoz2oWgdYp6pHnP2O/HM6GQNMdx4eAAq77VfIKSOJ8uuyFo0xxvgr\n73SdtcKt20xE8rttawxsdtanAS1FJIOIFAUigVW4bv5HikhRp3XU0qmbKGvRGGOMv0rl0ZMikhnX\naLEX3Yo/EJHygAJ7r2xT1S0iMhnXTf5YoL2qxjnH6QDMAYKBcaq6JanzWqIxxhh/lfKb/ElS1Rgg\n1zVlzyRRfyAw8DrlM4GZnp7XEo0xxvirAPl7MEs0xhjjr4KCfR1BqrBEY4wx/iqVu858xRKNMcb4\nK+s6M8YY41XWojHGGONVlmiMMcZ4VbANBjDGGONNdo/GGGOMV1nXmUmJyNvy8uX7zyU8LlowF2+P\nnEH2rOE81+QBjp6MBqDv8GnM+XUrISFBjOzzFOVLFSYkOIiJM1YxZNxcCkXcwn/fbk3eXFlRhXE/\n/Man3yz20VUl7asJX/DjD98jIpSIjKT/O++xYf06Pv5wMPHx8YSHh9N/4Hvceuttvg7VY4cPHaL3\nGz04cfw4iNDsieY89UwbRn46jB++n0zOHDkB6Ni5Kw9XqerjaBN3+PAh+vbuyYkTxxGgcbPmtHqq\nNSOH/4dfFi8kKCiIHDly0u/t98iTNy979+ymf59ebPtjK6907MwzbZ5L9hz+KC4ujlbNm5I3IoLh\nI0b5OpzkBUiLJtkZNkUkDtgEhOL6vpsJwFBVTXKqPxEZDNQFZqrqa6kTrneISFtgrqoeTKpeas2w\nGRQk7JozkKqtB/PM4/cTc+4iH3+54Ko6LWpXpF61crTu+TmZMoay/oc3qfnCf7h0OZZ8ubOxYdt+\nsoRnYNnXr9O862i27T58w/F4Y4bNqCNHeLb1k/wwdQYZM2akR7fOPPRwFcaOGcXQT0ZQrHhxJk/6\nms2bNjJg4KBUP7+3Ztg8ejSKY0ePckfpMsTERNPyiaZ8/MmnzJ0zi/DwcNo8+7xXzntFas2weexo\nFMeOHaXUHWWIiYnhmZZNGfLxcPJG5CNLliwATJr4Jbt376LXW/04cfw4hw4dZPGiBWTLli1VE01a\nzrA54YvP2bplM9Ex0V5PNKkyw2bNwZ7PsDn3Nb/NSp68wudVtbyqlsH1ZWx1gL4e7NcOuNPfk4yj\nLa6pStPEI/eWZM/+o/x16GSidRQlPGMYwcFBZMoQxqXLcZyNucDhY2fYsG0/ANHnLrJtz2EK5Lkl\nrUJPkbjYOC5evEBsbCwXzp8nT568iAgxMa7W29mzZ8mTJ6+Po0yZPHnyckfpMgBkzpyFYsWKERV1\nJJm9/E/uPHkpdceV68hMkWLFiYo6kpBkAM5fOJ/wgTpnrlyUKVuOkJD02wly5PBhli5ZTOOmzXwd\niudEPF/8WIo+SqhqFK4E0kFcgkVksIisdmZnexFARKYBWYC1ItJCRBqIyEoRWS8i80UkwqmXRUQ+\nF5FNzv5NnfKaIrJcRNaJyHciksUp3ysi74nIBmdin3tEZI6I7BKRl67EKSKvucXU3ykrIiJ/iMgY\nEdkiInNFJJOINAMqAhOd42a6+ac1aU/UqsDk2WsTHr/Usgqrvn2Dz/o+xS1ZXaefMn895y5cYs+8\ngfw5awAfT1jAyTPnrjrOrflzUr5kIVZv3uvtkFMsb0QErds+R53HqlPjkYfJkjUr9z/4EH36v0PH\nl9tR69GqzPh5Gs++cN25mdKFAwf2s+2PPyh3510ATPp6Is0aN6DPm29w5vRpH0fnuYMHDrB92x+U\nLee6jk+HfUy9mo8wa8bPvPRKJx9Hl3o+GPQuXbq9RlBQOrrvkYKJz/xZip9xZ57oYCAv8DxwWlUr\nAZWA/xORoqr6OP+0hL4FfgXuU9W7gUlAD+dwbzn7l1PVO4GFIpIbeBN4TFXvAdYAXd1C+EtVywNL\ngS+AZsB9wJWEUhPXvAn34poxroKIVHH2jQQ+dVpnp4Cmqvq9c46nnHjPp/Q5SYnQkGDqVS3HlHnr\nARjz3VJKN+hH5ZaDOHzsDIO6NgGgUpkixMXFU6xmb+6o15dXn6lOkYL/fOlq5kxhfDPkBV4b8gNn\nYy54M+Qbcub0aRYvWsD0OfOZu3AJ58+fZ8bP05g4YTzDRo5mzoJfaNioCR9+kPrdZmnhXEwM3Tp3\n4rWevciSJQvNW7Ri+ux5TP5hKnny5GXI4PRxXefOxdCjWye6vdYzoTXTvmNnZsxdRJ16DZg8aaKP\nI0wdvyxeRM6cOSldpqyvQ0kZ703lnKZuNrqaQGsR2QCsxPX105HXqVcImCMim4DXgDJO+WPAp1cq\nqepJXEmjNPCbc9w2gPvd4isT7GwCVqrqWVU9ClwUkVucmGoC64F1QCm3mPao6gZnfS1QJLkLdJ8e\nNfZYklMueKTWQ6XZsO1vok6cBSDqxFni4xVVZdyU36hY1nWpzetUZO6yrcTGxnP0ZDTLN+ymQulb\nAQgJCeKbIf/Ht7PWMHXh7zcdkzesXLGcAgULkTNnTkJDQ6n+aA02rF/Hn9u3JbQAatapw+8b1vs4\n0pS7fPkyXTt3om69BjxWoyYAuXLnJjg4mKCgIJo0e4LNmzb5OMrkxV6+TI+ur1K7bgOqP1bzX9vr\n1K3PgvlzfRBZ6tuwfh2LFy+kTo3qvN69K6tXruCN17v7Oqzk/S92nQGISDEgDogCBOjotATKq2pR\nVb3eT+YwYLiqlsM1qU7GpE4BzHM7ZmlVdb/DetH5P95t/crjEGf/99z2L6GqY6/ZF+caku1wVtXR\nqlpRVSuG5C6TXPVkNa9d8apus3y5syWsN6x+F1t3HQJg/+ETVKtUEoDwjGHce2cRtu913Qv4rO9T\nbN9zmE++WnjT8XhLvvz52bTxd86fP4+qsmrlcooVL0509Fn27d0DwIplyyharJiPI00ZVaVfn94U\nK1aM1m2fTSg/ejQqYX3h/PmUiLze5y3/oaoM6PcmRYsV4+nWbRPK/9q3N2F98aKFFCmavl6fxLza\npRvzFi5h1ryFvD/kIypVvo/33h/i67CSFyAtmhTd2RORPMBnuJKGisgc4GURWaiql0XkduCAM7mO\nu+z8M6d0G7fyeUB7oLNz/BzACuBTESmhqjudGeEKquqfHoY5B3hbRCaqarSIFAQuJ7PPWSCrh8e/\nYeEZw6heuRQd3kmYRZWBrzbizpKFUFX2HTpBR2fbZ98uYXT/p1n7fW9E4MupK9i84yAPlC/GU/Ur\ns+nPA6yY1BP4Z0i0Pyl35108VqMmTzZvQnBwCKVK3UHTJ1oQEZGP7l06IRJEtmzZ6Pf2u74ONUXW\nr1vL9GlTibz9dpo3aQi4hjLPmjmd7du2IQIFChTkrX4DfBxp0n5fv46Z06dRIvJ2nmzeGIBXOnZm\n6o8/sG/vHoKCgsifvwBvvNkPgGPHjtK61RPExEQjQUF889UEJv84/arBA8YL/DyBeOpGhjd/CXyk\nqvEiEgS8AzTA1ZI4CjRS1dMiEq2qV27iNwSGAieBhUAlVa3m3OT/FKiAq4XRX1WniEh14H0ggxPG\nm6o6TUT2AhVV9ZgzJLmiqnZwzuG+7VXgBWffaOBp5/jTVbWsU787kEVV+zmDEN4FzgP3J3afJrWG\nN/sbbwxv9jVvDW/2tdQa3uxP0nJ4c1pKleHNDUd5Prx56ot++0OfbKIx/7BEk35Yokk/LNEkLlOj\n0Z4nmp/a+e0PffodFG+MMYEuQLrOLNEYY4y/CpCWuSUaY4zxU2KJxhhjjDdZojHGGONVEmSJxhhj\njBdZi8YYY4xXWaIxxhjjVZZojDHGeFdg5BlLNMYY46+sRWOMMcar0tUkbUmwRGOMMX7KWjTGGGO8\nKzDyjCUaY4zxV9aiMcYY41WBkmgC406TMcYEIAkSjxePjidyi4h8LyLbROQPEblfRHKKyDwR2eH8\nn8OpKyLyiYjsFJGNInKP23HaOPV3iEibxM/oYi2aFDi5erivQ/CKHJU6+DqEVHdiVWC+VsEB8t1X\nxjNeaNH8B5itqs1EJAwIB3oBC1R1kIj0BHoCrwN1gEhnqQyMBCqLSE6gL1ARUGCtiExT1ZOJndRa\nNMYY46dExOPFg2NlB6oAYwFU9ZKqngIaAuOdauOBRs56Q2CCuqwAbhGR/EAtYJ6qnnCSyzygdlLn\ntkRjjDF+KjUTDVAUOAp8LiLrReS/IpIZiFDVQ06dw0CEs14Q+Ntt//1OWWLlibJEY4wxfioliUZE\n2onIGrel3TWHCwHuAUaq6t1ADK5usgSqqri6w1KV3aMxxhh/lYJbNKo6GhidRJX9wH5VXek8/h5X\nojkiIvlV9ZDTNRblbD8AFHbbv5BTdgCodk354qRisxaNMcb4qaCgII+X5KjqYeBvESnpFD0KbAWm\nAVdGjrUBpjrr04DWzuiz+4DTThfbHKCmiORwRqjVdMoSZS0aY4zxU14YddYRmOiMONsNPIurwTFZ\nRJ4H9gHNnbozgbrATuCcUxdVPSEibwOrnXoDVPVEUie1RGOMMf4qlfOMqm7ANSz5Wo9ep64C7RM5\nzjhgnKfntURjjDF+KlC+GcASjTHG+ClLNMYYY7zKEo0xxhiv8vQ7zPydJRpjjPFT1qIxxhjjVZZo\njDHGeFWA5BlLNMYY46+sRWOMMcargmwwgDHGGG8KkAaNJRp/0OfNN1jyy2Jy5szFlKnTAXitW2f2\n7dkDwNmzZ8maNSuTp0xN6jA+EXlbXr58/7mEx0UL5uLtkTPInjWc55o8wNGT0QD0HT6NOb9uJTQk\nmOFvtuKe0rcSr/F0/+AHlq7dAUBoSDBDezanSsVI4uPj6ffpdH5asMEn15WUuLg4nmzRlLx5Ixg2\nYhRv9e7J2jWryJIlKwADBg6iVKk7fBxl8vq91YslS1w/d9//+DMA8+bM5rORw9mzexdffjOZMmXK\nJdQf+99RTJ3yA0HBQfTo2ZsHHnzYV6F75Hrvq+GffMziRQsIkiBy5MrF2wPfI2/eiGSO5DvWokll\nIhIHbML17T5xQAdVXXaDxxoALFHV+akYotc0bNSEVk8+Te83Xk8oG/zhxwnrQz4YRJYsWXwRWrJ2\n7IvivpaDANebYtecgUxb9DvPPH4/w75axMdfLriq/nNNHgSgUvN3yZMjCz8Nf4WHnh6MqvL6C7U4\neuIsdzYagIiQM3t4ml+PJ77+agJFixUnJjo6oaxLtx7UqJnkJIN+p0HDxrRo9RRv9f5nSpLikZF8\nOPQT3hnQ96q6u3btZM6smXz/03SORkXx0v89y0/TZxMcHJzWYXvseu+rts+9QIdOnQGY+NUERo38\nlLf6DvBViMkKlBaNP00TcF5Vy6vqXcAbwHs3eiBV7ZNekgxAhYqVyJY9+3W3qSpz58yiTr36aRxV\nyj1yb0n27D/KX4cSnTqcUsXysXj1dgCOnozm9NnzVCh9KwBtGt7P4HFzAdd1Hz8V4/2gU+jI4cMs\nXbKYJk2b+TqUm1ahYiWyX/NzV6xYcYoULfavuosXLaBWnbqEhYVRsFAhCt96K5s3bUyrUG/I9d5X\n7h/YLpw/7/c321N5hk2f8adE4y4bkPDbSkReE5HVIrJRRPo7ZUVE5A8RGSMiW0RkrohkcrZ9ISLN\nnPW6IrJNRNaKyCciMt0p7yci40RksYjsFpFOPrjOZK1bu4ZcuXJx221FfB1Ksp6oVYHJs9cmPH6p\nZRVWffsGn/V9iluyZgJg058HqF+1HMHBQdxWIBd3ly5MoXw5yJ7Ftb1v+/os+/p1Jn7wHHlzZvXJ\ndSRl8Pvv0rnra4hc/dYZ/slQnmjcgMHvv8ulS5d8FJ33HD1yhHwR+RMe543IR1TUER9GdOOG/Wco\nNR+tyozpP/NKh1d9HU6SRDxf/Jk/JZpMIrJBRLYB/wXeBhCRmkAkcC9QHqggIlWcfSKBT1W1DHAK\naOp+QBHJCIwC6qhqBSDPNecsBdRyjt1XREK9cmU3YdbM6dSu6/+tmdCQYOpVLceUeesBGPPdUko3\n6EflloM4fOwMg7o2AWD81OUcOHKK3yb2YPBrTVnx+x7i4uIJCQmiUL4crPh9Nw88+T4rN+7lvS6N\nfXlJ/7Jk8SJy5MxJ6TJlryrv1LkrP/08m4nf/sDp06f5fGxSkxwaX+v4ahfmLviFevUbMOnrr3wd\nTpJSc+IzX/Kn6K50nZUCagMTxNUerOks64F1uJJDpLPPHmd+BYC1QJFrjlkK2K2qe5zH31yzfYaq\nXlTVY7imL/3XXUH3ebjHjknbXyCxsbEsmD+P2rXrpul5b0Sth0qzYdvfRJ04C0DUibPExyuqyrgp\nv1Gx7G0AxMXF0+PDKdzXchDNu4zmlqyZ2PFXFMdPxRBz/iI/LfgdgCnz1lH+jsKJns8XNqxfxy+L\nF1KnZnV6vtaV1atW0Ov17uTJkxcRISwsjIaNmrB50yZfh5rq8kREcPjIoYTHUUcO+/VNdE/UrdeA\n+fPm+jqMJFmLxotUdTmQG1cLRID3nCRUXlVLqOpYp+pFt93iSPnghmT3V9XRqlpRVSs+/3/tUnj4\nm7Ny+TKKFi1GRL58aXreG9G8dsWrus3y5c6WsN6w+l1s3eX6JZUpYyjhGcMAqF65FLFx8WzbfRiA\nmUs2U6Wi6zNEtXtLsm33P7/Y/EGnLt2Yu2AJs+YuZNDgj6h07328+/4Qjh51TbGuqixaOJ8SkZHJ\nHCn9qVatOnNmzeTSpUsc2L+fv/bto2y5O30dVort27c3YX3RogUUvc79KH8SKPdo/GbUmTsRKQUE\nA8dxzUX9tohMVNVoESkIXPbwUNuBYiJSRFX3Ai28EvBNer17V9asXsWpUyepUb0KL7fvSJOmTzB7\n1kxq163n6/CSFZ4xjOqVS9HhnX8ajANfbcSdJQuhquw7dIKOzrY8ObLy84j2xMcrB4+e4vk3xyfs\n8+Z/fmLsO20Y3L0px05G82I//+7WuKLX6905efIkqkrJkqV4s29/X4fkkZ49urJ29WpOnTpJrUer\n8lL7jmTPnp33332HkydP0OmVlyhZqhQjRo2leIlIataqQ9OG9QgOCaZn7z5+PeIMrv+++nXJEvbu\n3UNQkJA/f0G/f638PH94TFyzdfqe2/BmcLVieqnqDGfbq8ALzrZo4GlcLZDpqlrWqdMdyKKq/UTk\nC2fb9yLSABgMxOCa4zqrqj4lIv2AaFUd4uy/GajvJKTruhCLfzxZqSxHpQ6+DiHVnVg13NcheIW/\nvF9TU6D8rci1Mobc/ETMFd5e5PELvvatR/z2ifSbFo2qJvrxSFX/A/znOpvKutUZ4rbe1q3OIlUt\n5dzv+RRY49Tpd805rr7Da4wxPhYoLRq/STRe9H8i0gYIwzWgYJSP4zHGGI8ESmsv4BONqg4Fhvo6\nDmOMSSl/v8nvqYBPNMYYk14FSJ6xRGOMMf7KWjTGGGO8KkDyjCUaY4zxVzYYwBhjjFdZ15kxxhiv\nskRjjDHGqwIkz1iiMcYYf2UtGmOMMV4VIHnGEo0xxvgrG3VmjDHGq4ICpEljicYYY/xUgOQZ/5xh\n0xhjTOrPsCkiwSKyXkSmO4+/EJE9IrLBWco75SIin4jIThHZKCL3uB2jjYjscJY2npzXWjTGGOOn\nvHCL5lXgDyCbW9lrqvr9NfXqAJHOUhkYCVQWkZxAX6AioMBaEZmmqieTOqklmhQIwMkNATi6Ypiv\nQ0h1BZ/72tcheMW+MS19HUKqC5Qb3t6Qms+NiBQC6gEDga7JVG8ITFDXlK4rROQWEckPVAPmqeoJ\n55jzgNrAN4keCes6M8YYvyUp+OeBj4EeQPw15QOd7rGhIpLBKSsI/O1WZ79Tllh5kizRGGOMnwoS\nzxcRaScia9yWdleOIyL1gShVXXvNKd4ASgGVgJzA6964Dus6M8YYP5WSbwZQ1dHA6EQ2Pwg8LiJ1\ngYxANhH5SlWfdrZfFJHPge7O4wNAYbf9CzllB3B1n7mXL04uNmvRGGOMnxLxfEmKqr6hqoVUtQjQ\nElioqk87910QV0ZrBGx2dpkGtHZGn90HnFbVQ8AcoKaI5BCRHEBNpyxJ1qIxxhg/lQZ/sDlRRPIA\nAmwAXnLKZwJ1gZ3AOeBZAFU9ISJvA6udegOuDAxIiiUaY4zxU94Ykaeqi3G6u1S1eiJ1FGifyLZx\nwLiUnNMSjTHG+KlA+WYASzTGGOOn7LvOjDHGeFVgpBlLNMYY47ds4jNjjDFeFSjfzmOJxhhj/FSg\nfA+cJRpjjPFT1nVmjDHGqwKkQWOJxhhj/JW1aIwxxnhVYKQZSzR+4eLFizzX5ikuX7pEbFwcj9Wo\nxSsdOjHp66+Y+OV4/v77LxYtXU6OHDl9HWqS+vfpxdJfFpMzZy4m//gzAKNGDOPHKd8lxN6+Uxce\nerhqwj6HDh3kiUb1afdye1q3fd4ncV9PtvBQPnm+MqUKZQeFjv9dSf2Khah1d0Eux8azJyqaDmNW\ncObcZUKDgxj6XCXKF81FvCpvfLmW37ZFAdC48q10fbwMwUHCnA0H6f/tBh9fmcvhw4fo27snJ04c\nR4DGzZrT6qnW/OejwSz5ZRGhoaEUKlSYvgPeJWs212SMn48dzdQffyAoKIjXXu/N/Q8+5NuLSIG9\ne3bTo1uXhMf79//NKx068XTrtr4LygPBAdJ3lqaJRkQUmHjlq6lFJAQ4BKxU1fpJ7FcN6J5UnfQs\nLCyMMePGEx6emcuXL/Ns6yd56OEqlL/7Hh6uWo0Xnm3t6xA90uDxxjRv+RR9e/e8qvzJp9skmkSG\nDh7EAw89nBbhpch7T1dgwcZDtB32K6HBQWTKEEyWzSEMmPw7cfFK3xbl6dKgDP2/3UDrR4oD8FCv\nmeTOloHJ3R/h0b6zuSVzGP1b3s0jfWZz/OxFPm13H1VKR7Bk6xEfXx2EBAfTpXsPSt1RhpiYGJ5p\n2ZTK9z1A5fseoH2nLoSEhPDJ0CF8PnY0nbp0Z/euncydPZPJU37maFQUr7z4HFOmzSI4ONjXl+KR\nIkWLMXnKVADi4uKo8UgVqj9Ww8dRJS9Qus7SepqAGKCsiGRyHtfANb9BmnISnN8QEcLDMwMQGxtL\nbGwsIkKpO0pTsGAhH0fnuXsqViJ79uwe11+0cD4FChaiePESXowq5bJmCuWBUnn58pddAFyOi+fM\nucss2nyYuHjXfN5rdh6jQM5wAEoWzJ6QPI6ducjpc5e4u2guiuTJwq4jZzl+9iIAv2w5TINKha9z\nxrSXO09eSt1RBoDMmTNTpFhxoqKOcN8DDxIS4np7lLvzLqKiXNf1y+KF1Kxdl7CwMAoWKkThwrey\nZfNGn8V/M1auWE7hwoUpUCDZiSF9LrWmCfA1X8xHMxPXvNUArXCba1pE7hWR5SKyXkSWiUjJa3cW\nkU3O/NUiIsdFpLVTPkFEaohIERFZKiLrnOUBZ3s1p3wasNUpe1pEVonIBhEZJSI++3gWFxdH86YN\nqV7lAe67/wHK3XmXr0JJdZMnTaRF08fp36cXZ86cBuDcuRjGjxtDu5ev+wWxPnVbnswcO3OR4e3u\nY/HbtfnP8/cSnuHqH42nqhZn/u8HAdjy10nq3FOI4CDh1jyZKV8kJwVzhrP7yFki82ejcO7MBAcJ\n9SoUomCuzL64pCQdPHCA7dv+oGy5q3/mpv00hQcedLU2o44cISIiX8K2vBERREVFpWmcqWX2rBnU\nrps+OkeCRDxe/JkvEs0koKWIZATuBFa6bdsGPKyqdwN9gHevs/9vuGaLKwPsBq70u9wPLAOigBqq\neg/QAvjEbd97gFdV9XYRucPZ/qCqlgfigKdS5xJTLjg4mMk/TGXOgl/YvGkjO3f86atQUlWzFq2Y\nOmMe33z3E7lz52HokPcBGDViOE8+0zahJedPQoKDuKtIDj5fsINqb83m3MU4Otcvk7C96+NliI2L\n57tlewH46pfdHDxxjoUDavPuUxVYtfMYcaqcPneZbl+sZlyHB5n5Zg3+OhpDXPy107X71rlzMfTo\n1olur/UkS5YsCeVjx3xGcHAwdeo18GF0qe/ypUv8smghNWvV9nUoHgmUFk2adyGp6kYRKYKrNTPz\nms3ZgfEiEgkoEHqdQywFqgD7gJFAOxEpCJxU1RgRyQ4MF5EryeN2t31XqeoeZ/1RoAKw2ukHzYQr\nSV3FmXe7HcCwEaN4/oV211ZJVdmyZaPSvZX57dellIi8Pfkd/FyuXLkT1hs3fYLOHV4GYPOmjSyY\nP4dPhg7m7NmzBEkQGTJkoEWrpxM7VJo5eOIcB0+cY+2u4wBMXfUXnRuUBqDVw0WpVb4gjQYtSKgf\nF6/0nrgu4fHsPjXYdegMAHPWH2DOelfvcJtHihOnmlaXkazYy5fp0fVVatdtQPXHaiaU/zz1R35d\nspiRoz9PuEeQNyKCI0cOJ9SJOnKEvHnzpnnMN+vXX5dQqnQZcuXOnXxlPxAo92h8da9iGjAE19zT\nudzK3wYWqWpjJxktvs6+S3B6+tkPAAAgAElEQVRNyHMr0BtoDDTDlYAAugBHgLtwtdguuO0b47Yu\nwHhVfSOpQN3n4T5/Ga/8ljhx4gQhISFky5aNCxcusGL5Mp597v+8cao0d/RoFHnyuH4hLVo4n+KR\nkQCMHT8xoc6oEcPIFB7uF0kGIOr0BQ6cOEeJfFnZefgsVcvkY/uB0zxaLj+d6pWm/sD5nL8Ul1A/\nU1gwInDuYhzVyuYjNk7ZftCVaHJny8CxMxfJHh7Kc4/eznPDf/XVZV1FVRnQ702KFit21cirZb8t\nZcIXYxk9dgIZM2VKKK9S9RHefOM1nnqmLUejovj7r32UKXunDyK/ObNmzqBO3XrJV/QTwZZobso4\n4JSqbnJGlF2RnX8GB7S93o6q+reI5AbCVHW3iPwKdAc6uB1jv6rGi0gbILH7LguAqSIyVFWjRCQn\nkFVV993Uld2AY0ejeKt3T+Lj4ohXpWat2lSp9ghffzWBLz7/L8ePHaN5k8d56OGq9B0wMK3D81iv\nHl1Zs2Y1p06dpM5jVXnxlY6sXbOK7dv+QEQoUKAgvfr093WYHnl9whpGvfwAYSFB7D0aTYfRK1gw\noDYZQoKY8rprUsI1O4/R7YvV5M6Wke97PILGKwdPnuelz5YlHOe9pytQ9tYcAAz+aTO7Dp/1yfVc\n6/f165g5fRolIm/nyeaNAXilY2eGvP8uly9dov1LrlGCZcvdRa+3+lG8RCSP1azNE43rExwcTI9e\nb6WbEWdXnDt3jhXLlvFW3wG+DsVjATK6GdE0bMqLSLSqZrmmrBrO0GURuR8Yj6vlMQN4WlWLXDu8\nWUS+BIJV9UnnZv+vQB5VPe50u/2Aq+ttNtBeVbNcb4i0iLQA3sDV8rns1F2RWPzeatH42pWRVIHk\n1he+Sb5SOrRvTEtfh5DqQkN8cavY+zKG3PzfW3adts3jN+dHj5fy27SUpokmvbNEk35Yokk/LNEk\nrtvP2z1+c37YoKTfJhq/+nsSY4wx/wiUrjNLNMYY46cCZCyAJRpjjPFXIQGSaSzRGGOMnwqQPGOJ\nxhhj/JW/f7WMpyzRGGOMnwqQPGOJxhhj/JWNOjPGGONVNvGZMcYYrwqQPGOJxhhj/JXc/JcL+AVL\nNMYY46esRWOMMcarLNEYY4zxKpv4zBhjjFcFB8gXWwfIZRhjTOAJEvF4SY6IZBSRVSLyu4hsEZH+\nTnlREVkpIjtF5FsRCXPKMziPdzrbi7gd6w2nfLuI1Er2Om74GTDGGONVQeL54oGLQHVVvQsoD9QW\nkfuA94GhqloCOAk879R/HjjplA916iEipYGWQBmgNjBCRJKcbtW6zlLAJolLPw6Me9LXIXhFzkf7\n+TqEVHdiQT9fh+C3UvMWjbp+gUU7D0OdRYHqwJU3zHigHzASaOisA3wPDBfXTaOGwCRVvQjsEZGd\nwL3A8sTObS0aY4zxU0GIx4snRCRYRDYAUcA8YBdwSlVjnSr7gYLOekHgbwBn+2kgl3v5dfZJ5DqM\nMcb4JZGULNJORNa4Le2uPZ6qxqlqeaAQrlZIqbS4Dus6M8YYPxWSgj+kUdXRwGgP654SkUXA/cAt\nIhLitFoKAQecageAwsB+EQkBsgPH3cqvcN/nuqxFY4wxfiolLZrkjyV5ROQWZz0TUAP4A1gENHOq\ntQGmOuvTnMc42xc693mmAS2dUWlFgUhgVVLnthaNMcb4qVSe+Cw/MN4ZIRYETFbV6SKyFZgkIu8A\n64GxTv2xwJfOzf4TuEaaoapbRGQysBWIBdqralxSJ7ZEY4wxfiqVR51tBO6+TvluXPdrri2/ADyR\nyLEGAgM9PbclGmOM8VOBcm/DEo0xxvipVO468xlLNMYY46cs0RhjjPGqwEgzlmiMMcZvBUiDxhKN\nMcb4K5uPxhhjjFfZqDNjjDFeZYMBjDHGeJV1nRljjPEq6zozxhjjVdaiMTel31u9WLJkMTlz5uL7\nH38GYN6c2Xw2cjh7du/iy28mU6ZMOQBmTv+Z8V+MTdh3x5/b+WbyFEqWusMnsSemf59eLP3FdU2T\nnWsaNWIYP075jhw5cgLQvlMXHnq4KiuW/8awjz/k8uXLhIaG8mrXHtxb+T5fhu+xOjWrkzlzZoKC\ngggJDubryVPo0a0ze/fuAeDs2bNkzZqVyT9MTeZIaSuycC6+7PfPV1cVLZCDt8ctokDurNR9oCSX\nYuPYc+AE7QZN5XT0BQC6P/UQbevdQ1x8PN3+M4v5q3eRISyE+cOeJSw0mJDgIH5cvJV3Pl/so6tK\nXlxcHE+2aErevBEMGzGKlSuWM/TDD4iPjyc8PJwBAwdx6623+TrM6wqMNAOSltMTi0gcsAlXgvsD\n11dQ5wWmq2rZ69QfACxR1fmJHO8LZ9/vrykvAHyiqs2ut9+NOncp9Z6stWtWEx4ezlu9eyYkmt27\ndxEkwjsD+tKle4+ERONux5/b6fpqB36eNS+1QiE+la5q3ZrVZAoPp2/vnlclmkzh4bRu+/xVdbf9\nsZVcuXKRJ28EO3f8SYeXX2D2/CWpEwgQnIJ5PFKqTs3qfP3t9wnJ81ofDh5ElixZePHlDql+7tSa\nyjkoSNj1QzeqvjSGyFtzs3jdHuLi4nnnpccAePOz+ZS6LQ/j+zbl4RfHkD93VmZ+1JpyTw0jPl7J\nnCmMmPOXCAkOYuGnz9H9k9ms2rr/hmLx9lTOX47/nC1bNhMTHc2wEaN4vF4tPv5kBMWKF+fbSRPZ\nvGkTbw8clOrnzRR683ni501HPH53NigX4bd5Ka27AM+ranknqVwCXkqqsqr2SSzJJLPfwdROMqmt\nQsVKZM+e/aqyYsWKU6RosST3mz1rBrXq1PVmaDfsnutcU2JK3VGaPHkjACheIpKLFy5y6dIlb4aX\nJlSVubNnUbtufV+HkqRHKhRjz8ET/HXkNAtW7yIuLh6AVVv2UzBPNgDqP1SS7xZs5tLlOPYdOsWu\nAyeodIdrxt6Y867XKjQkmJCQYNLyA2tKHDl8mKVLFtOk6T+/DkQgJiYagOiz0eTJk9dX4SUrNeej\n8SVf3mtaCpRw1oNFZIyIbBGRuc6kPIjIFyLSzFkfJCJbRWSjiAxxO04VEVkmIrvd6hYRkc3OelsR\nmSIis0Vkh4h8cGVHEXleRP4UkVXO+YenyZXfhLmzZ1G7Tj1fh5EikydNpEXTx+nfpxdnzpz+1/YF\n8+ZQ6o7ShIWF+SC6lBOBl9s9T6vmTfj+u2+v2rZu7Rpy5crFbbcV8U1wHnqielkmL9j8r/LWde9m\nzoqdABTMk439UWcSth04eoYCuV1JKChIWDH2Jf6a+hoL1+xi9R9JTrDoM4Pff5fOXV9D5J9fdX37\nD6TDy+2o+WgVZvw8lede+NeMx35DUvDPn/kk0TjTgtbB1Y0GrhnaPlXVMsApoOk19XMBjYEyqnon\n8I7b5vzAQ0B9ILH2b3mgBVAOaCEihZ3utbeA+4AHSaO5s2/Gpo2/kzFjRkpE3u7rUDzWrEUrps6Y\nxzff/UTu3HkYOuT9q7bv2rmDTz7+kF59+vsowpT7fMI3TPruRz4dOYbJ30xk7ZrVCdtmz5zu962Z\n0JBg6j1YkimLtlxV3uOZh4mLi2fSvI3JHiM+Xrnv+c8o0ewjKpYqSOmi/tcqWLJ4ETly5qR0mat7\n5b+a8AXDR45m7oIlPN6oCR9+8J6PIkyetWhuTCYR2QCsAf7in5nc9qjqBmd9LVDkmv1OAxeAsSLS\nBDjntu0nVY1X1a1ARCLnXaCqp52JfLYCt+Ga6OcXVT2hqpeB7663o4i0E5E1IrJm3H89mo7ba+bM\nmkntuumrNZMrV26Cg4MJCgqicdMn2LJpU8K2I4cP071LBwYMfJ/ChW/1YZQpExHh+jHLmSsXjzxa\ng82bXL+YY2NjWTB/HrVq+2fX5hW17ivBhh2HiDoZk1D2dO3y1L3/dtq+PSWh7MDRMxTKmy3hccE8\n2Th47MxVxzodfYFf1u+lZuUS+JsN69fxy+KF1KlZnZ6vdWX1qhV0eLkdf27fRrk77wKgVp26/L5h\nvY8jTVwQ4vHiz3x1j6a8qnZU1Sud8hfd6sRxzWg4VY3FlRi+x9Vyme222X3fxJ7tJI+fFFUdraoV\nVbWiL5vY8fHxzJ07i1q101eiOXo0KmF90cL5FI+MBODsmTO82uFFOr7ajfJ33+Or8FLs/LlzCf37\n58+dY/my3yjhXNPKFcsoWqwYEfny+TLEZDV/tByT5/+T8GvcW4KuTz5Isze+4fzFywnlM37bzhOP\nliUsNJjb8t9CiUK5WP3HAXJnDyd7lowAZAwL4dGKxdi+71iaX0dyOnXpxtwFS5g1dyGDBn9EpXvv\n4+NhI4iOPss+Z4TgimW/UbRYcR9HmrhAadGki+HNIpIFCFfVmSLyG7A7FQ67GvhYRHIAZ3F1121K\nepfU07NHV9auXs2pUyep9WhVXmrfkezZs/P+u+9w8uQJOr3yEiVLlWLEKFejb93a1eTLl59ChQun\nVYgp1qtHV9ascV1Tnceq8uIrHVm7ZhXbt/2BiFCgQMGELrJvJ03k77/+YsyoEYwZNQKATz8bS85c\nuXx5Cck6fvw4XV9tD0BsXBx16tbnwYeqADB71ky/v38WnjGU6hWL0WHIzwllQzvXJUNYMNM/ag3A\nqq376fThdP7Ye5QfFm1h/YT2xMbF03noDOLjlXy5sjKmVyOCg4MIEuGHRVuYtfxPX11SioSEhNCn\n3zt069KJIBGyZstO/7ff9XVYiQqUr6BJ6+HN0aqa5ZqyIrgNbxaR7kAWVe13Zfgy8BswFciIq9Uy\nRFXHXzu8+crx3Y8pIm2Biqrawakz3dl/sYi0A14DTgDbgP2q2jux+FNzeLM/Sa3hzf7Em8ObfSm1\nhjf7E28Pb/aV1BjevGDbMY/fnY+Wyu23P/Rpmmj8jYhkUdVoZ3DCj8A4Vf0xsfqWaNIPSzTphyWa\nxC3cdtzjd2f1Urn89oc+UL5K50b1cwYnbAb2AD/5OB5jjElg92gCgKp293UMxhiTGH//+xhP/U8n\nGmOM8WeB0gNsicYYY/xUoIw6s0RjjDF+KjDSjCUaY4zxW9aiMcYY41WBkWYs0RhjjP8KkExjicYY\nY/yUdZ0ZY4zxqsBIM5ZojDHGfwVIprFEY4wxfsq+GcAYY4xXBcgtmv/5L9U0xhi/JSlYkj2WyDgR\niRKRzW5l/UTkgIhscJa6btveEJGdIrJdRGq5ldd2ynaKSE9PrsMSjTHG+CkR8XjxwBdA7euUD3Wb\n+Ximc97SQEugjLPPCBEJFpFg4FOgDlAaaOXUTZJ1nRljjJ9Kza4zVV3iTArpiYbAJFW9COwRkZ3A\nvc62naq62xWfTHLqbk3qYJZoUsDDTw3pToi1a9ONo/P6+jqEVJf36fG+DsErzn7b5qaPkUa/cTqI\nSGtgDdBNVU8CBYEVbnX2O2UAf19TXjm5E9ivGGOM8VcpuEkjIu1EZI3b0s6DM4wEigPlgUPAh164\nCmvRGGOMv0rJ8GZVHQ2MTsnxVfVIwrlExgDTnYcHgMJuVQs5ZSRRnihr0RhjjJ/y9lTOIpLf7WFj\nXNPaA0wDWopIBhEpCkQCq4DVQKSIFBWRMFwDBqYldx5r0RhjjJ9KzdvCIvINUA3ILSL7gb5ANREp\nDyiwF3gRQFW3iMhkXDf5Y4H2qhrnHKcDMAcIBsap6pbkzm2Jxhhj/FRqfjOAqra6TvHYJOoPBAZe\np3wmMDMl57ZEY4wxfipQBrpaojHGGD8VIHnGEo0xxvitAMk0lmiMMcZP2cRnxhhjvCow0owlGmOM\n8V8Bkmks0RhjjJ+yic+MMcZ4VYDcorFEY4wx/ipA8owlGmOM8VeBMjWJJRpjjPFTAZJn7Nub/UVc\nXBwtmjWi4ysvArByxXJaPtGY5k0b0vaZVvz11z4fR3jjLl68yJMtmvFE48dp/Hg9Rgz/xNchpZrf\nli7h8Xq1qF+7BmPHpOgb2n2uf59ePFb1AZo3bpBQNmrEMGo/VoVWTzSi1RON+HXpL1ftc+jQQR6q\nfA8Tvkj0K7J8Int4KF92qcrajxqx5qOG3BuZJ2Fbx/qlOfttG3JlzQDALZnD+LrbIyz/oAGLBtbj\njsK3AJAhNIhFA+ux7IMGrBrSkF5P3OWTa3GXgulo/Fq6adGISG/gSSAOiAdeVNWVvo0q9Xz91QSK\nFitOTHQ0AAPf7sfHn4ygWPHifDtpImNGjeTtgYN8HOWNCQsL47/jxhOeOTOXL1+m7TNP8tDDVbjz\nrvK+Du2mxMXF8e7AAYwa8zkRERE82aIZ1R6pTvESJXwdmkcaPN6Y5i2fom/vnleVP/l0G1q3ff66\n+wwdPIgHHno4LcJLkQ/a3sv83w/yzNBfCA0OIjxDMAAFc4VT/c4C/HU0OqFu90bl2LjvBE9+uIjb\nC2Tjw+fuo8E7c7l4OZ76A+YQczGWkGBhbv86zNtwgNU7jvnqsvw/g3goXbRoROR+oD5wj6reCTzG\n1dOJpmtHDh9m6ZLFNGnaLKFMBGJiXG+O6LPR5MmT11fh3TQRITxzZgBiY2OJjY0NiD6BzZs2Urjw\nbRQqXJjQsDBq163H4kULfB2Wx+6pWIns2bN7XH/RwvkUKFiI4sX9K5FmyxTKA3dEMH7hDgAux8Vz\n+txlAAa1rsRbE9ei+k/9UoVuYcnmQwD8efAMt+bJQp7sGQGIuRgLQGhwEKEhQVft5wuSgn/+LF0k\nGiA/cExVLwKo6jFVPSgie0XkAxHZJCKrRKQEgIg0EJGVIrJeROaLSIRT3k9ExovIUhHZJyJN3Paf\nLSKhvri4we+/S+euryHyz8vRt/9AOrzcjpqPVmHGz1N57gVPZmX1X3FxcTRv0pBHHn6A++5/gDvv\n9H23xM2KOnKEfPnzJTzOGxHBkSNHktgjfZg8aSItmj5O/z69OHPmNADnzsUwftwY2r3c3sfR/dtt\nebNw7MxFPnv5QX4dVJ/hL95PeIYQ6lUszMET59i87+RV9TftO0GDe28DoELx3NyaJzMFc4YDrq98\n+e39Buwe04JFGw+yZqcPWzN4f+KztJJeEs1coLCI/CkiI0Skqtu206paDhgOfOyU/Qrcp6p3A5OA\nHm71iwPVgceBr4BFzv7ngXpevo5/WbJ4ETly5qR0mbJXlX814QuGjxzN3AVLeLxREz784L20Di1V\nBQcHM3nKVOYu/IXNmzayY8efvg7JXEezFq2YOmMe33z3E7lz52HokPcBGDViOE8+05bw8Mw+jvDf\nQoKDKF80J/+dt52Hek4n5kIsvZrdRbdG5Rg4ecO/6n80dTO3ZA7jt/cb8GLtUvy+9wRx8a6mS7wq\nD77+M6Ve/o4KJXIn3L/xlSDxfPFn6eIejapGi0gF4GHgEeBbEbnSsfyN2/9DnfVCTp38QBiwx+1w\ns1T1sohswjVD3GynfBNQ5Npzi0g7oB3AsBGjeD6VWxYb1q/jl8UL+XXpEi5dvEhMTDQdXm7H3j27\nKed86q9Vpy7tX3whVc/rK9myZaPSvZVZ9utSIiNv93U4NyVvRASHDx1OeBx15AgRERE+jOjm5cqV\nO2G9cdMn6NzhZcDVTbhg/hw+GTqYs2fPEiRBZMiQgRatnvZVqAkOHI/hwPFzCa2PqSv38Uaz8hTJ\nm4VlHzwOuO7VLB1Un2q9ZhB1+gIvj/wtYf/Nw5qyNyr6qmOePneZJVsOU+Ougvzx96m0u5h/8fMM\n4qF0kWgAnGlEFwOLnSTR5som92rO/8OAj1R1mohUA/q51bnS/RYvIpdVE3ph47nO86Gqo4HRAOcv\nk+o9tp26dKNTl24ArF61kglfjGPoJ5/yWLUH2bd3D7cVKcqKZb9RtFjx1D51mjlx4gQhISFky5aN\nCxcusGL5Mp59/v98HdZNK1O2HH/9tZf9+/8mIm8Es2fO4L3BH/o6rJty9GhUwv3ARQvnUzwyEoCx\n4ycm1Bk1YhiZwsP9IskARJ2+wIHjMUTmz8aOQ2eoWjY/v+85ToN35ibU2TysKVV7Tef42YtkDw/l\n3MU4LsfF07Z6JL9tO8LZ85fJnTVDwv2djKHBVC9XgKHTNvvwyvy/S8xT6SLRiEhJIF5VdzhF5YF9\nQDmgBTDI+X+5sz07cMBZb0M6ExISQp9+79CtSyeCRMiaLTv9337X12HdsGNHo3izV0/i4+OIj1dq\n1qpN1WqP+DqsmxYSEsIbvfvwcrsXiI+Po1HjppQoEenrsDzWq0dX1qxZzalTJ6nzWFVefKUja9es\nYvu2PxARChQoSK8+/X0dpke6f76S/3Z8mLCQIPZGRV/VYrlWyYK3MOqVB1Hgj/2naP/ZMgAicoQz\n6pUHCQ4SgoKEKcv3Mnvd/rS5gEQESJ5B1NfDKjzgdJsNA24BYoGduLqz1gDfAnVwtVRaqepOEWmI\nqxvtJLAQqKSq1USkHxCtqkOc40arahZn/apt1+ONFo0/CJRPTf8LYuMC70cwf+sJvg7BK85+2+am\n31mHTl/y+AXPnz3Mb9/J6SLRJEZE9gIVVTVNhoZYojG+Zokm/UiNRHP4zGWPX/B82UL99p2cLrrO\njDHmf5HfZo4USteJRlWL+DoGY4zxlkDpbUjXicYYYwKZv//Fv6cs0RhjjL8KjDxjicYYY/xVgOQZ\nSzTGGOOvggLkJo0lGmOM8VMBkmfSzZdqGmOMSaesRWOMMX4qUFo0lmiMMcZP2fBmY4wxXmUtGmOM\nMV5licYYY4xXBUrXmY06M8YYPyXi+eLZ8aS2iGwXkZ1usxR7nSUaY4zxU5KCJdljiQQDn+Kav6s0\n0EpESnsh7H+xRGOMMf4qNTMN3AvsVNXdqnoJmAQ0TP2g/83u0RhjjJ9K5a+gKQj87fZ4P1A5NU+Q\nGEs0KZApNO3uzIlIO1UdnVbnSyuBeF1pek0haXdzOK2u6+y3bbx9iqukp5/BjCGe/84RkXa4pri/\nYrS/XKd1nfmvdslXSZcC8boC8ZrAritdUdXRqlrRbbk2yRwACrs9LuSUeZ0lGmOM+d+wGogUkaIi\nEga0BKalxYmt68wYY/4HqGqsiHQA5gDBwDhV3ZIW57ZE47/8om/VCwLxugLxmsCuK+Co6kxgZlqf\nV1Q1rc9pjDHmf4jdozHGGONVlmg8ICJxIrJBRLaIyO8i0k1Ekn3uRGSws8/gtIjzZohIWxEpkEyd\nK8/D7yKyTkQeuInzDRCRx250/5shIioiX7k9DhGRoyIyPZn9qiVXx5fcXp/NIvKdiISLSBER2ZxI\n/SRfAxH5QkSaXae8gIh8n5qx3ygR6e28xzY6154mfxdiUsbu0XjmvKqWBxCRvMDXQDagbzL7tQNy\nqmqcl+NLDW2BzcDBJOq4Pw+1gPeAqjdyMlXtcyP7pZIYoKyIZFLV80AN0miYpzsRCVHV2FQ8pPvr\nMxF4CZiSWOUbfQ1U9SDwrwSU1kTkfqA+cI+qXhSR3ECYj8My12EtmhRS1ShcCaSDuAQ7LZfVzqeq\nFwFEZBqQBVgrIi1EpIGIrBSR9SIyX0QinHpZRORzEdnk7N/UKa8pIsudlsN3IpLFKd8rIu85n97W\niMg9IjJHRHaJyEtX4hSR19xi6u+UFRGRP0RkjPMpcK6IZHI+tVYEJjrHzeTBU5ENOHkj53O2JXxa\nFpG6IrJNRNaKyCdXWg0i0k9ExonIYhHZLSKdbua1u8ZMoJ6z3gr4xu1a7nWe+/UiskxESl67s/N6\n3eL8DBwXkdZO+QQRqeFc+1Ln9Uto/TmtoqXOz8dWp+xpEVnlPPejxPWdVDdrKVDCWQ/24DUYJCJb\nnddviNtxqjjPwW63ugmtJHG1hKeIyGwR2SEiH7g9R8+LyJ/OtY0RkeGpcF3u8gPHVPUigKoeU9WD\nznvkA+c1WiUiJZx4EnsP9hOR8c7rsk9EmrjtP1tEQlM57v89qmpLMgsQfZ2yU0AErqTzplOWAVgD\nFL12PyAH/wy+eAH40Fl/H/j4mnq5gSVAZqfsdaCPs74XeNlZHwpsBLICeYAjTnlNXCNrBNeHielA\nFaAIEAuUd+pNBp521hcDFZN5HuKADcA24DRQ4SbO9wWuT8UZcX0txpXn7BtgurPeD1jmPK+5geNA\naGq8nsCdwPfO+TcA1dzOmw0IcdYfA35w1t3rfIYrUZXF9fcJY5zyHUBmIBzI6JRFAmvcjhHjdr13\nAD9fuS5gBND6Zn5OcfVUTAVe9vA1yAVs55+fz1vctn/nvKalcX1PFs4xNzvrbYHdQHbnudyH648C\nC+D6Wc0JhOJKfMNT+X2ZxXnt/nSet6pu75Heznprt9cssfdgP+BXJ867gHNAHWfbj0CjtPx9E4iL\ndZ3dvJrAnfJPX3Z2XL9Y9lxTrxDwrYjkx9W8v7L9MVx/OAWAqp4Ukfq43ti/ieu7jsKA5W7HuvJH\nVpuALKp6FjgrIhdF5BYnpprAeqdeFiemv4A9qrrBKV+L65eGp9y7Zu4HJohI2Zs8Xylgt6peeT6+\n4eq/3J6hrk+sF0UkCldy35+CmK9LVTeKSBFcrZlrh3tmB8aLSCSguH4BXWsprmS6DxgJtBORgsBJ\nVY0RkezAcBEpjytB3+627yq3630UqACsdl7rTEDUDV5WJhG58lwvBcbi+oX//+2dS4gcRRjHf39Z\nUUgkrJKTYDTg6yYIgoj4OHswGAiCYhA8+DpEvIg5rCgKIgkYFQMaFA85BDUGxN2E6GqE+AAPri6I\nIoqK+IoEE6Ks2b+HrzrTO/TM7M5sZyN8v8t0V3U9uqurvkdV1wxqg6PA38DLxZqsz0PttT0PzFYW\nQAMHbR8FkDQLrCMUg/dtHynhe1j4DEbG9jFJVwPXAzcR/ava+n537Xd7Oe7VBwHesT0naYb4xmSy\nhM+wtD6SNJCCZggkrScGj18JLf5B21MDku0AttneJ+lGQovqWQRwwPbtPeL/Kb/ztePqfKykf8r2\nzq56X9x1/UliYFsytg8rfOJrWy6vO/1yvrP7gGcIK+OCWvjjwHu2N5R7mG5I+wFwP3AR8CiwgbAO\nDpX4LcAvhIZ8FjGQVzT+J/sAAAJvSURBVByvHQt41fYjI91JcEoROJV5CK++beD4kO8aQuhtBB4A\nbi7R9bS99t1qs4364pj/nAami5CoNk6rf7dRHffrg5X7bV7SnIs5Q6dPJSOQczRLRNJawm3yXHkZ\np4B7Kz+upMskrWpIuobOhHN9F8EDxIBV5T8OfARcV/Mtr5K0FG1wCrhbnXmdCxWLGPrxF+GCWxSS\nriA0vz+GLK/iK2B9GdABNi22DsvALuAx2zNd4fW22tyU0PYPhNZ+qe1vCdfLw4QAqvL4uVgDdxLP\nqomDwMbqeUk6X9K64W5nOEq7rXF8zLeFEI6j8ilwg6RxSWPAbcuQ5wIkXV6szoqrCAsTOu/RJjre\ngF59MGmZlNSLo3JJnE34u18DtpW4lwjT+jOF+vgbcGtDHhPAHkl/Au8Cl5TwJ4Dny+TqSWLge0PS\nZmC3pHPKdVsJX/RAbO+XdCVwuGi0x4A7Sv69eAV4UdIJ4FrHaqxu6q4ZAXcVjXKY8qq6npB0HzAp\n6TgxQJ0WbP8IPNsQ9TThOtsKvN0ni4/pCJBDxCq8D8v5C8DrikUCkyy0Yup1mC3l7FcsmZ8jFI/v\nm65vifOAtySdS7TrQ6NmaPsnSU8CnwBH6MzrLSergR3FXfwv8A3hdr0FGJf0OWGpVJ6BCZr7YNIy\nuTNAsuJIWl387SL+AfBr29sHpUvObGrtOkZMqu+y/eZpKPc7YmHL722XlSyOdJ0lZwL3FEvpS8K9\nsXPA9cn/g4nSrl8QE+97V7g+yQqRFk2SJEnSKmnRJEmSJK2SgiZJkiRplRQ0SZIkSaukoEmSJEla\nJQVNkiRJ0iopaJIkSZJW+Q/lm5QQq0WrlQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeVn3oV7plyQ",
        "colab_type": "text"
      },
      "source": [
        "#### Graph of Categorical Cross-Entropy Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzH_f_T3plyU",
        "colab_type": "code",
        "outputId": "b0cedaa0-4883-4595-da9e-e02eb137786f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plot('Categorical Model Accuracy')"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtWNPIlMplyY",
        "colab_type": "text"
      },
      "source": [
        "### Write results to file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_YP-tODMplyZ",
        "colab": {}
      },
      "source": [
        "resultFile = os.path.join(resultPath, dataFile)\n",
        "with open('{}.result'.format(resultFile), 'a') as fout:\n",
        "  fout.write('{} results...'.format(model_name+model_extension))\n",
        "  fout.write('\\taccuracy: {:.2f} loss: {:.2f}\\n'.format(acc, loss))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOJNLEDfplyc",
        "colab_type": "text"
      },
      "source": [
        "## Binary Classification of Labels\n",
        "> Change all malicious labels to value 1 and benign label to 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH5SPeyD1zUX",
        "colab_type": "text"
      },
      "source": [
        "### Binarize Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzpGC1TYplyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lblTypes = list(lblTypes)\n",
        "lblTypes = dict(zip(lblTypes, [1, 1, 1, 1, 1]))\n",
        "lblTypes['benign'] = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQ5kYPnTplyg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "binary_df = df1.copy()\n",
        "binary_df[label] = binary_df[label].map(lblTypes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taziMECuplyj",
        "colab_type": "text"
      },
      "source": [
        "### Train the Binary Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llXRdiQ1plyk",
        "colab_type": "code",
        "outputId": "b8302b4a-c633-4a7d-edf0-e2609437a88f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model, history, X , encoded_y = experiment(binary_df)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running fold #1\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33026 samples, validate on 3671 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0716 22:57:18.012020 139875824584576 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "33026/33026 [==============================] - 3s 88us/sample - loss: 0.3518 - acc: 0.8629 - val_loss: 0.1559 - val_acc: 0.9262\n",
            "Epoch 2/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.1968 - acc: 0.9210 - val_loss: 0.1281 - val_acc: 0.9447\n",
            "Epoch 3/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.1650 - acc: 0.9320 - val_loss: 0.1119 - val_acc: 0.9567\n",
            "Epoch 4/100\n",
            "33026/33026 [==============================] - 2s 57us/sample - loss: 0.1471 - acc: 0.9408 - val_loss: 0.0986 - val_acc: 0.9624\n",
            "Epoch 5/100\n",
            "33026/33026 [==============================] - 2s 57us/sample - loss: 0.1313 - acc: 0.9472 - val_loss: 0.0931 - val_acc: 0.9619\n",
            "Epoch 6/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.1238 - acc: 0.9513 - val_loss: 0.0885 - val_acc: 0.9646\n",
            "Epoch 7/100\n",
            "33026/33026 [==============================] - 2s 57us/sample - loss: 0.1195 - acc: 0.9539 - val_loss: 0.0835 - val_acc: 0.9711\n",
            "Epoch 8/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.1143 - acc: 0.9560 - val_loss: 0.0818 - val_acc: 0.9698\n",
            "Epoch 9/100\n",
            "33026/33026 [==============================] - 2s 56us/sample - loss: 0.1084 - acc: 0.9585 - val_loss: 0.0764 - val_acc: 0.9744\n",
            "Epoch 10/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.1054 - acc: 0.9606 - val_loss: 0.0732 - val_acc: 0.9747\n",
            "Epoch 11/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0998 - acc: 0.9625 - val_loss: 0.0745 - val_acc: 0.9730\n",
            "Epoch 12/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.1005 - acc: 0.9625 - val_loss: 0.0701 - val_acc: 0.9738\n",
            "Epoch 13/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0956 - acc: 0.9635 - val_loss: 0.0682 - val_acc: 0.9774\n",
            "Epoch 14/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0931 - acc: 0.9662 - val_loss: 0.0674 - val_acc: 0.9752\n",
            "Epoch 15/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0886 - acc: 0.9665 - val_loss: 0.0647 - val_acc: 0.9785\n",
            "Epoch 16/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0878 - acc: 0.9674 - val_loss: 0.0639 - val_acc: 0.9785\n",
            "Epoch 17/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0893 - acc: 0.9673 - val_loss: 0.0628 - val_acc: 0.9774\n",
            "Epoch 18/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.0847 - acc: 0.9692 - val_loss: 0.0603 - val_acc: 0.9793\n",
            "Epoch 19/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.0812 - acc: 0.9697 - val_loss: 0.0584 - val_acc: 0.9812\n",
            "Epoch 20/100\n",
            "33026/33026 [==============================] - 2s 56us/sample - loss: 0.0826 - acc: 0.9706 - val_loss: 0.0611 - val_acc: 0.9798\n",
            "Epoch 21/100\n",
            "33026/33026 [==============================] - 2s 59us/sample - loss: 0.0820 - acc: 0.9712 - val_loss: 0.0636 - val_acc: 0.9801\n",
            "Epoch 22/100\n",
            "33026/33026 [==============================] - 2s 59us/sample - loss: 0.0798 - acc: 0.9708 - val_loss: 0.0576 - val_acc: 0.9796\n",
            "Epoch 23/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0782 - acc: 0.9715 - val_loss: 0.0564 - val_acc: 0.9815\n",
            "Epoch 24/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0774 - acc: 0.9718 - val_loss: 0.0587 - val_acc: 0.9798\n",
            "Epoch 25/100\n",
            "33026/33026 [==============================] - 2s 52us/sample - loss: 0.0789 - acc: 0.9722 - val_loss: 0.0568 - val_acc: 0.9831\n",
            "Epoch 26/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.0726 - acc: 0.9733 - val_loss: 0.0565 - val_acc: 0.9823\n",
            "Epoch 27/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.0724 - acc: 0.9740 - val_loss: 0.0561 - val_acc: 0.9826\n",
            "Epoch 28/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0735 - acc: 0.9737 - val_loss: 0.0560 - val_acc: 0.9828\n",
            "Epoch 29/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0744 - acc: 0.9728 - val_loss: 0.0565 - val_acc: 0.9817\n",
            "Epoch 30/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0693 - acc: 0.9751 - val_loss: 0.0554 - val_acc: 0.9815\n",
            "Epoch 31/100\n",
            "33026/33026 [==============================] - 2s 56us/sample - loss: 0.0722 - acc: 0.9744 - val_loss: 0.0551 - val_acc: 0.9823\n",
            "Epoch 32/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0722 - acc: 0.9740 - val_loss: 0.0542 - val_acc: 0.9837\n",
            "Epoch 33/100\n",
            "33026/33026 [==============================] - 2s 53us/sample - loss: 0.0698 - acc: 0.9749 - val_loss: 0.0548 - val_acc: 0.9839\n",
            "Epoch 34/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0722 - acc: 0.9749 - val_loss: 0.0538 - val_acc: 0.9823\n",
            "Epoch 35/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0674 - acc: 0.9764 - val_loss: 0.0516 - val_acc: 0.9828\n",
            "Epoch 36/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0685 - acc: 0.9751 - val_loss: 0.0513 - val_acc: 0.9834\n",
            "Epoch 37/100\n",
            "33026/33026 [==============================] - 2s 57us/sample - loss: 0.0659 - acc: 0.9769 - val_loss: 0.0515 - val_acc: 0.9839\n",
            "Epoch 38/100\n",
            "33026/33026 [==============================] - 2s 57us/sample - loss: 0.0660 - acc: 0.9762 - val_loss: 0.0563 - val_acc: 0.9809\n",
            "Epoch 39/100\n",
            "33026/33026 [==============================] - 2s 57us/sample - loss: 0.0703 - acc: 0.9757 - val_loss: 0.0507 - val_acc: 0.9826\n",
            "Epoch 40/100\n",
            "33026/33026 [==============================] - 2s 55us/sample - loss: 0.0684 - acc: 0.9757 - val_loss: 0.0501 - val_acc: 0.9837\n",
            "Epoch 41/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0641 - acc: 0.9773 - val_loss: 0.0509 - val_acc: 0.9820\n",
            "Epoch 42/100\n",
            "33026/33026 [==============================] - 2s 60us/sample - loss: 0.0649 - acc: 0.9775 - val_loss: 0.0527 - val_acc: 0.9845\n",
            "Epoch 43/100\n",
            "33026/33026 [==============================] - 2s 61us/sample - loss: 0.0643 - acc: 0.9776 - val_loss: 0.0507 - val_acc: 0.9853\n",
            "Epoch 44/100\n",
            "33026/33026 [==============================] - 2s 60us/sample - loss: 0.0625 - acc: 0.9783 - val_loss: 0.0512 - val_acc: 0.9839\n",
            "Epoch 45/100\n",
            "33026/33026 [==============================] - 2s 54us/sample - loss: 0.0653 - acc: 0.9775 - val_loss: 0.0507 - val_acc: 0.9842\n",
            "Running fold #2\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 87us/sample - loss: 0.3662 - acc: 0.8551 - val_loss: 0.1651 - val_acc: 0.9371\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.2001 - acc: 0.9175 - val_loss: 0.1389 - val_acc: 0.9466\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1651 - acc: 0.9330 - val_loss: 0.1229 - val_acc: 0.9534\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1458 - acc: 0.9414 - val_loss: 0.1088 - val_acc: 0.9572\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1328 - acc: 0.9471 - val_loss: 0.0999 - val_acc: 0.9589\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1257 - acc: 0.9514 - val_loss: 0.0907 - val_acc: 0.9689\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.1190 - acc: 0.9537 - val_loss: 0.0854 - val_acc: 0.9711\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1195 - acc: 0.9530 - val_loss: 0.0857 - val_acc: 0.9695\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1065 - acc: 0.9590 - val_loss: 0.0820 - val_acc: 0.9730\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1038 - acc: 0.9603 - val_loss: 0.0752 - val_acc: 0.9717\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0998 - acc: 0.9619 - val_loss: 0.0742 - val_acc: 0.9744\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1038 - acc: 0.9615 - val_loss: 0.0750 - val_acc: 0.9752\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.1007 - acc: 0.9620 - val_loss: 0.0692 - val_acc: 0.9747\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0967 - acc: 0.9637 - val_loss: 0.0686 - val_acc: 0.9757\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0942 - acc: 0.9644 - val_loss: 0.0680 - val_acc: 0.9779\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0915 - acc: 0.9682 - val_loss: 0.0642 - val_acc: 0.9779\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0866 - acc: 0.9689 - val_loss: 0.0664 - val_acc: 0.9768\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0861 - acc: 0.9691 - val_loss: 0.0658 - val_acc: 0.9774\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0840 - acc: 0.9694 - val_loss: 0.0648 - val_acc: 0.9782\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0826 - acc: 0.9704 - val_loss: 0.0633 - val_acc: 0.9787\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0842 - acc: 0.9696 - val_loss: 0.0618 - val_acc: 0.9798\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0817 - acc: 0.9697 - val_loss: 0.0593 - val_acc: 0.9809\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0825 - acc: 0.9707 - val_loss: 0.0555 - val_acc: 0.9823\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0822 - acc: 0.9702 - val_loss: 0.0634 - val_acc: 0.9782\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0774 - acc: 0.9720 - val_loss: 0.0567 - val_acc: 0.9817\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0779 - acc: 0.9726 - val_loss: 0.0575 - val_acc: 0.9804\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0785 - acc: 0.9723 - val_loss: 0.0576 - val_acc: 0.9801\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0761 - acc: 0.9722 - val_loss: 0.0535 - val_acc: 0.9820\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0736 - acc: 0.9729 - val_loss: 0.0578 - val_acc: 0.9782\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.0725 - acc: 0.9744 - val_loss: 0.0534 - val_acc: 0.9817\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0731 - acc: 0.9739 - val_loss: 0.0534 - val_acc: 0.9834\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0700 - acc: 0.9748 - val_loss: 0.0518 - val_acc: 0.9823\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0739 - acc: 0.9734 - val_loss: 0.0545 - val_acc: 0.9807\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0711 - acc: 0.9752 - val_loss: 0.0527 - val_acc: 0.9812\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0697 - acc: 0.9741 - val_loss: 0.0488 - val_acc: 0.9823\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.0722 - acc: 0.9744 - val_loss: 0.0537 - val_acc: 0.9820\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.0664 - acc: 0.9761 - val_loss: 0.0507 - val_acc: 0.9842\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 67us/sample - loss: 0.0677 - acc: 0.9763 - val_loss: 0.0519 - val_acc: 0.9837\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 68us/sample - loss: 0.0695 - acc: 0.9751 - val_loss: 0.0525 - val_acc: 0.9839\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 68us/sample - loss: 0.0674 - acc: 0.9761 - val_loss: 0.0495 - val_acc: 0.9839\n",
            "Running fold #3\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 98us/sample - loss: 0.3654 - acc: 0.8574 - val_loss: 0.1530 - val_acc: 0.9360\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.2006 - acc: 0.9195 - val_loss: 0.1269 - val_acc: 0.9463\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.1619 - acc: 0.9346 - val_loss: 0.1104 - val_acc: 0.9556\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.1443 - acc: 0.9408 - val_loss: 0.0956 - val_acc: 0.9659\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.1315 - acc: 0.9481 - val_loss: 0.0906 - val_acc: 0.9678\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.1282 - acc: 0.9509 - val_loss: 0.0841 - val_acc: 0.9700\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.1227 - acc: 0.9527 - val_loss: 0.0792 - val_acc: 0.9730\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.1152 - acc: 0.9569 - val_loss: 0.0763 - val_acc: 0.9736\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.1093 - acc: 0.9597 - val_loss: 0.0698 - val_acc: 0.9766\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.1051 - acc: 0.9599 - val_loss: 0.0682 - val_acc: 0.9733\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.1036 - acc: 0.9613 - val_loss: 0.0660 - val_acc: 0.9757\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.1004 - acc: 0.9625 - val_loss: 0.0646 - val_acc: 0.9766\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0949 - acc: 0.9651 - val_loss: 0.0622 - val_acc: 0.9793\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0959 - acc: 0.9650 - val_loss: 0.0603 - val_acc: 0.9771\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0941 - acc: 0.9654 - val_loss: 0.0644 - val_acc: 0.9766\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.0905 - acc: 0.9676 - val_loss: 0.0613 - val_acc: 0.9774\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.0898 - acc: 0.9685 - val_loss: 0.0596 - val_acc: 0.9790\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0863 - acc: 0.9692 - val_loss: 0.0557 - val_acc: 0.9807\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0814 - acc: 0.9713 - val_loss: 0.0586 - val_acc: 0.9807\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0824 - acc: 0.9702 - val_loss: 0.0526 - val_acc: 0.9804\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.0826 - acc: 0.9702 - val_loss: 0.0548 - val_acc: 0.9804\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0821 - acc: 0.9711 - val_loss: 0.0497 - val_acc: 0.9826\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0784 - acc: 0.9726 - val_loss: 0.0499 - val_acc: 0.9839\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0771 - acc: 0.9731 - val_loss: 0.0524 - val_acc: 0.9796\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0777 - acc: 0.9723 - val_loss: 0.0520 - val_acc: 0.9804\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.0760 - acc: 0.9731 - val_loss: 0.0469 - val_acc: 0.9847\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.0732 - acc: 0.9740 - val_loss: 0.0533 - val_acc: 0.9815\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0742 - acc: 0.9744 - val_loss: 0.0476 - val_acc: 0.9839\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.0757 - acc: 0.9720 - val_loss: 0.0499 - val_acc: 0.9823\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.0770 - acc: 0.9733 - val_loss: 0.0480 - val_acc: 0.9834\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.0713 - acc: 0.9743 - val_loss: 0.0476 - val_acc: 0.9847\n",
            "Running fold #4\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 103us/sample - loss: 0.3821 - acc: 0.8481 - val_loss: 0.1697 - val_acc: 0.9278\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.2073 - acc: 0.9159 - val_loss: 0.1340 - val_acc: 0.9452\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.1738 - acc: 0.9289 - val_loss: 0.1192 - val_acc: 0.9545\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.1556 - acc: 0.9378 - val_loss: 0.1049 - val_acc: 0.9649\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.1421 - acc: 0.9426 - val_loss: 0.0943 - val_acc: 0.9678\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.1290 - acc: 0.9492 - val_loss: 0.0869 - val_acc: 0.9714\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 63us/sample - loss: 0.1226 - acc: 0.9521 - val_loss: 0.0850 - val_acc: 0.9722\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 65us/sample - loss: 0.1160 - acc: 0.9554 - val_loss: 0.0793 - val_acc: 0.9755\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.1133 - acc: 0.9562 - val_loss: 0.0720 - val_acc: 0.9790\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.1073 - acc: 0.9596 - val_loss: 0.0688 - val_acc: 0.9768\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.1070 - acc: 0.9595 - val_loss: 0.0691 - val_acc: 0.9774\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0998 - acc: 0.9618 - val_loss: 0.0628 - val_acc: 0.9798\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.1014 - acc: 0.9625 - val_loss: 0.0630 - val_acc: 0.9807\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0979 - acc: 0.9645 - val_loss: 0.0644 - val_acc: 0.9790\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0943 - acc: 0.9649 - val_loss: 0.0623 - val_acc: 0.9790\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0904 - acc: 0.9662 - val_loss: 0.0626 - val_acc: 0.9823\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0914 - acc: 0.9664 - val_loss: 0.0595 - val_acc: 0.9815\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0893 - acc: 0.9672 - val_loss: 0.0555 - val_acc: 0.9834\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0852 - acc: 0.9690 - val_loss: 0.0557 - val_acc: 0.9837\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0863 - acc: 0.9685 - val_loss: 0.0570 - val_acc: 0.9837\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0831 - acc: 0.9706 - val_loss: 0.0552 - val_acc: 0.9837\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0818 - acc: 0.9700 - val_loss: 0.0520 - val_acc: 0.9842\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0803 - acc: 0.9714 - val_loss: 0.0551 - val_acc: 0.9850\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0787 - acc: 0.9725 - val_loss: 0.0538 - val_acc: 0.9842\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0778 - acc: 0.9717 - val_loss: 0.0524 - val_acc: 0.9850\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0762 - acc: 0.9727 - val_loss: 0.0507 - val_acc: 0.9864\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0775 - acc: 0.9727 - val_loss: 0.0527 - val_acc: 0.9847\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0768 - acc: 0.9730 - val_loss: 0.0519 - val_acc: 0.9853\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0780 - acc: 0.9723 - val_loss: 0.0484 - val_acc: 0.9864\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0744 - acc: 0.9735 - val_loss: 0.0513 - val_acc: 0.9842\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0770 - acc: 0.9729 - val_loss: 0.0490 - val_acc: 0.9864\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0745 - acc: 0.9737 - val_loss: 0.0498 - val_acc: 0.9845\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0717 - acc: 0.9746 - val_loss: 0.0465 - val_acc: 0.9820\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0718 - acc: 0.9751 - val_loss: 0.0510 - val_acc: 0.9837\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0769 - acc: 0.9730 - val_loss: 0.0485 - val_acc: 0.9853\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0721 - acc: 0.9760 - val_loss: 0.0480 - val_acc: 0.9845\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0715 - acc: 0.9750 - val_loss: 0.0482 - val_acc: 0.9869\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0687 - acc: 0.9752 - val_loss: 0.0463 - val_acc: 0.9847\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0672 - acc: 0.9754 - val_loss: 0.0462 - val_acc: 0.9869\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0687 - acc: 0.9754 - val_loss: 0.0457 - val_acc: 0.9847\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0696 - acc: 0.9757 - val_loss: 0.0477 - val_acc: 0.9850\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0669 - acc: 0.9772 - val_loss: 0.0491 - val_acc: 0.9842\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0710 - acc: 0.9751 - val_loss: 0.0448 - val_acc: 0.9864\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0727 - acc: 0.9743 - val_loss: 0.0478 - val_acc: 0.9842\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0689 - acc: 0.9755 - val_loss: 0.0478 - val_acc: 0.9856\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0647 - acc: 0.9768 - val_loss: 0.0488 - val_acc: 0.9842\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0643 - acc: 0.9767 - val_loss: 0.0440 - val_acc: 0.9877\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0634 - acc: 0.9786 - val_loss: 0.0438 - val_acc: 0.9864\n",
            "Epoch 49/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0630 - acc: 0.9777 - val_loss: 0.0437 - val_acc: 0.9850\n",
            "Epoch 50/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0637 - acc: 0.9772 - val_loss: 0.0440 - val_acc: 0.9856\n",
            "Epoch 51/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0630 - acc: 0.9782 - val_loss: 0.0424 - val_acc: 0.9866\n",
            "Epoch 52/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0613 - acc: 0.9787 - val_loss: 0.0414 - val_acc: 0.9869\n",
            "Epoch 53/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0636 - acc: 0.9767 - val_loss: 0.0406 - val_acc: 0.9856\n",
            "Epoch 54/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0650 - acc: 0.9767 - val_loss: 0.0428 - val_acc: 0.9869\n",
            "Epoch 55/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0653 - acc: 0.9776 - val_loss: 0.0460 - val_acc: 0.9853\n",
            "Epoch 56/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0629 - acc: 0.9784 - val_loss: 0.0415 - val_acc: 0.9866\n",
            "Epoch 57/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0655 - acc: 0.9770 - val_loss: 0.0442 - val_acc: 0.9866\n",
            "Epoch 58/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0610 - acc: 0.9791 - val_loss: 0.0397 - val_acc: 0.9858\n",
            "Epoch 59/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0618 - acc: 0.9780 - val_loss: 0.0399 - val_acc: 0.9869\n",
            "Epoch 60/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0604 - acc: 0.9788 - val_loss: 0.0404 - val_acc: 0.9864\n",
            "Epoch 61/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0609 - acc: 0.9793 - val_loss: 0.0411 - val_acc: 0.9888\n",
            "Epoch 62/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0611 - acc: 0.9788 - val_loss: 0.0397 - val_acc: 0.9864\n",
            "Epoch 63/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0573 - acc: 0.9803 - val_loss: 0.0381 - val_acc: 0.9886\n",
            "Epoch 64/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0588 - acc: 0.9788 - val_loss: 0.0399 - val_acc: 0.9886\n",
            "Epoch 65/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0577 - acc: 0.9800 - val_loss: 0.0387 - val_acc: 0.9866\n",
            "Epoch 66/100\n",
            "33027/33027 [==============================] - 2s 61us/sample - loss: 0.0600 - acc: 0.9796 - val_loss: 0.0387 - val_acc: 0.9877\n",
            "Epoch 67/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.0599 - acc: 0.9791 - val_loss: 0.0399 - val_acc: 0.9875\n",
            "Epoch 68/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0590 - acc: 0.9792 - val_loss: 0.0405 - val_acc: 0.9875\n",
            "Running fold #5\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 96us/sample - loss: 0.3583 - acc: 0.8581 - val_loss: 0.1690 - val_acc: 0.9302\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.2041 - acc: 0.9167 - val_loss: 0.1396 - val_acc: 0.9390\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 66us/sample - loss: 0.1712 - acc: 0.9297 - val_loss: 0.1206 - val_acc: 0.9542\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 64us/sample - loss: 0.1497 - acc: 0.9384 - val_loss: 0.1061 - val_acc: 0.9556\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.1397 - acc: 0.9427 - val_loss: 0.0968 - val_acc: 0.9599\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1258 - acc: 0.9497 - val_loss: 0.0899 - val_acc: 0.9681\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1206 - acc: 0.9522 - val_loss: 0.0853 - val_acc: 0.9684\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1203 - acc: 0.9518 - val_loss: 0.0804 - val_acc: 0.9711\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.1114 - acc: 0.9565 - val_loss: 0.0786 - val_acc: 0.9719\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.1041 - acc: 0.9614 - val_loss: 0.0726 - val_acc: 0.9736\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.1019 - acc: 0.9617 - val_loss: 0.0667 - val_acc: 0.9766\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0982 - acc: 0.9642 - val_loss: 0.0660 - val_acc: 0.9768\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0978 - acc: 0.9637 - val_loss: 0.0672 - val_acc: 0.9766\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0932 - acc: 0.9652 - val_loss: 0.0649 - val_acc: 0.9809\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0919 - acc: 0.9658 - val_loss: 0.0629 - val_acc: 0.9807\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0884 - acc: 0.9670 - val_loss: 0.0627 - val_acc: 0.9804\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0873 - acc: 0.9690 - val_loss: 0.0618 - val_acc: 0.9817\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0881 - acc: 0.9690 - val_loss: 0.0621 - val_acc: 0.9790\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0895 - acc: 0.9674 - val_loss: 0.0598 - val_acc: 0.9820\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0870 - acc: 0.9691 - val_loss: 0.0596 - val_acc: 0.9820\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0844 - acc: 0.9695 - val_loss: 0.0647 - val_acc: 0.9793\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0813 - acc: 0.9704 - val_loss: 0.0555 - val_acc: 0.9831\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0771 - acc: 0.9720 - val_loss: 0.0536 - val_acc: 0.9826\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0791 - acc: 0.9712 - val_loss: 0.0534 - val_acc: 0.9837\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.0779 - acc: 0.9721 - val_loss: 0.0518 - val_acc: 0.9828\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0795 - acc: 0.9720 - val_loss: 0.0512 - val_acc: 0.9845\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0811 - acc: 0.9719 - val_loss: 0.0520 - val_acc: 0.9847\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 60us/sample - loss: 0.0755 - acc: 0.9726 - val_loss: 0.0523 - val_acc: 0.9845\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0791 - acc: 0.9714 - val_loss: 0.0514 - val_acc: 0.9845\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0758 - acc: 0.9734 - val_loss: 0.0516 - val_acc: 0.9847\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0766 - acc: 0.9734 - val_loss: 0.0512 - val_acc: 0.9853\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0751 - acc: 0.9740 - val_loss: 0.0510 - val_acc: 0.9837\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0761 - acc: 0.9715 - val_loss: 0.0528 - val_acc: 0.9839\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0693 - acc: 0.9762 - val_loss: 0.0480 - val_acc: 0.9850\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0706 - acc: 0.9764 - val_loss: 0.0494 - val_acc: 0.9866\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.0684 - acc: 0.9752 - val_loss: 0.0494 - val_acc: 0.9842\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0733 - acc: 0.9742 - val_loss: 0.0498 - val_acc: 0.9850\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.0698 - acc: 0.9746 - val_loss: 0.0499 - val_acc: 0.9847\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0705 - acc: 0.9754 - val_loss: 0.0484 - val_acc: 0.9856\n",
            "Running fold #6\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33027 samples, validate on 3670 samples\n",
            "Epoch 1/100\n",
            "33027/33027 [==============================] - 3s 99us/sample - loss: 0.3636 - acc: 0.8580 - val_loss: 0.1645 - val_acc: 0.9319\n",
            "Epoch 2/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.1977 - acc: 0.9187 - val_loss: 0.1346 - val_acc: 0.9431\n",
            "Epoch 3/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1682 - acc: 0.9317 - val_loss: 0.1114 - val_acc: 0.9561\n",
            "Epoch 4/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1486 - acc: 0.9391 - val_loss: 0.1020 - val_acc: 0.9594\n",
            "Epoch 5/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.1364 - acc: 0.9458 - val_loss: 0.0937 - val_acc: 0.9657\n",
            "Epoch 6/100\n",
            "33027/33027 [==============================] - 2s 62us/sample - loss: 0.1228 - acc: 0.9513 - val_loss: 0.0846 - val_acc: 0.9678\n",
            "Epoch 7/100\n",
            "33027/33027 [==============================] - 2s 57us/sample - loss: 0.1195 - acc: 0.9529 - val_loss: 0.0841 - val_acc: 0.9681\n",
            "Epoch 8/100\n",
            "33027/33027 [==============================] - 2s 66us/sample - loss: 0.1151 - acc: 0.9559 - val_loss: 0.0802 - val_acc: 0.9722\n",
            "Epoch 9/100\n",
            "33027/33027 [==============================] - 2s 59us/sample - loss: 0.1182 - acc: 0.9545 - val_loss: 0.0811 - val_acc: 0.9687\n",
            "Epoch 10/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.1071 - acc: 0.9598 - val_loss: 0.0711 - val_acc: 0.9730\n",
            "Epoch 11/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1035 - acc: 0.9604 - val_loss: 0.0684 - val_acc: 0.9744\n",
            "Epoch 12/100\n",
            "33027/33027 [==============================] - 2s 58us/sample - loss: 0.1002 - acc: 0.9628 - val_loss: 0.0662 - val_acc: 0.9755\n",
            "Epoch 13/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0983 - acc: 0.9640 - val_loss: 0.0662 - val_acc: 0.9757\n",
            "Epoch 14/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.0945 - acc: 0.9645 - val_loss: 0.0597 - val_acc: 0.9771\n",
            "Epoch 15/100\n",
            "33027/33027 [==============================] - 2s 52us/sample - loss: 0.0926 - acc: 0.9656 - val_loss: 0.0608 - val_acc: 0.9787\n",
            "Epoch 16/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0942 - acc: 0.9655 - val_loss: 0.0612 - val_acc: 0.9790\n",
            "Epoch 17/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0885 - acc: 0.9676 - val_loss: 0.0599 - val_acc: 0.9790\n",
            "Epoch 18/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0869 - acc: 0.9678 - val_loss: 0.0585 - val_acc: 0.9798\n",
            "Epoch 19/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0845 - acc: 0.9693 - val_loss: 0.0576 - val_acc: 0.9787\n",
            "Epoch 20/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0854 - acc: 0.9686 - val_loss: 0.0558 - val_acc: 0.9798\n",
            "Epoch 21/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0816 - acc: 0.9704 - val_loss: 0.0563 - val_acc: 0.9804\n",
            "Epoch 22/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0837 - acc: 0.9698 - val_loss: 0.0557 - val_acc: 0.9793\n",
            "Epoch 23/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0822 - acc: 0.9714 - val_loss: 0.0561 - val_acc: 0.9796\n",
            "Epoch 24/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0803 - acc: 0.9709 - val_loss: 0.0598 - val_acc: 0.9782\n",
            "Epoch 25/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0805 - acc: 0.9715 - val_loss: 0.0547 - val_acc: 0.9804\n",
            "Epoch 26/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0781 - acc: 0.9721 - val_loss: 0.0553 - val_acc: 0.9815\n",
            "Epoch 27/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0766 - acc: 0.9734 - val_loss: 0.0553 - val_acc: 0.9793\n",
            "Epoch 28/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0728 - acc: 0.9740 - val_loss: 0.0563 - val_acc: 0.9815\n",
            "Epoch 29/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0757 - acc: 0.9728 - val_loss: 0.0529 - val_acc: 0.9820\n",
            "Epoch 30/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0748 - acc: 0.9734 - val_loss: 0.0551 - val_acc: 0.9807\n",
            "Epoch 31/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0736 - acc: 0.9741 - val_loss: 0.0517 - val_acc: 0.9820\n",
            "Epoch 32/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0751 - acc: 0.9735 - val_loss: 0.0516 - val_acc: 0.9817\n",
            "Epoch 33/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0718 - acc: 0.9741 - val_loss: 0.0515 - val_acc: 0.9820\n",
            "Epoch 34/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0718 - acc: 0.9746 - val_loss: 0.0522 - val_acc: 0.9817\n",
            "Epoch 35/100\n",
            "33027/33027 [==============================] - 2s 51us/sample - loss: 0.0708 - acc: 0.9746 - val_loss: 0.0518 - val_acc: 0.9828\n",
            "Epoch 36/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0720 - acc: 0.9748 - val_loss: 0.0513 - val_acc: 0.9828\n",
            "Epoch 37/100\n",
            "33027/33027 [==============================] - 2s 54us/sample - loss: 0.0662 - acc: 0.9766 - val_loss: 0.0525 - val_acc: 0.9807\n",
            "Epoch 38/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0714 - acc: 0.9749 - val_loss: 0.0523 - val_acc: 0.9809\n",
            "Epoch 39/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0679 - acc: 0.9764 - val_loss: 0.0485 - val_acc: 0.9823\n",
            "Epoch 40/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0682 - acc: 0.9748 - val_loss: 0.0480 - val_acc: 0.9834\n",
            "Epoch 41/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0669 - acc: 0.9763 - val_loss: 0.0486 - val_acc: 0.9834\n",
            "Epoch 42/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0675 - acc: 0.9764 - val_loss: 0.0494 - val_acc: 0.9826\n",
            "Epoch 43/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0672 - acc: 0.9757 - val_loss: 0.0478 - val_acc: 0.9839\n",
            "Epoch 44/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0642 - acc: 0.9771 - val_loss: 0.0481 - val_acc: 0.9823\n",
            "Epoch 45/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0651 - acc: 0.9776 - val_loss: 0.0487 - val_acc: 0.9823\n",
            "Epoch 46/100\n",
            "33027/33027 [==============================] - 2s 56us/sample - loss: 0.0661 - acc: 0.9764 - val_loss: 0.0506 - val_acc: 0.9831\n",
            "Epoch 47/100\n",
            "33027/33027 [==============================] - 2s 55us/sample - loss: 0.0644 - acc: 0.9767 - val_loss: 0.0491 - val_acc: 0.9842\n",
            "Epoch 48/100\n",
            "33027/33027 [==============================] - 2s 53us/sample - loss: 0.0637 - acc: 0.9773 - val_loss: 0.0484 - val_acc: 0.9831\n",
            "Running fold #7\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 3s 95us/sample - loss: 0.3772 - acc: 0.8551 - val_loss: 0.1682 - val_acc: 0.9319\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.2028 - acc: 0.9148 - val_loss: 0.1353 - val_acc: 0.9447\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.1701 - acc: 0.9295 - val_loss: 0.1129 - val_acc: 0.9578\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.1508 - acc: 0.9388 - val_loss: 0.1017 - val_acc: 0.9646\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.1416 - acc: 0.9424 - val_loss: 0.0921 - val_acc: 0.9659\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1286 - acc: 0.9491 - val_loss: 0.0899 - val_acc: 0.9684\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1220 - acc: 0.9520 - val_loss: 0.0841 - val_acc: 0.9681\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1172 - acc: 0.9563 - val_loss: 0.0818 - val_acc: 0.9692\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1168 - acc: 0.9548 - val_loss: 0.0758 - val_acc: 0.9706\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.1083 - acc: 0.9576 - val_loss: 0.0767 - val_acc: 0.9706\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.1064 - acc: 0.9587 - val_loss: 0.0746 - val_acc: 0.9700\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1004 - acc: 0.9615 - val_loss: 0.0704 - val_acc: 0.9755\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0981 - acc: 0.9638 - val_loss: 0.0712 - val_acc: 0.9727\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0965 - acc: 0.9630 - val_loss: 0.0659 - val_acc: 0.9752\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0947 - acc: 0.9637 - val_loss: 0.0681 - val_acc: 0.9744\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0946 - acc: 0.9649 - val_loss: 0.0617 - val_acc: 0.9763\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0864 - acc: 0.9671 - val_loss: 0.0615 - val_acc: 0.9787\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0876 - acc: 0.9678 - val_loss: 0.0597 - val_acc: 0.9777\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0861 - acc: 0.9688 - val_loss: 0.0581 - val_acc: 0.9787\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0883 - acc: 0.9677 - val_loss: 0.0598 - val_acc: 0.9798\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0861 - acc: 0.9696 - val_loss: 0.0624 - val_acc: 0.9779\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0854 - acc: 0.9686 - val_loss: 0.0597 - val_acc: 0.9790\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0817 - acc: 0.9715 - val_loss: 0.0564 - val_acc: 0.9812\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0805 - acc: 0.9708 - val_loss: 0.0606 - val_acc: 0.9790\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0817 - acc: 0.9708 - val_loss: 0.0569 - val_acc: 0.9804\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0801 - acc: 0.9710 - val_loss: 0.0534 - val_acc: 0.9828\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0771 - acc: 0.9717 - val_loss: 0.0543 - val_acc: 0.9812\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0768 - acc: 0.9732 - val_loss: 0.0521 - val_acc: 0.9809\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0740 - acc: 0.9741 - val_loss: 0.0530 - val_acc: 0.9815\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0750 - acc: 0.9737 - val_loss: 0.0537 - val_acc: 0.9815\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0756 - acc: 0.9727 - val_loss: 0.0533 - val_acc: 0.9806\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0737 - acc: 0.9728 - val_loss: 0.0509 - val_acc: 0.9823\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0751 - acc: 0.9729 - val_loss: 0.0546 - val_acc: 0.9790\n",
            "Epoch 34/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0714 - acc: 0.9749 - val_loss: 0.0503 - val_acc: 0.9834\n",
            "Epoch 35/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0711 - acc: 0.9748 - val_loss: 0.0480 - val_acc: 0.9834\n",
            "Epoch 36/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0699 - acc: 0.9751 - val_loss: 0.0499 - val_acc: 0.9823\n",
            "Epoch 37/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0697 - acc: 0.9757 - val_loss: 0.0506 - val_acc: 0.9823\n",
            "Epoch 38/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0705 - acc: 0.9756 - val_loss: 0.0493 - val_acc: 0.9828\n",
            "Epoch 39/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0675 - acc: 0.9751 - val_loss: 0.0483 - val_acc: 0.9847\n",
            "Epoch 40/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0694 - acc: 0.9745 - val_loss: 0.0470 - val_acc: 0.9845\n",
            "Epoch 41/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0661 - acc: 0.9759 - val_loss: 0.0479 - val_acc: 0.9847\n",
            "Epoch 42/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0716 - acc: 0.9740 - val_loss: 0.0492 - val_acc: 0.9834\n",
            "Epoch 43/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.0669 - acc: 0.9761 - val_loss: 0.0484 - val_acc: 0.9834\n",
            "Epoch 44/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0665 - acc: 0.9771 - val_loss: 0.0482 - val_acc: 0.9839\n",
            "Epoch 45/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0649 - acc: 0.9767 - val_loss: 0.0466 - val_acc: 0.9839\n",
            "Epoch 46/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0663 - acc: 0.9763 - val_loss: 0.0499 - val_acc: 0.9842\n",
            "Epoch 47/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0635 - acc: 0.9777 - val_loss: 0.0478 - val_acc: 0.9845\n",
            "Epoch 48/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0643 - acc: 0.9774 - val_loss: 0.0470 - val_acc: 0.9847\n",
            "Epoch 49/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0636 - acc: 0.9781 - val_loss: 0.0455 - val_acc: 0.9856\n",
            "Epoch 50/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0654 - acc: 0.9766 - val_loss: 0.0465 - val_acc: 0.9861\n",
            "Epoch 51/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0622 - acc: 0.9774 - val_loss: 0.0451 - val_acc: 0.9845\n",
            "Epoch 52/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0611 - acc: 0.9774 - val_loss: 0.0466 - val_acc: 0.9856\n",
            "Epoch 53/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0620 - acc: 0.9776 - val_loss: 0.0451 - val_acc: 0.9842\n",
            "Epoch 54/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.0648 - acc: 0.9780 - val_loss: 0.0432 - val_acc: 0.9861\n",
            "Epoch 55/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.0637 - acc: 0.9778 - val_loss: 0.0460 - val_acc: 0.9858\n",
            "Epoch 56/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0631 - acc: 0.9777 - val_loss: 0.0451 - val_acc: 0.9828\n",
            "Epoch 57/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0607 - acc: 0.9787 - val_loss: 0.0430 - val_acc: 0.9845\n",
            "Epoch 58/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0616 - acc: 0.9783 - val_loss: 0.0449 - val_acc: 0.9856\n",
            "Epoch 59/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0616 - acc: 0.9781 - val_loss: 0.0427 - val_acc: 0.9861\n",
            "Epoch 60/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.0597 - acc: 0.9788 - val_loss: 0.0435 - val_acc: 0.9847\n",
            "Epoch 61/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0580 - acc: 0.9797 - val_loss: 0.0437 - val_acc: 0.9861\n",
            "Epoch 62/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0612 - acc: 0.9790 - val_loss: 0.0427 - val_acc: 0.9864\n",
            "Epoch 63/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0605 - acc: 0.9790 - val_loss: 0.0436 - val_acc: 0.9850\n",
            "Epoch 64/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0589 - acc: 0.9796 - val_loss: 0.0428 - val_acc: 0.9861\n",
            "Running fold #8\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 3s 97us/sample - loss: 0.3532 - acc: 0.8626 - val_loss: 0.1647 - val_acc: 0.9215\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.1932 - acc: 0.9211 - val_loss: 0.1248 - val_acc: 0.9439\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.1593 - acc: 0.9366 - val_loss: 0.1120 - val_acc: 0.9548\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.1428 - acc: 0.9416 - val_loss: 0.0996 - val_acc: 0.9608\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.1333 - acc: 0.9476 - val_loss: 0.0920 - val_acc: 0.9635\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.1260 - acc: 0.9516 - val_loss: 0.0905 - val_acc: 0.9657\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.1178 - acc: 0.9559 - val_loss: 0.0794 - val_acc: 0.9700\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.1172 - acc: 0.9554 - val_loss: 0.0786 - val_acc: 0.9722\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.1079 - acc: 0.9594 - val_loss: 0.0750 - val_acc: 0.9719\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.1050 - acc: 0.9612 - val_loss: 0.0706 - val_acc: 0.9736\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.1023 - acc: 0.9632 - val_loss: 0.0736 - val_acc: 0.9708\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0999 - acc: 0.9626 - val_loss: 0.0768 - val_acc: 0.9695\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0963 - acc: 0.9658 - val_loss: 0.0652 - val_acc: 0.9768\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0940 - acc: 0.9657 - val_loss: 0.0727 - val_acc: 0.9744\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0927 - acc: 0.9655 - val_loss: 0.0647 - val_acc: 0.9771\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0923 - acc: 0.9662 - val_loss: 0.0653 - val_acc: 0.9766\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0896 - acc: 0.9675 - val_loss: 0.0603 - val_acc: 0.9801\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0836 - acc: 0.9696 - val_loss: 0.0597 - val_acc: 0.9771\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0835 - acc: 0.9695 - val_loss: 0.0591 - val_acc: 0.9782\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0838 - acc: 0.9703 - val_loss: 0.0601 - val_acc: 0.9779\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0851 - acc: 0.9695 - val_loss: 0.0562 - val_acc: 0.9796\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0804 - acc: 0.9719 - val_loss: 0.0569 - val_acc: 0.9779\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0818 - acc: 0.9710 - val_loss: 0.0541 - val_acc: 0.9817\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0771 - acc: 0.9720 - val_loss: 0.0559 - val_acc: 0.9801\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0760 - acc: 0.9731 - val_loss: 0.0530 - val_acc: 0.9804\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0754 - acc: 0.9734 - val_loss: 0.0568 - val_acc: 0.9790\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0734 - acc: 0.9753 - val_loss: 0.0516 - val_acc: 0.9809\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0752 - acc: 0.9736 - val_loss: 0.0528 - val_acc: 0.9815\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 52us/sample - loss: 0.0733 - acc: 0.9744 - val_loss: 0.0510 - val_acc: 0.9817\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0717 - acc: 0.9735 - val_loss: 0.0507 - val_acc: 0.9812\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0723 - acc: 0.9741 - val_loss: 0.0489 - val_acc: 0.9831\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0730 - acc: 0.9749 - val_loss: 0.0512 - val_acc: 0.9823\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0688 - acc: 0.9759 - val_loss: 0.0470 - val_acc: 0.9820\n",
            "Epoch 34/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0709 - acc: 0.9741 - val_loss: 0.0578 - val_acc: 0.9766\n",
            "Epoch 35/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0697 - acc: 0.9756 - val_loss: 0.0484 - val_acc: 0.9831\n",
            "Epoch 36/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0711 - acc: 0.9757 - val_loss: 0.0470 - val_acc: 0.9826\n",
            "Epoch 37/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0702 - acc: 0.9753 - val_loss: 0.0469 - val_acc: 0.9836\n",
            "Epoch 38/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0684 - acc: 0.9760 - val_loss: 0.0473 - val_acc: 0.9836\n",
            "Epoch 39/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0671 - acc: 0.9754 - val_loss: 0.0464 - val_acc: 0.9820\n",
            "Epoch 40/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0649 - acc: 0.9778 - val_loss: 0.0453 - val_acc: 0.9836\n",
            "Epoch 41/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0667 - acc: 0.9759 - val_loss: 0.0453 - val_acc: 0.9853\n",
            "Epoch 42/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0671 - acc: 0.9766 - val_loss: 0.0452 - val_acc: 0.9839\n",
            "Epoch 43/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0635 - acc: 0.9778 - val_loss: 0.0443 - val_acc: 0.9834\n",
            "Epoch 44/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0651 - acc: 0.9769 - val_loss: 0.0467 - val_acc: 0.9839\n",
            "Epoch 45/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0646 - acc: 0.9777 - val_loss: 0.0448 - val_acc: 0.9853\n",
            "Epoch 46/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0656 - acc: 0.9770 - val_loss: 0.0463 - val_acc: 0.9842\n",
            "Epoch 47/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0630 - acc: 0.9783 - val_loss: 0.0482 - val_acc: 0.9828\n",
            "Epoch 48/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0630 - acc: 0.9779 - val_loss: 0.0441 - val_acc: 0.9834\n",
            "Epoch 49/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0626 - acc: 0.9782 - val_loss: 0.0433 - val_acc: 0.9842\n",
            "Epoch 50/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0607 - acc: 0.9789 - val_loss: 0.0450 - val_acc: 0.9847\n",
            "Epoch 51/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0618 - acc: 0.9785 - val_loss: 0.0422 - val_acc: 0.9839\n",
            "Epoch 52/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0600 - acc: 0.9789 - val_loss: 0.0407 - val_acc: 0.9839\n",
            "Epoch 53/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0596 - acc: 0.9793 - val_loss: 0.0466 - val_acc: 0.9847\n",
            "Epoch 54/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0579 - acc: 0.9793 - val_loss: 0.0426 - val_acc: 0.9845\n",
            "Epoch 55/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0593 - acc: 0.9790 - val_loss: 0.0403 - val_acc: 0.9850\n",
            "Epoch 56/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0583 - acc: 0.9807 - val_loss: 0.0400 - val_acc: 0.9842\n",
            "Epoch 57/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0578 - acc: 0.9792 - val_loss: 0.0425 - val_acc: 0.9847\n",
            "Epoch 58/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0585 - acc: 0.9799 - val_loss: 0.0401 - val_acc: 0.9850\n",
            "Epoch 59/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0596 - acc: 0.9784 - val_loss: 0.0421 - val_acc: 0.9853\n",
            "Epoch 60/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0584 - acc: 0.9791 - val_loss: 0.0425 - val_acc: 0.9853\n",
            "Epoch 61/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0582 - acc: 0.9802 - val_loss: 0.0403 - val_acc: 0.9845\n",
            "Running fold #9\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 3s 102us/sample - loss: 0.3564 - acc: 0.8561 - val_loss: 0.1601 - val_acc: 0.9343\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.1959 - acc: 0.9195 - val_loss: 0.1295 - val_acc: 0.9504\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1633 - acc: 0.9338 - val_loss: 0.1109 - val_acc: 0.9496\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.1462 - acc: 0.9408 - val_loss: 0.1001 - val_acc: 0.9578\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.1326 - acc: 0.9481 - val_loss: 0.0937 - val_acc: 0.9594\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.1244 - acc: 0.9509 - val_loss: 0.0861 - val_acc: 0.9681\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.1173 - acc: 0.9543 - val_loss: 0.0817 - val_acc: 0.9697\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.1132 - acc: 0.9566 - val_loss: 0.0789 - val_acc: 0.9727\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.1090 - acc: 0.9593 - val_loss: 0.0776 - val_acc: 0.9744\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.1040 - acc: 0.9614 - val_loss: 0.0755 - val_acc: 0.9760\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0976 - acc: 0.9645 - val_loss: 0.0704 - val_acc: 0.9744\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.1009 - acc: 0.9633 - val_loss: 0.0691 - val_acc: 0.9774\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.0967 - acc: 0.9639 - val_loss: 0.0662 - val_acc: 0.9779\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0903 - acc: 0.9684 - val_loss: 0.0643 - val_acc: 0.9793\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0886 - acc: 0.9675 - val_loss: 0.0656 - val_acc: 0.9777\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0879 - acc: 0.9690 - val_loss: 0.0622 - val_acc: 0.9785\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0849 - acc: 0.9685 - val_loss: 0.0590 - val_acc: 0.9798\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0832 - acc: 0.9696 - val_loss: 0.0597 - val_acc: 0.9787\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0839 - acc: 0.9699 - val_loss: 0.0576 - val_acc: 0.9798\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0821 - acc: 0.9708 - val_loss: 0.0569 - val_acc: 0.9809\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0810 - acc: 0.9710 - val_loss: 0.0576 - val_acc: 0.9801\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0787 - acc: 0.9725 - val_loss: 0.0573 - val_acc: 0.9806\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0793 - acc: 0.9705 - val_loss: 0.0557 - val_acc: 0.9817\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0765 - acc: 0.9724 - val_loss: 0.0566 - val_acc: 0.9820\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0774 - acc: 0.9738 - val_loss: 0.0555 - val_acc: 0.9809\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0742 - acc: 0.9734 - val_loss: 0.0501 - val_acc: 0.9831\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0736 - acc: 0.9737 - val_loss: 0.0510 - val_acc: 0.9826\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 53us/sample - loss: 0.0758 - acc: 0.9746 - val_loss: 0.0525 - val_acc: 0.9820\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0732 - acc: 0.9741 - val_loss: 0.0537 - val_acc: 0.9815\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0709 - acc: 0.9750 - val_loss: 0.0519 - val_acc: 0.9847\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0724 - acc: 0.9743 - val_loss: 0.0501 - val_acc: 0.9820\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 56us/sample - loss: 0.0751 - acc: 0.9739 - val_loss: 0.0531 - val_acc: 0.9820\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0715 - acc: 0.9758 - val_loss: 0.0540 - val_acc: 0.9826\n",
            "Epoch 34/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0691 - acc: 0.9757 - val_loss: 0.0553 - val_acc: 0.9831\n",
            "Epoch 35/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0687 - acc: 0.9761 - val_loss: 0.0502 - val_acc: 0.9826\n",
            "Epoch 36/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.0674 - acc: 0.9773 - val_loss: 0.0503 - val_acc: 0.9831\n",
            "Running fold #10\n",
            "Binary Cross-Entropy Loss Function\n",
            "Train on 33028 samples, validate on 3669 samples\n",
            "Epoch 1/100\n",
            "33028/33028 [==============================] - 3s 103us/sample - loss: 0.3662 - acc: 0.8566 - val_loss: 0.1556 - val_acc: 0.9316\n",
            "Epoch 2/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.2033 - acc: 0.9171 - val_loss: 0.1305 - val_acc: 0.9458\n",
            "Epoch 3/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.1686 - acc: 0.9293 - val_loss: 0.1094 - val_acc: 0.9608\n",
            "Epoch 4/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1467 - acc: 0.9410 - val_loss: 0.0981 - val_acc: 0.9659\n",
            "Epoch 5/100\n",
            "33028/33028 [==============================] - 2s 54us/sample - loss: 0.1298 - acc: 0.9495 - val_loss: 0.0869 - val_acc: 0.9703\n",
            "Epoch 6/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.1256 - acc: 0.9513 - val_loss: 0.0843 - val_acc: 0.9689\n",
            "Epoch 7/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.1191 - acc: 0.9532 - val_loss: 0.0763 - val_acc: 0.9703\n",
            "Epoch 8/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.1175 - acc: 0.9538 - val_loss: 0.0721 - val_acc: 0.9752\n",
            "Epoch 9/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.1104 - acc: 0.9567 - val_loss: 0.0703 - val_acc: 0.9763\n",
            "Epoch 10/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.1060 - acc: 0.9602 - val_loss: 0.0703 - val_acc: 0.9768\n",
            "Epoch 11/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.1072 - acc: 0.9590 - val_loss: 0.0685 - val_acc: 0.9768\n",
            "Epoch 12/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0984 - acc: 0.9629 - val_loss: 0.0650 - val_acc: 0.9782\n",
            "Epoch 13/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0962 - acc: 0.9637 - val_loss: 0.0610 - val_acc: 0.9793\n",
            "Epoch 14/100\n",
            "33028/33028 [==============================] - 2s 57us/sample - loss: 0.0953 - acc: 0.9652 - val_loss: 0.0606 - val_acc: 0.9817\n",
            "Epoch 15/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0928 - acc: 0.9658 - val_loss: 0.0602 - val_acc: 0.9809\n",
            "Epoch 16/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0942 - acc: 0.9660 - val_loss: 0.0612 - val_acc: 0.9812\n",
            "Epoch 17/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0907 - acc: 0.9662 - val_loss: 0.0564 - val_acc: 0.9809\n",
            "Epoch 18/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.0868 - acc: 0.9690 - val_loss: 0.0595 - val_acc: 0.9785\n",
            "Epoch 19/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0871 - acc: 0.9675 - val_loss: 0.0566 - val_acc: 0.9834\n",
            "Epoch 20/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0851 - acc: 0.9688 - val_loss: 0.0545 - val_acc: 0.9831\n",
            "Epoch 21/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0877 - acc: 0.9668 - val_loss: 0.0556 - val_acc: 0.9826\n",
            "Epoch 22/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0853 - acc: 0.9685 - val_loss: 0.0518 - val_acc: 0.9828\n",
            "Epoch 23/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0803 - acc: 0.9719 - val_loss: 0.0512 - val_acc: 0.9836\n",
            "Epoch 24/100\n",
            "33028/33028 [==============================] - 2s 62us/sample - loss: 0.0824 - acc: 0.9707 - val_loss: 0.0519 - val_acc: 0.9828\n",
            "Epoch 25/100\n",
            "33028/33028 [==============================] - 2s 63us/sample - loss: 0.0816 - acc: 0.9706 - val_loss: 0.0535 - val_acc: 0.9834\n",
            "Epoch 26/100\n",
            "33028/33028 [==============================] - 2s 61us/sample - loss: 0.0802 - acc: 0.9718 - val_loss: 0.0551 - val_acc: 0.9815\n",
            "Epoch 27/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0765 - acc: 0.9720 - val_loss: 0.0502 - val_acc: 0.9826\n",
            "Epoch 28/100\n",
            "33028/33028 [==============================] - 2s 55us/sample - loss: 0.0775 - acc: 0.9724 - val_loss: 0.0515 - val_acc: 0.9826\n",
            "Epoch 29/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0763 - acc: 0.9719 - val_loss: 0.0486 - val_acc: 0.9826\n",
            "Epoch 30/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0775 - acc: 0.9720 - val_loss: 0.0500 - val_acc: 0.9823\n",
            "Epoch 31/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0735 - acc: 0.9743 - val_loss: 0.0495 - val_acc: 0.9839\n",
            "Epoch 32/100\n",
            "33028/33028 [==============================] - 2s 60us/sample - loss: 0.0720 - acc: 0.9740 - val_loss: 0.0493 - val_acc: 0.9834\n",
            "Epoch 33/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0715 - acc: 0.9751 - val_loss: 0.0455 - val_acc: 0.9847\n",
            "Epoch 34/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0720 - acc: 0.9747 - val_loss: 0.0478 - val_acc: 0.9842\n",
            "Epoch 35/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0708 - acc: 0.9740 - val_loss: 0.0471 - val_acc: 0.9847\n",
            "Epoch 36/100\n",
            "33028/33028 [==============================] - 2s 58us/sample - loss: 0.0740 - acc: 0.9728 - val_loss: 0.0494 - val_acc: 0.9845\n",
            "Epoch 37/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0675 - acc: 0.9753 - val_loss: 0.0493 - val_acc: 0.9831\n",
            "Epoch 38/100\n",
            "33028/33028 [==============================] - 2s 59us/sample - loss: 0.0704 - acc: 0.9749 - val_loss: 0.0464 - val_acc: 0.9845\n",
            "Time to complete 15 min 944.2358016967773 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ta-ruG0K1eVN",
        "colab_type": "text"
      },
      "source": [
        "### Model Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FV5LefXplyt",
        "colab_type": "code",
        "outputId": "c17805d8-03f2-4c99-cd6b-22e4a472ce26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "scores = model.evaluate(X,encoded_y, verbose=1)\n",
        "print(model.metrics_names)\n",
        "acc, loss = scores[1]*100, scores[0]\n",
        "print('Baseline: accuracy: {:.2f}%: loss: {:.2f}'.format(acc, loss))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36697/36697 [==============================] - 2s 46us/sample - loss: 0.0411 - acc: 0.9857\n",
            "['loss', 'acc']\n",
            "Baseline: accuracy: 98.57%: loss: 0.04\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DviMoEPX1hxZ",
        "colab_type": "text"
      },
      "source": [
        "#### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kU7Ncq7ply2",
        "colab_type": "code",
        "outputId": "2b2236bf-cd33-4ed1-84af-c2d8f8da584f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prediction_y = model.predict_classes(X, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36697/36697 [==============================] - 1s 29us/sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0T2dZA9Nply8",
        "colab_type": "code",
        "outputId": "866a4685-8ca6-4b91-cc75-ce81459573c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "y=LabelEncoder().fit_transform(binary_df[dep_var].values)\n",
        "cm = confusion_matrix(y, prediction_y)\n",
        "sn.heatmap(cm, cmap='Blues', annot=True, fmt='g', xticklabels=['Benign', 'Malicious'],\n",
        "        yticklabels=['Benign', 'Malicious'])"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f370527fc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW0AAAD8CAYAAAC8TPVwAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB20zswxx0DYzyxAHbTOzDHHQNjPLkP8HXFszf3we8VcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YdW3jb8plzB",
        "colab_type": "text"
      },
      "source": [
        "#### Graph of Binary Cross-Entropy Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzeLN1BSplzC",
        "colab_type": "code",
        "outputId": "ff97254c-5c20-4f1d-8bbb-1c85eb90b44f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plot('Binary Model Accuracy')"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nS5YsjDHGdOn/A/3WTBhcNhrhAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8Il5_kL1oQt",
        "colab_type": "text"
      },
      "source": [
        "### Write Results to File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qz6wRy6GplzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resultFile = os.path.join(resultPath, dataFile)\n",
        "with open('{}.result'.format(resultFile), 'a') as fout:\n",
        "  fout.write('{} results...'.format(model_name+model_extension))\n",
        "  fout.write('\\taccuracy: {:.2f} loss: {:.2f}\\n'.format(acc, loss))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}